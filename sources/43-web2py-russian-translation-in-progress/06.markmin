## Абстрактный уровень базы данных
``DAL``:inxx

### Зависимости

web2py поставляется с модулем абстрактного уровня базы данных (DAL), который посредством API позволяет отобразить объекты Python в такие объекты базы данных, как запросы, таблицы и записи. DAL динамически генерирует SQL в режиме реального времени, используя указанный диалект для конечной базы данных, так что вам не придется писать код SQL или изучать различные диалекты SQL (термин SQL используется обобщенно), и приложение будет переносимым между различными типами баз данных. Частичный список поддерживаемых баз данных показан в таблице ниже. Пожалуйста, проверьте на веб-сайте web2py и список рассылки для получения более свежих адаптеров. Google NoSQL рассматривается как частный случай в главе 13.

Раздел Gotchas в конце этой главы содержит более подробную информацию о конкретных базах данных.

Бинарный дистрибутив Windows, работает из коробки с SQLite, MySQL, PostgreSQL и MySQL. Бинарный дистрибутив Mac работает из коробки с SQLite.
Чтобы использовать любую другую базу данных со стороны сервера(back-end), из дистрибутива с исходным кодом запустите и установите соответствующий драйвер, требуемый со стороны сервера(back end).
``database drivers``:inxx

После того, как правильный драйвер установлен, запустите web2py из источника, и он найдет драйвер. Вот список драйверов web2py которые можно использовать:

``DAL``:inxx ``SQLite``:inxx ``MySQL``:inxx ``PostgresSQL``:inxx ``Oracle``:inxx ``MSSQL``:inxx ``FireBird``:inxx ``DB2``:inxx ``Informix``:inxx ``Sybase``:inxx ``Teradata``:inxx ``MongoDB``:inxx ``CouchDB``:inxx ``SAPDB``:inxx ``Cubrid``:inxx

----------
database | drivers (source)
SQLite | sqlite3 or pysqlite2 or zxJDBC ``zxjdbc``:cite  (on Jython)
PostgreSQL | psycopg2 ``psycopg2``:cite  or pg8000 ``pg8000``:cite or zxJDBC ``zxjdbc``:cite  (on Jython)
MySQL | pymysql ``pymysql``:cite or MySQLdb ``mysqldb``:cite
Oracle | cx_Oracle ``cxoracle``:cite
MSSQL | pyodbc ``pyodbc``:cite or pypyodbc``pypyodbc``:cite
FireBird | kinterbasdb ``kinterbasdb``:cite or fdb or pyodbc
DB2 | pyodbc ``pyodbc``:cite
Informix | informixdb ``informixdb``:cite
Ingres | ingresdbi ``ingresdbi``:cite
Cubrid | cubriddb ``cubridb``:cite ``cubridb``:cite
Sybase | Sybase ``Sybase``:cite
Teradata | pyodbc ``Teradata``:cite
SAPDB    | sapdb ``SAPDB``:cite
MongoDB | pymongo ``pymongo``:cite
IMAP | imaplib ``IMAP``:cite
---------

``sqlite3``, ``pymysql``, ``pg8000``, и ``imaplib`` идут вместе с web2py. Поддержка MongoDB является экспериментальной. Опция IMAP позволяет использовать DAL для доступа по протоколу IMAP.

### DAL: Быстрый тур
web2py определяет следующие классы, которые составляют DAL:

Объект **DAL** представляет собой соединение с базой данных. Например:
``sqlite``:inxx
``
db = DAL('sqlite://storage.sqlite')
``:code

``define_table``:inxx
Класс таблицы **Table** представляет собой таблицу базы данных. Вы не создаете экземпляр Table  напрямую, за вас это делает ``DAL.define_table``, который и создает экземпляр.
``
db.define_table('mytable', Field('myfield'))
``:code

Наиболее важными методами класса Table являются:
``insert``:inxx
``truncate``:inxx
``drop``:inxx
``import_from_csv_file``:inxx
``count``:inxx
``.insert``, ``.truncate``, ``.drop``, и ``.import_from_csv_file``.

``Field``:inxx
Класс **Field** представляет собой поле базы данных. Вы можете создать экземпляр класса и передать в качестве аргумента при определении таблицы ``DAL.define_table``.

``Rows``:inxx
**DAL Rows** ``Row``:inxx  это объект, возвращаемый методом select базы данных. Это можно представить в виде списка строк ``Row``:
``
rows = db(db.mytable.myfield != None).select()
``:code

``Row``:inxx
**Row** содержит значения поля.
``
for row in rows:
    print row.myfield
``:code

``Query``:inxx
**Query** это объект, который представляет собой SQL условие "Где":
``
myquery = (db.mytable.myfield != None) | (db.mytable.myfield > 'A')
``:code

``Set``:inxx
**Set** это объект, который представляет собой набор записей. Его наиболее важные методы ``count``, ``select``, ``update``, and ``delete``. For example:
``
myset = db(myquery)
rows = myset.select()
myset.update(myfield='somevalue')
myset.delete()
``:code

``Expression``:inxx

**Expression** это что-то вроде ``orderby`` или ``groupby`` выражение. Класс Field происходит от Expression. Вот пример.
``
myorder = db.mytable.myfield.upper() | db.mytable.id
db().select(db.table.ALL, orderby=myorder)
``:code

### Использование DAL "автономно"
Web2py DAL может использоваться вне среды web2py с помощью
``
from gluon import DAL, Field
# также рассмотреть: from gluon.validators import *
``:code

[[dal_constructor]]
### DAL конструктор
Основное использование:
``
>>> db = DAL('sqlite://storage.sqlite')
``:code

Теперь база данных подключена и соединение сохраняется в глобальной переменной ``db``.

В любое время вы можете получить строку подключения.
``_uri``:inxx
``
>>> print db._uri
sqlite://storage.sqlite
``:code

и имя базы данных
``_dbname``:inxx
``
>>> print db._dbname
sqlite
``:code

Строка соединения называется ``_uri`` потому что это экземпляр унифицированного идентификатора ресурса (Uniform Resource Identifier).

DAL допускает несколько соединений с одной базой данных или с различными базами данных, и даже с базами данных различных типов. На данный момент, мы будем предполагать наличие единой базы данных, так как это самая распространенная ситуация.

#### DAL подпись
``
DAL(uri='sqlite://dummy.db',
    pool_size=0,
    folder=None,
    db_codec='UTF-8',
    check_reserved=None,
    migrate=True,
    fake_migrate=False,
    migrate_enabled=True,
    fake_migrate_all=False,
    decode_credentials=False,
    driver_args=None,
    adapter_args=None,
    attempts=5,
    auto_import=False,
    bigint_id=False,
    debug=False,
    lazy_tables=False,
    db_uid=None,
    do_connect=True,
    after_connection=None,
    tables=None,
    ignore_field_case=True,
    entity_quoting=False,
    table_hash=None)
``:code

[[connection_strings]]
#### Строки подключения (Значение URI параметров)
``connection strings``:inxx

Соединение с базой данных устанавливается путем создания экземпляра объекта DAL:
``
>>> db = DAL('sqlite://storage.sqlite', pool_size=0)
``:code
``db`` не является ключевым словом; это локальная переменная, которая хранит объект подключения ``DAL``. Вы можете дать ему другое имя. Конструктор ``DAL`` требует один аргумент, строку подключения. Строка соединения является единственным кодом web2py, который зависит от конкретной конечной базы данных. Ниже приведены примеры строк соединения для конкретных типов поддерживаемых конечных баз данных (во всех случаях, мы предполагаем, что база данных работает с локального хоста на его порту по умолчанию и называется "test"):

``ndb``:index

-------------
**SQLite**     | ``sqlite://storage.sqlite``
**MySQL**      | ``mysql://username:password@localhost/test``
**PostgreSQL** | ``postgres://username:password@localhost/test``
**MSSQL (legacy)**      | ``mssql://username:password@localhost/test``
**MSSQL (>=2005)**      | ``mssql3://username:password@localhost/test``
**MSSQL (>=2012)**      | ``mssql4://username:password@localhost/test``
**FireBird**   | ``firebird://username:password@localhost/test``
**Oracle**     | ``oracle://username/password@test``
**DB2**        | ``db2://username:password@test``
**Ingres**     | ``ingres://username:password@localhost/test``
**Sybase**     | ``sybase://username:password@localhost/test``
**Informix**   | ``informix://username:password@test``
**Teradata**   | ``teradata://DSN=dsn;UID=user;PWD=pass;DATABASE=test``
**Cubrid**     | ``cubrid://username:password@localhost/test``
**SAPDB**      | ``sapdb://username:password@localhost/test``
**IMAP**       | ``imap://user:password@server:port``
**MongoDB**    | ``mongodb://username:password@localhost/test``
**Google/SQL** | ``google:sql://project:instance/database``
**Google/NoSQL** | ``google:datastore``
**Google/NoSQL/NDB** | ``google:datastore+ndb``
-------------

Обратите внимание на то, что база данных SQLite состоит из одного файла. Если он не существует, то он будет создан. Этот файл блокируется каждый раз при обращении к нему. В случае с MySQL, PostgreSQL, MSSQL, FireBird, Oracle, DB2, Ingres и Informix база данных "test" должна быть создана за пределами web2py. После того, как соединение установлено, web2py сможет создавать, изменять и удалять таблицы соответствующим образом. 

В случае Google/NoSQL опция ``+ ndb`` включает NDB. NDB использует Memcache буфер для чтения данных, доступ к которым часто используется. Это выпорлянется полностью автоматически и сделано на уровне хранилища данных, а не на уровне web2py.

Кроме того, можно установить строку подключения на ``None``. В этом случае DAL не будет подключаться к любой конечной базе данных, но API все еще может быть доступен для тестирования. Примеры этого будут рассмотрены в главе 7.

Иногда вам может понадобиться генерировать SQL-запросы таким образом, как если бы у вас была связь, но без фактического подключения к базе данных. Это можно сделать с помощью

``
db = DAL('...', do_connect=False)
``:code

В этом случае вы сможете вызвать ``_select``, ``_insert``, ``_update``, и ``_delete`` для генерации SQL, но не можете вызвать ``select``, ``insert``, ``update``, и ``delete``. В большинстве случаев вы можете использовать ``do_connect=False`` даже не имея необходимых драйверов баз данных.

Обратите внимание на то, что по умолчанию web2py использует кодировку utf8 для баз данных. Если вы работаете с существующими базами данных, которые ведут себя по-разному, вы должны изменить его с дополнительным параметром вроде ``db_codec``.

``
db = DAL('...', db_codec='latin1')
``:code

В противном случае вы получите билеты UnicodeDecodeError.


#### Пулы соединений
``connection pooling``:inxx

Общепринятым аргументом конструктора DAL является ``pool_size``; он по умолчанию равен нулю.

Поскольку это довольно медленно, чтобы установить новое соединение с базой данных для каждого запроса, web2py реализует механизм пула соединений. После того, как соединение установлено, и страница подана и транзакция завершена, соединение не закрывается, а уходит в пул. При поступлении следующего запроса HTTP, web2py пытается переработать соединение из пула и использовать его для новой транзакции. Если нет доступных соединений в пуле, то устанавливается новое соединение.

Когда запускается web2py, пул всегда пустой. Пул вырастает до минимума между значением ``pool_size`` и максимальным числом одновременных запросов. Это означает, что если ``pool_size = 10`` и наш сервер никогда не получает более 5 одновременных запросов, то фактический размер пула будет расти только до 5. Если ``pool_size = 0`` то пул соединений не используется.

Соединения в пулах распределяются последовательно между потоками, в том смысле, что они могут использовать два различных, но не одновременных потоков. Существует только один пул для каждого процесса web2py.

Параметр ``pool_size`` игнорируется SQLite и Google App Engine.
Пул соединений игнорируется для SQLite, так как это не дает никакой выгоды.

#### Неудачи подключения (число попыток)

Если web2py не удается подключиться к базе данных, то он ожидает 1 секунду и по умолчанию делает еще 5 попыток, прежде чем объявить неудачу. В случае пула соединений вполне возможно, что существует пулированное соединение, которое остается открытым, но не используется в течение некоторого времени, будет закрыто со стороны базы данных. Благодаря функции повторной попытки web2py пытается повторно установить данные разрывы подключений.
Количество попыток устанавливается с помощью параметра числа попыток.

#### Ленивые Таблицы (Lazy Tables)
Настройка ``lazy_tables = True`` обеспечивает значительное повышение производительности. Смотри ниже: [[Ленивые Таблицы #lazy_tables]]

#### Без-Модельные приложения

Использование каталога модели web2py для ваших моделей приложений является очень удобным и продуктивным. С ленивыми таблицами и условными моделями, производительность обычно приемлемая даже для больших приложений. Многие опытные разработчики используют это для производственной среды. 

Тем не менее, можно определить таблицы DAL по требованию внутри функций контроллера или модулей. Это может иметь смысл, когда число или сложность определения таблиц перегружает использование ленивых таблиц и условных моделей.

Это именуется как разработка "без-модели" ("model-less") посредством web2py сообщества.
Это означает отсутствие использования автоматического выполнения python файлов в каталоге модели.
Это не означает отказа от концепции модели, представления и контроллеры.

Web2py автоматически выполняет Python код внутри каталога модели:

+ Модели выполняются автоматически каждый раз, когда обрабатывается запрос
+ Моделям доступна глобальная область видимости web2py.

Модели также делают полезными сессии интерактивной оболочки, когда web2py запускается из командной строки с опцией -M.

Кроме того, помните о сопровождаемости: другие разработчики web2py рассчитывают найти определения модели в каталоге модели.

Используя подход "без модели", вы принимаете ответственность за выполнения этих двух задач по ведению домашнего хозяйства. 
Вы вызываете определения таблицы, когда вы нуждаетесь в них, и обеспечиваете необходимый доступ к глобальной области видимости с помощью текущего объекта. (как описано в главе 4 [[Использование объектом current глобальной области видимости ../04#current_object]] )

Например, типичное приложение без модели может пропустить определения объектов подключения к базе данных в файле модели, но определить таблицы по первому требованию функции контроллера.

Типичный случай заключается в перемещении определений таблицы в файл модуля (файл Python, сохраненный в каталоге модулей).

Если функция, которая определяет набор таблиц, называется ``define_employee_tables()`` и содержиться в модуле, который называется "table_setup.py", то ваш контроллер, который хочет обратиться к таблицам, связанными с записями сотрудников, с тем чтобы сделать SQLFORM должен вызвать функцию ``define_employee_tables()`` перед доступом к любым таблицам. Функция ``define_employee_tables()`` должна получить доступ к объекту подключения базы данных с целью определения таблиц. Именно поэтому вам нужно правильно использовать объект ``current`` в файле модуля, содержащего ``define_employee_tables()``(как упоминалось выше).

#### Реплицируемые базы данных

Первый аргумент ``DAL(...)`` может быть списком URI-адресов. В этом случае web2py пытается подключиться к каждой из них. Основная цель этого заключается в том, чтобы иметь дело с несколькими серверами баз данных и распределить нагрузку между ними). Вот типичный случай использования:

``
db = DAL(['mysql://...1', 'mysql://...2', 'mysql://...3'])
``:code

В этом случае DAL пытается подключиться к первой и, в случае неудачи, он будет пытаться соединиться со второй и третьей. Это также может быть использовано для распределения нагрузки в конфигурации ведущей-ведомой базы данных. Мы поговорим об этом больше в главе 13 в контексте масштабируемости.

#### Зарезервированные ключевые слова
``reserved Keywords``:inxx

``check_reserved`` говорит конструктору проверить имена таблиц и имена столбцов на наличие зарезервированных ключевых слов SQL в целевых серверных базах данных. ``check_reserved`` по умолчанию None.

Это список строк, которые содержат имена адаптеров серверных баз данных.

Имя адаптера является тем же самым, что и используемое в строке подключения DAL. Так что если вы хотите проверить в отношении PostgreSQL и MSSQL, то строка подключения будет выглядеть следующим образом:
``
db = DAL('sqlite://storage.sqlite',
         check_reserved=['postgres', 'mssql'])
``:code

DAL будет сканировать ключевые слова в том же порядке, как и в списке.

Есть два дополнительных варианта "all" и "common". Если вы укажете все ("all"), то он будет проверять по всем известным ключевым словам SQL. Если вы укажете общие ("common"), он будет проверять только по общим ключевым словам, таких как SQL ``SELECT``, ``INSERT``, ``UPDATE`` и т.п.

Для поддерживаемых серверных частей вы можете также указать, что вы также хотите проверять в отношении незарезервированных ключевых слов SQL. В этом случае необходимо добавить в конец ``_nonreserved`` к названию. Например:
``
check_reserved=['postgres', 'postgres_nonreserved']
``:code

Следующие серверные базы данных поддерживают проверку зарезервированных слов.

-----
**PostgreSQL** | ``postgres(_nonreserved)``
**MySQL** | ``mysql``
**FireBird** | ``firebird(_nonreserved)``
**MSSQL** | ``mssql``
**Oracle** | ``oracle``
-----

#### Настройки кваотирования и случаев базы данных (entity_quoting, ignore_field)

Вы также можете использовать явное кваотирование сущностей SQL на уровне DAL.  Это работает прозрачно, так что вы можете использовать одни и те же имена в Python и в схеме DB.

``ignore_field_case = True``
``entity_quoting = True``
Вот пример:
``
db = DAL('postgres://...', ...,ignore_field_case=False, entity_quoting=True)

db.define_table('table1', Field('column'), Field('COLUMN'))

print db(db.table1.COLUMN != db.table1.column).select()
``:code

#### Другие параметры конструктора DAL

##### Расположение папки базы данных
``folder`` – где будут созданы файлы .table. Автоматически устанавливается в пределах web2py. Используйте явный путь при использовании DAL вне web2py

##### Настройки миграции по умолчанию
Миграция подробно описана ниже в Tables [[миграции таблицы #table_migrations]]. Параметры DAL конструктора миграции являются булевыми, затрагивают значения по умолчанию и глобальное поведение.

``migrate = True`` устанавливает поведение по умолчанию при миграции для всех таблиц

``fake_migrate = False`` устанавливает поведение по умолчанию поддельной миграции fake_migrate для всех таблиц

``migrate_enabled = True`` Если задано False, то отключает все миграции

``fake_migrate_all = False`` Если задано True, то поддельные миграции во всех таблицах


#### Эксперимент с web2py оболочкой

Вы можете поэкспериментировать с DAL API используя web2py оболочку (-S [[опция коммандной строки ../04#CommandLineOptions]]).

Начните с создания соединения. Ради примера, вы можете использовать SQLite. Ничего в этой дискуссии не меняется при изменении серверного движка.


[[table_constructor]]
### Конструктор таблиц
``define_table``:inxx ``Field``:inxx

#### Подпись define_table
Подпись для define_table:

Таблицы определяются в DAL с помощью ``define_table``:
``
>>> db.define_table('person',
                    Field('name'),
                    id=id,
                    rname=None,
                    redefine=True
                    common_filter,
                    fake_migrate,
                    fields,
                    format,
                    migrate,
                    on_define,
                    plural,
                    polymodel,
                    primarykey,
                    redefine,
                    sequence_name,
                    singular,
                    table_class,
                    trigger_name)
``:code

Он определяет, сохраняет и возвращает ``Table`` объект под названием "person", содержащей поле (столбец) "name". Этот объект также может быть доступен через ``db.person``, так что вам не нужно ловить возвращаемое значение.

#### ``id``: Замечания по первичному ключу 
Не объявляйте поле, называемое "id", потому что оно все равно создается web2py. Каждая таблица имеет поле с названием "id" по умолчанию. Это целочисленное поле с автоматическим приращением (начиная с 1) используется для перекрестных ссылок и для создания каждой записи уникальным образом, поэтому "id" является первичным ключем. (Примечание: счетчик id начинается с 1 и  задается серверной частью. Например, это не относится к the Google App Engine NoSQL.)

``named id field``:inxx
При желании вы можете определить поле ``type='id'`` и web2py будет использовать это поле как поле id с автоприращением. Это не рекомендуется, кроме случаев доступа к таблицам устаревших баз данных, которые имеют первичный ключ под другим именем.
С некоторыми ограничениями, вы можете также использовать различные первичные ключи используя ``primarykey`` параметр. [[Первичный ключ #primarykey]] будет объяснен ниже в ближайшее время.

#### Множественное ``plural`` и единственное ``singular`` числа
SmartGrid объектам возможно понадобится знать имя таблицы в единственном и множественном числе. Значения по умолчанию являются умными, но эти параметры вы можете конкретизировать. Смотрите SmartGrid для получения дополнительной информации. 
#### Переопределение ``redefine``
Таблицы могут быть определены только один раз, но вы можете заставить web2py переопределить существующую таблицу:

``
db.define_table('person', Field('name'))
db.define_table('person', Field('name'), redefine=True)
``:code

Переопределение может вызвать миграцию, если содержимое поля отличается.

[[record_representation]]
#### Формат ``format``: представление записи

Это не является обязательным, но рекомендуется указать представление формата для записей с помощью ``format`` параметра.
``
>>> db.define_table('person', Field('name'), format='%(name)s')
``:code

или
``
>>> db.define_table('person', Field('name'), format='%(name)s %(id)s')
``:code

или даже более сложные, с помощью функции:
``
>>> db.define_table('person',
                    Field('name'),
                    format=lambda r: r.name or 'anonymous')
``:code

Атрибут формата format будет использоваться для двух целей:
- Для представления записей, имеющих ссылки при выборе/опций в раскрывающихся списках.
- Установите атрибут ``db.othertable.person.represent`` для всех полей, ссылающихся на эту таблицу. Это значит, что SQLTABLE не будет показывать ссылки по id, а вместо этого будет использовать предпочтительный формат представления.

#### ``rname``: Представление записи
``rname`` задает имя конечной базы данных для таблицы. Это создает псевдоним имени таблицы web2py, а ``rname`` это реальное имя, используемое при создании запроса для конечной базы данных.
Проиллюстрируем только одино использование, ``rname`` может быть использован для обеспечения MSSQL полностью квалифицированного по именам таблиц доступа к таблицам, принадлежащих к другим базам данных на сервере: ``rname = 'db1.dbo.table1'``:code

[[primarykey]]
#### Первичный ключ ``primarykey``: Поддержка традиционных таблиц

``primarykey`` помогает поддерживать устаревшие таблицы с существующими первичными ключами, даже из нескольких частей.
Смотрите [[Устаревшие Базы данных #LegacyDatabases]] ниже.

#### ``migrate``, ``fake_migrate``
``migrate`` задает параметры миграции для таблицы. Смотреть [[Миграции Таблицы #table_migrations]] ниже

#### ``table_class``
Если вы определили свой собственный класс Table в качестве подкласса gluon.dal.Table, вы можете предоставить его здесь; это позволяет вам расширить и переопределить методы. Например: ``table_class=MyTable``:code

#### ``sequence_name``
(Необязательно) Имя последовательности пользовательской таблицы (если поддерживается базой данных). Можно создать SEQUENCE (начинающейся с 1 и увеличивающийся на 1) или использовать это для устаревших таблиц с пользовательскими последовательностями. Обратите внимание, что в случае необходимости, web2py будет создавать последовательности автоматически по умолчанию (начинающихся с 1). 

#### ``trigger_name``
(Опционально) Относится к `` sequence_name``. Уместно для некоторых движков баз данных, которые не поддерживают автоприращение числовых полей. 

#### ``polymodel``
Для Google App Engine

#### ``on_define``
``on_define`` является обратным вызовом , срабатывающим когда создается экземпляр lazy_table, хотя он вызывается в любом случае, даже если таблица не является ленивой. Это позволяет динамически вносить изменения в таблицу без потери преимуществ задержанного создания экземпляра. 

Например:
``
db = DAL(lazy_tables=True)
db.define_table('person',
                Field('name'),
                Field('age', 'integer'))

on_define=lambda table: [table.name.set_attributes(requires=IS_NOT_EMPTY(), default=''),
                         table.age.set_attributes(requires=IS_INT_IN_RANGE(0, 120), default=30)]
``:code
Обратите внимание, этот пример показывает, как использовать ``on_define``, но это фактически не нужно. Простые значения ``requires`` могут быть добавлены к определениям поля и таблица все еще будет ленивой. Тем не менее, ``requires``, который принимает объект Set в качестве первого аргумента, такие как IS_IN_DB, будет делать запрос, как ``db.sometable.somefield == some_value``:code, который бы вызвал ``sometable`` определенную в начале. Это является ситуацией, сохраняемой через ``on_define``.

[[lazy_tables]]
#### Ленивые таблицы, основной прирост производительности
``lazy tables``:inxx
web2py модели выполняются перед контроллерами, так что все таблицы определяются в каждом запросе. Не все таблицы необходимы для обработки каждого запроса, так что вполне возможно, что некоторое время тратится при определениях таблиц впустую. Условные модели ([[условные модели, глава 4 ../04#conditional_models]]) могут помочь, но web2py предлагает большой прирост производительности с помощью lazy_tables. Эта возможность означает, что создание таблиц откладывается, пока таблица не будет на самом деле ссылаемой. Включение ленивых таблиц производится при инициализации базы данных с помощью конструктора DAL. Он требует установки параметра ``DAL(...,lazy_tables=True)``. Это одно из наиболее существенных повышений производительности по времени отклика в web2py.

#### Добавление атрибутов к полям и таблицам
Если вам нужно добавить пользовательские атрибуты полей, то вы можете просто сделать:
``db.table.field.extra = {}``:code 

"extra" не является ключевым словом; это пользовательские атрибуты ныне прикрепленные к объекту поля. Вы можете сделать это с таблицами также, но они  должны предворяться знаком подчеркивания, чтобы избежать конфликта имен с полями: 

``db.table._extra = {} ``:code


[[field_constructor]]
### Конструктор Field
``Field constructor``:inxx
Вот значения по умолчанию конструктора Field:
``
Field(fieldname, type='string', length=None, default=None,
      required=False, requires='<default>',
      ondelete='CASCADE', notnull=False, unique=False,
      uploadfield=True, widget=None, label=None, comment=None,
      writable=True, readable=True, update=None, authorize=None,
      autodelete=False, represent=None, compute=None,
      uploadfolder=None, uploadseparate=None, uploadfs=None, rname=None)
``:code

Не все из них актуальны для каждого поля. "length" имеет значение только для полей типа "string". "uploadfield" и "authorize" имеют значения только для полей типа "upload". "ondelete" имеет значение только для полей типа "reference" и "upload".
- ``length`` устанавливает максимальную длину поля типа "string", "password" или "upload".  Если ``length`` не задано, то используется значение по умолчанию, но значение по умолчанию не гарантирует обратную совместимость. ''Чтобы избежать нежелательных миграции при обновлении, мы рекомендуем вам всегда указывать длину полей строк, пароля и загрузки.''
- ``default`` устанавливает значение по умолчанию для поля. Значение по умолчанию используется при выполнении вставки, если значение не указано явно. Он также используется для предварительного заполнения формы, построенных из таблицы с использованием SQLFORM. Обратите внимание, что скорее всего взамен фиксированного значения по умолчанию может быть функция (в том числе лямбда-функция), которая возвращает значение соответствующего типа для поля. В этом случае функция вызывается один раз для каждой вставляемой записи, даже при наличии нескольких вставляемых записей в одной транзакции.
- ``required`` сообщает DAL, что вставка не должна быть разрешена в этой таблице, если значение для этого поля не указано явно.
- ``requires`` является валидатором или списком валидаторов. Параметр не используется DAL, но он используется SQLFORM. Валидаторы по умолчанию для заданных типов полей приведены в следующем разделе.
-------
Обратите внимание на то , что ``requires=...`` обеспечивается на уровне форм, ``required=True`` обеспечивается на уровне DAL (вставка), в то время как ``notnull``, ``unique`` и ``ondelete`` применяются на уровне базы данных. Несмотря на то, что они иногда могут показаться излишними, важно поддерживать различие при программировании с помощью DAL.
-------
- ``uploadfolder`` пока по умолчанию используется ``None``, большинство адаптеров баз данных будет по умолчанию загружать файлы в os.path.join(request.folder, 'uploads'). MongoAdapter кажется, не будет делать это в настоящее время.
- ``rname`` обеспечивает чтобы поле имело "реальное имя" ("real name"), имя для поля, известного адаптера базы данных; когда поле используется, то значение rname это то значение, которое отправляется в базу данных. Имя для поля web2py является в действительности псевдонимом.
``ondelete``:inxx
- ``ondelete`` переводится как выражение SQL "ON DELETE". По умолчанию он установлен на "CASCADE". Это сообщает базе данных, чтобы при удалении заданной записи также необходимо удалить все связанные записи. Чтобы отключить эту функцию, задайте параметру ``ondelete`` значение "NO ACTION" или "SET NULL".
- ``notnull=True`` переводится как выражение SQL "NOT NULL". Он ограждает базу данных от вставки нулевых значений для поля.
- ``unique=True`` переводится как выражение SQL "UNIQUE" и он гарантирует, что значения этого поля являются уникальными в пределах таблицы. Это обеспечивается на уровне базы данных.
- ``uploadfield`` применяется только к полям типа "upload". Поле типа "upload" хранит имя файла, сохраненного в другом месте, по умолчанию в папке приложения "uploads/" файловой системы. Если ``uploadfield`` задан как True, то файл хранится в поле двоичных объектов в пределах одной таблицы и значение ``uploadfield`` это имя поля двоичных объектов. Этот вопрос будет обсуждаться более подробно позже в контексте SQLFORM.
- ``uploadfolder`` по умолчанию является папкой приложения "uploads/". Если задать другой путь, то файлы будут загружены в другую папку. 
-------
Например поле,
``
Field(...,uploadfolder=os.path.join(request.folder,'static/temp'))
``:code
будет загружать файлы в папку "web2py/applications/myapp/static/temp".
-------
- ``uploadseparate`` Если занано значение True, поле будет загружать файлы в различные подпапки внутри папки ''uploadfolder''. Предназначено для оптимизации во избежание слишком большого количества файлов в одной той же самой папке folder/subfolder. ВНИМАНИЕ: Вы не можете изменить значение ``uploadseparate`` с True на False без разрушения ссылок на существующие загрузки. web2py либо использует отдельные вложенные папки, либо нет. Изменение поведения после того, как файлы были загружены будет препятствовать web2py при получении этих файлов. Если это произойдет, то можно переместить файлы и исправить эту проблему, но этот способ не описывается здесь.
- ``uploadfs`` позволяет указать другую файловую систему, куда загружать файлы, включая хранилище Amazon S3 или удаленное хранилище SFTP. Данный параметр требует установленный PyFileSystem. ``uploadfs`` должен указывать на ``PyFileSystem``. ``PyFileSystem``:inxx ``uploadfs``:idxx
- ``widget`` должен быть одним из доступных виджет объектов, в том числе одним из пользовательских виджетов, например: ``SQLFORM.widgets.string.widget``. Список доступных виджетов будет обсуждаться позже. Каждый тип поля имеет виджет по умолчанию.
- ``label`` является строкой (или помощником или чем-то, что может быть сериализовано в строку), содержащей метку и которая будет использоваться для этого поля в автоматически генерируемых формах.
- ``comment``  является строкой (или помощником или чем-то, что может быть сериализовано в строку), содержащей связанный с этим полем комментарий и который будет отображаться справа от поля ввода в автоматически сгенерированных формах.
- ``writable`` объявляет поле в формах, предназначенное для записи.
- ``readable`` объявляет поле в формах, предназначенное для чтения. Если поле не предназначено ни для чтения, ни для записи, то оно не будет отображаться при создании и обновлении формы.
- ``update`` содержит значение по умолчанию для этого поля, когда запись является обновленной.
- ``compute`` является дополнительной функцией. Если запись вставлена или обновлена, то данная функция вычислений будет выполнена, и поле будет заполнено результатом действия функции. Запись передается в функцию вычисления в виде словаря ``dict``, и словарь dict не будет включать в себя текущее значение этого, или любого другого вычисляемого поля.
- ``authorize`` может быть использован при необходимости контроля доступа для соответствующего поля, только для полей загрузки "upload". Это будет обсуждаться более подробно в контексте аутентификации и авторизации.
- ``autodelete`` задает автоматическое удаление соответствующего загруженного файла при удалении ссылки на файл. Только для полей загрузки "upload". Тем не менее, удаление записей  самой базой данных при выполнении соответсвующей операции CASCADE, не будет вызывать автоудаление web2py. В Web2py группе Google имеется обсуждение обходных путей.
- ``represent`` может быть None или может указывать на функцию, которая принимает значение поля и возвращает альтернативное представление для значения поля. 
-------
Пример:
``
db.mytable.name.represent = lambda name, row: name.capitalize()
db.mytable.other_id.represent = lambda id, row: row.myfield
db.mytable.some_uploadfield.represent = lambda value, row: A('get it', _href=URL('download', args=value))
``:code
-------


[[field_types]]
#### Типы полей
``field types``:inxx

----------
**Тип поля** | **Валидаторы поля по умолчанию**| **Описание**
``string`` | ``IS_LENGTH(length)``| Строка (длина по умолчанию 512)
``text`` | ``IS_LENGTH(65536)``| Текст
``blob`` | ``None``| Большой двочный объект (big large object)
``boolean`` | ``None``| Булевое значение
``integer`` | ``IS_INT_IN_RANGE(-1e100, 1e100)``| Целое число
``double`` | ``IS_FLOAT_IN_RANGE(-1e100, 1e100)``| Действительное число
``decimal(n,m)`` | ``IS_DECIMAL_IN_RANGE(-1e100, 1e100)``| Десятичная дробь
``date`` | ``IS_DATE()``| Дата
``time`` | ``IS_TIME()``| Время
``datetime`` | ``IS_DATETIME()``| Дата-время
``password`` | ``None``| Пароль
``upload`` | ``None``| Поле загрузки
``reference <table>``  | ``IS_IN_DB(db, table.field, format)``| Ссылка
``list:string`` | ``None``| Список строк
``list:integer`` | ``None``| Список целых
``list:reference <table>`` | ``IS_IN_DB(db, table.field, format, multiple=True)``| список ссылок
``json`` | ``IS_JSON()``| Формат json
``bigint`` | ``None``| Большое целое
``big-id`` | ``None``| Длинный индентификатор
``big-reference`` | ``None``| Длинная ссылка
---------

Десятичный тип поля требует и возвращает значения в виде объектов класса ``Decimal``, как это определено в Python модуле ``decimal``. SQLite не обрабатывает тип ``decimal``, таким образом внутренне мы рассматриваем его как ``double``. (n,m) это количество цифр в целой и в дробной части соответственно.

Типы ``big-id`` и ``big-reference`` поддерживаются только некоторыми движками баз данных и являются экспериментальными. Они обычно не используются в качестве типов полей, за исключением использования для устаревших таблиц, однако, конструктор DAL имеет аргумент ``bigint_id``, который при установке на ``True`` преобразует тип полей ``id`` и ``reference`` в ``big-id`` и ``big-reference`` соответственно.

Поля с типом ``list:<тип>`` являются особенным, потому что они разработаны, чтобы воспользоваться преимуществами определенных функций денормализации на NoSQL (в случае с Google App Engine NoSQL, типы полей ``ListProperty`` и ``StringListProperty``) и  обратного-портирования (back-port) данных типов полей для всех других поддерживаемым реляционных баз данных. В реляционных базах данных списки хранятся в виде поля ``text``. Элементы разделяются символом ``|`` и каждый символ ``|``, находящийся в элементе строки должен экранироваться путем удваивания ``||``. Они обсуждаются в отдельном разделе.

Тип поля ``json`` не требует много пояснений. Он может хранить любой сериализованный объект в формате JSON. Он предназначен специально для работы с MongoDB и обратного портирования на другие адаптеры базы данных для портативности.


``blob``:inxx
Поля ``blob``(big large object) также являются специальными. По умолчанию, двоичные данные кодируются в base64 перед сохранением в фактическое поле базы данных и декодируются при извлечении. Это оказывает негативное влияние, используя на 33% больше места для хранения, чем это необходимо в blob  полях, но имеет преимущество в создании связи независимо от определенных соглашений по экранированию конечных баз данных.

#### Поле времени выполнения и таблица модификации

Большинство атрибутов полей и таблиц могут быть изменены после их определения:

``
db.define_table('person',
                Field('name', default=''),
                format='%(name)s')

db.person._format = '%(name)s/%(id)s'
db.person.name.default = 'anonymous'
``
(Обратите внимание, что атрибуты таблиц, как правило, начинается с префикса подчеркивания, чтобы избежать конфликта с возможными именами полей).

Вы можете получить список таблиц, которые были определены для данного соединения с базой данных:

``tables``:inxx
``
>>> print db.tables
['person']
``:code

Вы также можете перечислить поля, которые были определены для данной таблицы:

``fields``:inxx
``
>>> print db.person.fields
['id', 'name']
``:code


Вы можете запросить тип таблицы:

``Table``:inxx
``
>>> print type(db.person)
<class 'gluon.sql.Table'>
``:code

и вы можете получить доступ к таблицы из DAL соединения используя:
``
>>> print type(db['person'])
<class 'gluon.sql.Table'>
``:code

Точно так же вы можете получить доступ к полям по их имени несколькими эквивалентными способами:
``
>>> print type(db.person.name)
<class 'gluon.sql.Field'>
>>> print type(db.person['name'])
<class 'gluon.sql.Field'>
>>> print type(db['person']['name'])
<class 'gluon.sql.Field'>
``:code

Получив поле, вы можете получить доступ к атрибутам, установленным в его определении:
``
>>> print db.person.name.type
string
>>> print db.person.name.unique
False
>>> print db.person.name.notnull
False
>>> print db.person.name.length
32
``:code

в том числе его родительскую таблицу, имя таблицы и родительское подключение:
``
>>> db.person.name._table == db.person
True
>>> db.person.name._tablename == 'person'
True
>>> db.person.name._db == db
True
``:code

Поле также имеет методы. Некоторые из них используются для построения запросов, и мы рассмотрим их позже.
Особым методом объекта поля является ``validate``, который вызывает валидаторы для поля.

``
print db.person.name.validate('John')
``

возвращает кортеж ``(value, error)``. ``error`` равняется ``None`` если входные данные прошли проверку.



[[table_migrations]]
### Миграции

``migrations``:inxx

Метод ``define_table`` проверяет, существует ли или нет соответствующая таблица. Если это не так, то он генерирует SQL-код для ее создания и выполняет его. Если таблица все же существует, но отличается от определяемой, то он генерирует SQL-код, чтобы изменить таблицу и выполняет его. Если изменяется тип поля, но не имя, то он попытается преобразовать данные (Если вы не хотите этого, то вам нужно переопределить таблицу дважды, в первый раз, позволяя web2py отбросить поле путем его удаления, и во второй раз добавляя вновь определенное поле таким образом, что web2py может создать его.). Если таблица существует и соответствует текущему определению, то он оставит его в покое. Во всех случаях он будет создавать объект ``db.person``, который представляет таблицу.

Мы называем это поведение как "миграция" ("migration"). web2py регистрирует все миграции и попытки миграции в файле "databases/sql.log".

Первый аргумент ``define_table`` всегда является именем таблицы. Остальными безымянными аргументами являются поля (Field). Функция также принимает необязательный аргумент - ключевое слово с именем "migrate":
``
>>> db.define_table('person', Field('name'), migrate='person.table')
``:code

Значением migrate является имя файла (в папке "databases" приложения) где web2py хранит внутреннюю миграционную информацию для этой таблицы.
Эти файлы очень важны и не должны удаляться до тех пор, пока соответствующие таблицы существуют.  В тех случаях, когда таблица была отброшена и соответствующий файл по-прежнему существует, он может быть удален вручную. По умолчанию, миграция имеет значение True. Это заставляет web2py генерировать имя файла из хэш строки подключения. Если миграция имеет значение False, то миграция не выполняется, и web2py предполагает, что таблица существует в хранилище данных и содержит (по крайней мере) поля, перечисленные в ``define_table``.
Передовая практика заключается в задании явного имени таблицы миграции.

Здесь не может быть две таблицы в одном и том же приложении с аналогичным именем файла миграции.

Класс DAL также принимает аргумент "migrate", который определяет значение migrate по умолчанию для вызовов в ``define_table``. Например,
``
>>> db = DAL('sqlite://storage.sqlite', migrate=False)
``:code

будет устанавливать значение migrate по умолчанию на False всякий раз, когда ``db.define_table`` вызывается без аргумента migrate.

------
Обратите внимание на то, что web2py мигрирует только новые столбцы, удаленные столбцы, а также столбцы с измененным типом (за исключением SQLite). web2py не выполняет миграцию такие изменений в атрибутах, как изменения значений ``default``, ``unique``, ``notnull``, и ``ondelete``.
------

Миграции могут быть отключены для всех таблиц одновременно:

``
db = DAL(..., migrate_enabled=False)
``

Это рекомендуемое поведение, когда два приложения имеют общую базу данных. Только одно из этих двух приложений должен выполнять миграции, а другому следует отключить их.

### Фиксация нарушенных миграций
``fake_migrate``:inxx

Есть две общие проблемы, связанные с миграциями и есть способы восстановления после них.

Одна из проблем специфична для SQLite. SQLite не применяет типы столбцов и не может отбросить столбцы. Это означает, что если у вас есть столбец строкового типа и вы удалите его, то он на самом деле не удаляется. Если вы добавляете столбец снова с другим типом (например, datetime ) вы получите в конечном итоге столбец datetime, который содержит строки (барахло для практических целей). web2py не пожалуется на это, потому что не знает, что находится в базе данных, пока он не пытается получить записи и не потерпит неудачу.

Если web2py возвращает ошибку в функции gluon.sql.parse при выборе записей, это проблема: появляются поврежденные данные в столбце из-за указанной выше проблемы.

Решение состоит в обновлении всех записей таблицы и обновления значений в столбце на None.

Другая проблема является более общией, но типичной для MySQL. MySQL не позволяет более одного оператора ALTER TABLE в транзакции. Это означает, что web2py должен разбить сложные операции на более мелкие (один ALTER TABLE за раз) и зафиксировать один кусок в это же время. Поэтому возможно, что одна часть сложной транзакции получит фиксацию, а другая часть потерпит неудачу, оставляя web2py в испорченном состоянии. Почему части транзакции потерпит неудачу? Потому что, например, она может включать в себя изменение таблицы и преобразования строкового столбца в столбец даты и времени, web2py попытается преобразовать данные, но данные не могут быть преобразованы. Что произойдет с web2py? Он будет поставлен в тупик о том, какая именно структура таблицы на самом деле хранится в базе данных.

Решение состоит из отключение миграций для всех таблиц и включения поддельных миграций:
``
db.define_table(...., migrate=True, fake_migrate=True)
``:code

Это позволит web2py перестроить метаданные о таблице в соответствии с определением таблицы. Попробуйте несколько определений таблицы, чтобы увидеть, какая из них работает (одна перед неудачной миграцией, и одна после неудачной миграции). После успешного завершения удалите ``fake_migrate=True`` параметр.

Прежде чем пытаться исправить проблемы миграции благоразумно сделать копию файлов "applications/yourapp/databases/*.table".

Проблемы миграции также могут быть фиксированными для всех таблиц сразу:

``
db = DAL(..., fake_migrate_all=True)
``:code

Это также терпит неудачу, если модель описывает таблицы, которые не существуют в базе данных, но это может помочь сузить проблему.

### Краткое описание миграционного контроля

Логика различных аргументов миграции обобщена в этом псевдокоде:
``
if DAL.migrate_enabled and table.migrate:
   if DAL.fake_migrate_all or table.fake_migrate:
       perform fake migration
   else:
       perform migration
``:code

### Вставка ``insert``

Получив таблицу, вы можете вставлять записи

``insert``:inxx
``
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
``:code

при вставке возвращаются уникальные значения "id" для каждой из вставленной записи.

Вы можете обрезать (truncate) таблицу, то есть, удалить все записи и сбросить счетчик id.

``truncate``:inxx
``
>>> db.person.truncate()
``:code

Теперь, если вы вставите запись снова, то отсчет начнется с 1 (это зависит от конкретной базы данных и не относится к Google NoSQL):
``
>>> db.person.insert(name="Alex")
1
``:code

Обратите внимание, вы можете передать некоторые параметры ``truncate``, например, чтобы соотщить SQLITE перезапустить счетчик id.

``
db.person.truncate('RESTART IDENTITY CASCADE')
``:code

Аргумент является необработанным SQL-кодом и поэтому зависит от конкретного движка базы данных.

``bulk_insert``:inxx
web2py также предоставляет метод массовой вставки bulk_insert
``
>>> db.person.bulk_insert([{'name': 'Alex'}, {'name': 'John'}, {'name': 'Tim'}])
[3, 4, 5]
``:code

Она принимает список из словарей с именами полей и их значениями, которые подлежат вставке, и выполняет несколько вставок одновременно. Он возвращает идентификаторы вставленных записей. На поддерживаемых реляционных базах данных не существует каких-либо преимуществ в использовании этой функции в отличие от цикла и выполнения отдельных вставок, а вот на Google App Engine NoSQL, это дает основное преимущество в скорости.

### Фиксация ``commit`` и откат ``rollback`` изменений

Операции создания, отбрасывания, вставки, обрезания, удаления или обновления фактически не фиксируются до тех пор, пока web2py выдаст команду фиксировать. В моделях, представлениях и контроллерах, web2py делает это за вас, но в модулях вам потребуется делать фиксации самостоятельно.

``commit``:inxx
``
>>> db.commit()
``:code

Чтобы проверить это, давайте вставим новую запись:
``
>>> db.person.insert(name="Bob")
2
``:code

и откатим, т.е. проигнорируем все операции после последней фиксации:

``rollback``:inxx
``
>>> db.rollback()
``:code

Если вы теперь вставите снова, то счетчик вновь будет установлен на 2, так как предыдущая вставка откатывается.
``
>>> db.person.insert(name="Bob")
2
``:code

Код в моделях, представлениях и контроллерах заключен в web2py код, который выглядит следующим образом (псевдокод):
``
try:
    выполнение моделей, функции контроллера и представления
except:
    откат всех подключений
    запись трэйсбэка (отслеживания ошибки) в лог
    отправка билета (страницы с кодом ошибки) посетителю
else:
    фиксация всех подключений
    сохранение куки, сессий и возврат страницы
``:code

Таким образом, в моделях, представлениях и контроллерах web2py нет необходимости в явном вызове ``commit``  или ``rollback``, если вам не требуется более детальный контроль.
Тем не менее, в модулях вам нужно будет использовать ``commit()``.

### Сырой SQL

#### Расчет времени запросов

Все запросы автоматически просчитываются по времени web2py. Переменная ``db._timings`` является списком кортежей. Каждый кортеж содержит необработанный запрос SQL, который передается драйверу базы данных и время выполнения запроса в секундах. Эта переменная может быть отображена в представлениях с помощью панели инструментов:

``
{{=response.toolbar()}}
``

#### Метод ``executesql``

DAL позволяет явно выдавать SQL операторы.

``executesql``:inxx
``
>>> print db.executesql('SELECT * FROM person;')
[(1, u'Massimo'), (2, u'Massimo')]
``:code

В этом случае возвращаемые значения не разбираются или преобразуются через DAL, и формат зависит от конкретного драйвера базы данных. Такое использование с выборкой данных, как правило, не требуется, но чаще встречается с индексами.
Метод ``executesql`` принимает четыре необязательных аргумента: ``placeholders``, ``as_dict``, ``fields`` и ``colnames``.
``placeholders`` является необязательной последовательностью значений, которые будут замещены, или, если эта функция поддерживается драйвером базы данных, словарем с ключами, соответствующими именованным местозаполнителям в вашем SQL.

Если ``as_dict`` установлен на True, то результирующий курсор, возвращаемый драйвером базы данных будет преобразован в последовательность словарей, ключированных именами полей базы данных. Результаты возвращенные с ``as_dict = True`` являются теми же самыми, которые возвращается при применении **. as_list()** для нормального выбора.
``
[{field1: value1, field2: value2}, {field1: value1b, field2: value2b}]
``:code

аргумент ``fields`` явялется списком объектов DAL Field, которые соответствуют возвращаемым из базы данных полям. Объекты Field должны быть частью одного или нескольких объектов Table, определенных в объекте DAL. Список ``fields`` может включать в себя один или несколько объектов DAL Table в дополнение или взамен включения объектов Field, или он может быть только одной таблицей (нет в списке). В этом случае объекты Field будут извлечены из таблиц(ы).

Вместо уточнения аргумента ``fields``, аргумент ``colnames``  может быть указан в виде списка имен полей в следующем формате tablename.fieldname. Опять же, они должны представлять таблицы и поля, определенные в объекте DAL.

Кроме того, можно указать оба ``fields`` и связанный с ним ``colnames``. В этом случае, ``fields`` может также включать в себя объекты DAL Expression в дополнение к объектам Field. Для объектов Field в "полях", связанный с ним `` colnames`` должны еще быть в tablename.fieldname формате.
Для Expression объектов в ``fields``, связанные с ним ``colnames`` могут быть любыми произвольными метками.

Обратите внимание, объекты DAL Table ссылаемые через ``fields`` или ``colnames`` могут быть фиктивными таблицами и не должны представлять какие-либо реальные таблицы в базе данных. Кроме того, обратите внимание, что ``fields`` и ``colnames`` должны идти в том же порядке, что и поля в курсоре результатов, возвращаемых из базы данных.


#### Атрибут ``_lastsql``

Если SQL был выполнен вручную с помощью executesql или SQL-код был сгенерирован через DAL, то вы всегда можете найти код SQL в атрибуте ``db._lastsql``. Это полезно для целей отладки:

``_lastdb``:inxx
``
>>> rows = db().select(db.person.ALL)
>>> print db._lastsql
SELECT person.id, person.name FROM person;
``:code

-------
web2py никогда не генерирует запросы, используя оператор "*". web2py всегда явно указывает при выборе полей.
-------

### Метод ``drop``

Наконец, вы можете отбрасывать таблицы и все данные будут потеряны:

``drop``:inxx
``
>>> db.person.drop()
``:code

Примечание для SQLite: web2py не будет заново создавать отброшенную таблицу, пока вы не перейдете по файловой системе в каталог баз данных вашего приложения, и не удалите файл, связанный с отброшенной таблицей. 

### Индексы

В настоящее время DAL API не предоставляет команду для создания индексов таблиц, но это можно сделать с помощью команды ``executesql``. Это происходит потому, что существование индексов может сделать миграции сложным, и лучше иметь дело с ними в явном виде. Индексы могут быть необходимы для тех полей, которые используются в повторяющихся запросах.

Ниже приведен пример того, как [[создать индекс с помощью SQL в SQLite http://www.sqlite.org/lang_createindex.html]]:
``
>>> db = DAL('sqlite://storage.sqlite')
>>> db.define_table('person', Field('name'))
>>> db.executesql('CREATE INDEX IF NOT EXISTS myidx ON person (name);')
``:code

Другие диалекты базы данных имеют очень похожие синтаксисы, но могут не поддерживать необязательную "IF NOT EXISTS" директиву.

[[LegacyDatabases]]
### Унаследованные базы данных и ключированные таблицы

web2py может подключаться к унаследованным базам данных при некоторых условиях.

Самый простой способ, когда будут выполнены эти условия:
- Каждая таблица должена иметь уникальное автоприращаемое целочисленное поле, называемое "id"
- Записи должны быть привязаны исключительно с помощью "id" поля.

При обращении к существующей таблице, то есть к таблица не созданной web2py в текущем приложении, всегда устанавливается ``migrate=False``.

Если унаследованная таблица имеет автоприращаемое целочисленное поле, но оно не называется "id", то web2py по-прежнему может получить доступ к нему, но определение таблицы должно содержать в явном виде ``Field('....','id')``, где вместо точек ... подставляется имя автоприращаемого целочисленного поля.

``keyed table``:inxx

И, наконец, если унаследованная таблица использует первичный ключ, который не является автоприращаемым id полем, то существует возможность использовать "ключированную таблицу", например:

``
db.define_table('account',
                Field('accnum','integer'),
                Field('acctype'),
                Field('accdesc'),
                primarykey=['accnum','acctype'],
                migrate=False)
``:code

- ``primarykey`` список имен полей, которые составляют первичный ключ.
- Все первичные ключевые поля имеют установку ``NOT NULL``, даже если это не указано.
- Ключированные таблицы могут ссылаться только на другие ключированные таблицы.
- Ссылочные поля должны использовать формат ``reference tablename.fieldname``.
- Функция ``update_record`` недоступна для Строк ключированных таблиц.

-------
В настоящее время ключированные таблицы поддерживаются только для DB2, MS-SQL, Ingres и Informix, но будут добавлены другие движки.
-------

На момент написания, мы не можем гарантировать, что атрибут ``primarykey`` работает с каждой существующей унаследованной таблицей и каждой поддерживаемой конечной базой данных.
Для простоты, мы рекомендуем, если это возможно, создать представление базы данных, которое имеет автоприращаемое id поле.


### Распределенные транзакции
``distributed transactions``:inxx

------
На момент написания эта возвожность поддерживается только PostgreSQL, MySQL и Firebird, так как они предоставляют API для двухфазных фиксаций.
------

Если у вас есть два (или более) подключений к отдельным базам данных PostgreSQL, например:
``
db_a = DAL('postgres://...')
db_b = DAL('postgres://...')
``:code

В ваших моделях или контроллерах, вы можете зафиксировать их одновременно:
``
DAL.distributed_transaction_commit(db_a, db_b)
``:code

В случае неудачи, эта функция выполняет откат и поднимает исключение (``Exception``).

В контроллерах, когда действие возвращает результат, если у вас есть два различных соединения и вы не вызывали вышеприведенную функцию, то web2py фиксирует их по отдельности. Это означает, что существует вероятность того, что одна из фиксаций может завершиться успешно, а другая потерпеть неудачу. Распределенная транзакция предотвращает от такого происшествия.

### Дополнительная информация по загрузкам

Рассмотрим следующую модель:
``
>>> db.define_table('myfile',
                    Field('image', 'upload', default='path/'))
``:code

В случае поля 'загрузки', значение атрибута default при необходимости может быть задано как путь (абсолютный путь или относительный путь к текущей папке приложения) и файл изображения по умолчанию будет копироваться по заданному пути. Новая копия делается для каждой новой записи, которая не определяет изображение.

Обычно вставка выполняется автоматически с помощью SQLFORM или формы crud (которая представляет собой SQLFORM), но иногда у вас уже есть файл в файловой системе и вы хотите загрузить его программно. Это может быть сделано таким образом:
``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename))
``:code

Кроме того, можно вставить файл более простым способом и иметь автоматически вызываемый хранилищем метод вставки:

``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=stream)
``:code

В этом случае имя файла получается из объекта stream, если таковой имеется.

Метод ``store`` объекта поля загрузки принимает stream файла и имя файла. Он использует имя файла, чтобы определить расширение (тип) файла, создает новое временное имя для файла (в соответствии с web2py механизмом загрузки) и загружает содержимое файла в этот новый временный файл (в папку uploads, если не указано иное). Он возвращает новое временное имя, которое затем хранится в поле ``image`` из таблицы ``db.myfile``.

Обратите внимание, если файл должен быть сохранен в соответствующем blob поле, а не в файловой системе, метод ``store()`` не будет вставлять файл в blob поле (потому что ``store()`` вызывается перед вставкой), поэтому файл должен явно вставляться в blob поле:
``
>>> db.define_table('myfile',
                    Field('image', 'upload', uploadfield='image_file'),
                    Field('image_file', 'blob'))
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename),
                     image_file=stream.read())
``:code


Противоположностью ``.store`` является ``.retrieve``:

``
>>> row = db(db.myfile).select().first()
>>> (filename, stream) = db.myfile.image.retrieve(row.image)
>>> import shutil
>>> shutil.copyfileobj(stream, open(filename, 'wb'))
``

### Объекты ``Query``, ``Set``, ``Rows``

Давайте снова рассмотрим ранее определенные таблицы (и сброшенные) и вставим три записи:
``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
>>> db.person.insert(name="Carl")
3
``:code

Вы можете сохранить таблицу в переменной. Например, в переменной ``person``, вы могли бы сделать:

``Table``:inxx
``
>>> person = db.person
``:code

Вы также можете сохранить поля в такую переменную как ``name``.  Например, вы могли бы также сделать:

``Field``:inxx
``
>>> name = person.name
``:code

Вы даже можете построить запрос (используя операторы вроде ==, !=, <, >, <=, >=, like, belongs) и сохранить запрос в переменной ``q``:

``Query``:inxx
``
>>> q = name == 'Alex'
``:code

Когда вы вызываете ``db`` с запросом, вы определяете набор (Set) записей. Вы можете сохранить его в переменной ``s`` и написать:

``Set``:inxx
``
>>> s = db(q)
``:code

Обратите внимание на то, что до сих пор ни один запрос к базе данных еще не был выполнен. DAL + Query просто определяют набор записей в этой базе данных, который соответствуют запросу.
web2py из запроса определяет какая таблица (или таблицы) вовлечены и, по факту, нет необходимости указывать что-либо еще.

### Метод ``select``

Допустим задан объект Set, в переменной``s``, тогда вы можете получить записи с помощью команды ``select``:

``Rows``:inxx ``select``:inxx
``
>>> rows = s.select()
``:code

``Row``:inxx
Она возвращает итерируемый объект класса ``gluon.sql.Rows`` элементы которого являются объектами Row. ``gluon.sql.Row`` объекты действуют как словари, но их элементы также могут быть доступны в качестве атрибутов, как в ``gluon.storage.Storage``. Объекты класса Row отличаются от объектов класса Storage тем, что их значения предназначены только для чтения.

The Rows object allows looping over the result of the select and printing the selected field values for each row:
``
>>> for row in rows:
        print row.id, row.name
1 Alex
``:code

You can do all the steps in one statement:
``
>>> for row in db(db.person.name == 'Alex').select():
        print row.name
Alex
``:code

``ALL``:inxx

The select command can take arguments. All unnamed arguments are interpreted as the names of the fields that you want to fetch. For example, you can be explicit on fetching field "id" and field "name":
``
>>> for row in db().select(db.person.id, db.person.name):
        print row.name
Alex
Bob
Carl
``:code

The table attribute ALL allows you to specify all fields:
``
>>> for row in db().select(db.person.ALL):
        print row.name
Alex
Bob
Carl
``:code

Notice that there is no query string passed to db. web2py understands that if you want all fields of the table person without additional information then you want all records of the table person.

An equivalent alternative syntax is the following:
``
>>> for row in db(db.person.id > 0).select():
        print row.name
Alex
Bob
Carl
``:code

and web2py understands that if you ask for all records of the table person (id > 0) without additional information, then you want all the fields of table person.

Given one row

``
row = rows[0]
``

you can extract its values using multiple equivalent expressions:

``
>>> row.name
Alex
>>> row['name']
Alex
>>> row('person.name')
Alex
``

The latter syntax is particularly handy when selecting en expression instead of a column. We will show this later.

You can also do
``
rows.compact = False
``
to disable the notation
``
row[i].name
``
and enable, instead, the less compact notation:
``
row[i].person.name
``
Yes this is unusual and rarely needed.

Row objects also have two important methods:
``
row.delete_record()
``
and 
``
row.update_record(name="new value")
``

#### Using an iterator-based select for lower memory use

Python "iterators" are a type of "lazy-evaluation". They 'feed' data one step at time; traditional python loops create the entrire set of data in memory before looping. 

The traditional use of select is:
``
for row in db(db.table.id > 0).select():
    rtn = row
``:code

but for large numbers of rows, using an iterator-based alternative has dramatically lower memory use:
``
for row in db(db.table.id > 0).iterselect():
    rtn = row
``:code
Testing shows this is around 10% faster as well, even on machines with large RAM.


#### Rendering rows using represent
You may wish to rewrite rows returned by select to take advantage of formatting information contained in the represents setting of the fields. 

``
rows = db(query).select()  
repr_row = rows.render(0)
``:code

If you don't specify an index, you get a generator to iterate over all the rows:

``
for row in rows.render():
    print row.myfield
``:code

Can also be applied to slices:

``
for row in rows[0:10].render():
    print row.myfield
``:code

If you only want to transform selected fields via their "represent" attribute, you can list them in the "fields" argument:

``
repr_row = row.render(0, fields=[db.mytable.myfield])
``:code

Note, it returns a transformed copy of the original Row, so there's no update_record (which you wouldn't want anyway) or delete_record.


#### Shortcuts
``DAL shortcuts``:inxx

The DAL supports various code-simplifying shortcuts.
In particular:
``
myrecord = db.mytable[id]
``:code

returns the record with the given ``id`` if it exists. If the ``id`` does not exist, it returns ``None``. The above statement is equivalent to

``
myrecord = db(db.mytable.id == id).select().first()
``:code


You can delete records by id:

``
del db.mytable[id]
``:code

and this is equivalent to

``
db(db.mytable.id == id).delete()
``:code

and deletes the record with the given ``id``, if it exists.

Note: This delete shortcut syntax does not currently work if [[versioning #versioning]] is activated

You can insert records:

``
db.mytable[0] = dict(myfield='somevalue')
``:code

It is equivalent to

``
db.mytable.insert(myfield='somevalue')
``:code

and it creates a new record with field values specified by the dictionary on the right hand side.

You can update records:

``
db.mytable[id] = dict(myfield='somevalue')
``:code

which is equivalent to

``
db(db.mytable.id == id).update(myfield='somevalue')
``:code

and it updates an existing record with field values specified by the dictionary on the right hand side.

#### Fetching a ``Row``

Yet another convenient syntax is the following:

``
record = db.mytable(id)
record = db.mytable(db.mytable.id == id)
record = db.mytable(id, myfield='somevalue')
``:code

Apparently similar to ``db.mytable[id]`` the above syntax is more flexible and safer. First of all it checks whether ``id`` is an int (or ``str(id)`` is an int) and returns ``None`` if not (it never raises an exception). It also allows to specify multiple conditions that the record must meet. If they are not met, it also returns ``None``.

#### Recursive ``select``s
``recursive selects``:inxx

Consider the previous table person and a new table "thing" referencing a "person":
``
>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id', 'reference person'))
``:code

and a simple select from this table:
``
>>> things = db(db.thing).select()
``:code

which is equivalent to

``
>>> things = db(db.thing._id > 0).select()
``:code

where ``._id`` is a reference to the primary key of the table. Normally ``db.thing._id`` is the same as ``db.thing.id`` and we will assume that in most of this book. ``_id``:inxx


For each Row of things it is possible to fetch not just fields from the selected table (thing) but also from linked tables (recursively):
``
>>> for thing in things: print thing.name, thing.owner_id.name
``:code

Here ``thing.owner_id.name`` requires one database select for each thing in things and it is therefore inefficient. We suggest using joins whenever possible instead of recursive selects, nevertheless this is convenient and practical when accessing individual records.

You can also do it backwards, by selecting the things referenced by a person:

``
person =  db.person(id)
for thing in person.thing.select(orderby=db.thing.name):
    print person.name, 'owns', thing.name
``:code

In this last expressions ``person.thing`` is a shortcut for

``
db(db.thing.owner_id == person.id)
``:code

i.e. the Set of ``thing``s referenced by the current ``person``. This syntax breaks down if the referencing table has multiple references to the referenced table. In this case one needs to be more explicit and use a full Query.

[[sqltable]]
#### Serializing ``Rows`` in views

Given the following action containing a query
``SQLTABLE``:inxx

``
def index():
    return dict(rows = db(query).select())
``:code

The result of a select can be displayed in a view with the following syntax:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=rows}}
``:code

Which is equivalent to:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=SQLTABLE(rows)}}
``:code

``SQLTABLE`` converts the rows into an HTML table with a header containing the column names and one row per record. The rows are marked as alternating class "even" and class "odd". Under the hood, Rows is first converted into a SQLTABLE object (not to be confused with Table) and then serialized. The values extracted from the database are also formatted by the validators associated to the field and then escaped.

Yet it is possible and sometimes convenient to call SQLTABLE explicitly.

The SQLTABLE constructor takes the following optional arguments:

- ``linkto`` lambda function or an action to be used to link reference fields (default to None).
If you assign it a string with the name of an action, it will generate a link to that function passing it, as args, the name of the table and the id of each record (in this order). Example:
``
linkto = 'pointed_function' # generates something like <a href="pointed_function/table_name/id_value">
``:code
If you want a different link to be generated, you can specify a lambda, wich will receive as parameters, the value of the id, the type of the object (e.g. table), and the name of the object. For example, if you want to receive the args in reverse order:
``
linkto = lambda id, type, name: URL(f='pointed_function', args=[id, name])
``:code
- ``upload`` the URL or the download action to allow downloading of uploaded files (default to None)
- ``headers`` a dictionary mapping field names to their labels to be used as headers (default to ``{}``). It can also be an instruction. Currently we support ``headers='fieldname:capitalize'``.
- ``truncate`` the number of characters for truncating long values in the table (default is 16)
- ``columns`` the list of fieldnames to be shown as columns (in tablename.fieldname format).
   Those not listed are not displayed (defaults to all).
- ``**attributes`` generic helper attributes to be passed to the most external TABLE object.

Here is an example:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=SQLTABLE(rows,
            headers='fieldname:capitalize',
            truncate=100,
            upload=URL('download'))
}}
``:code

``SQLFORM.grid``:inxx ``SQLFORM.smartgrid``:inxx

------
``SQLTABLE`` is useful but there are times when one needs more. ``SQLFORM.grid`` is an extension of SQLTABLE that creates a table with search features and pagination, as well as ability to open detailed records, create, edit and delete records. ``SQLFORM.smartgrid`` is a further generalization that allows all of the above but also creates buttons to access referencing records.
------

Here is an example of usage of ``SQLFORM.grid``:

``
def index():
    return dict(grid=SQLFORM.grid(query))
``:code

and the corresponding view:

``
{{extend 'layout.html'}}
{{=grid}}
``

For working with multiple rows, ``SQLFORM.grid`` and ``SQLFORM.smartgrid`` are preferred to ``SQLTABLE`` because they are more powerful. Please see chapter 7.

[[orderby]] [[limitby]] [[distinct]]
#### ``orderby``, ``groupby``, ``limitby``, ``distinct``, ``having``,``orderby_on_limitby``,``left``,``cache``

The ``select`` command takes a number of optional arguments.

##### orderby
You can fetch the records sorted by name:

``orderby``:inxx ``groupby``:inxx ``having``:inxx
``
>>> for row in db().select(
        db.person.ALL, orderby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

You can fetch the records sorted by name in reverse order (notice the tilde):
``
>>> for row in db().select(db.person.ALL, orderby=~db.person.name):
        print row.name
Carl
Bob
Alex
``:code

You can have the fetched records appear in random order:
``
>>> for row in db().select(db.person.ALL, orderby='<random>'):
        print row.name
Carl
Alex
Bob
``:code

-----
The use of ``orderby='<random>'`` is not supported on Google NoSQL.  However, in this situation and likewise in many others where built-ins are insufficient, imports can be used:
``
import random
rows=db(...).select().sort(lambda row: random.random())
``:code
-----

You can sort the records according to multiple fields by concatenating them with a "|":
``
>>> for row in db().select(db.person.ALL, orderby=db.person.name|db.person.id):
        print row.name
Carl
Bob
Alex
``:code

##### groupby, having
Using ``groupby`` together with ``orderby``, you can group records with the same value for the specified field (this is back-end specific, and is not on the Google NoSQL):
``
>>> for row in db().select(db.person.ALL,
                           orderby=db.person.name,
                           groupby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

You can use ``having`` in conjunction with ``groupby`` to group conditionally (only those ``having`` the condition are grouped.

``
>>> print db(query1).select(db.person.ALL, groupby=db.person.name, having=query2)
``

Notice that query1 filters records to be displayed, query2 filters records to be grouped.

##### distinct
``distinct``:inxx

With the argument ``distinct=True``, you can specify that you only want to select distinct records. This has the same effect as grouping using all specified fields except that it does not require sorting. When using distinct it is important not to select ALL fields, and in particular not to select the "id" field, else all records will always be distinct.

Here is an example:
``
>>> for row in db().select(db.person.name, distinct=True):
        print row.name
Alex
Bob
Carl
``:code

Notice that ``distinct`` can also be an expression for example:
``
>>> for row in db().select(db.person.name, distinct=db.person.name):
        print row.name
Alex
Bob
Carl
``:code
##### limitby
With limitby=(min, max), you can select a subset of the records from offset=min to but not including offset=max (in this case, the first two starting at zero):

``limitby``:inxx
``
>>> for row in db().select(db.person.ALL, limitby=(0, 2)):
        print row.name
Alex
Bob
``:code

##### orderby_on_limitby
``orderby_on_limitby``:inxx
Note that the DAL defaults to implicitly adding an orderby when using a limitby.
This ensures the same query returns the same results each time, important for pagination.
But it can cause performance problems. 
use ``orderby_on_limitby = False`` to change this (this defaults to True). 

##### left
Discussed below in the section on joins

##### cache, cacheable
An example use which gives much faster selects is:
``rows = db(query).select(cache=(cache.ram, 3600), cacheable=True)``:code
See discussion on 'caching selects', below, to understand what the trade-offs are.


#### Logical operators

Queries can be combined using the binary AND operator "``&``":

``and``:inxx ``or``:inxx ``not``:inxx
``
>>> rows = db((db.person.name=='Alex') & (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
4 Alex
``:code

and the binary OR operator "``|``":
``
>>> rows = db((db.person.name == 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
1 Alex
``:code

You can negate a query (or sub-query) with the "``!=``" binary operator:
``
>>> rows = db((db.person.name != 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
2 Bob
3 Carl
``:code

or by explicit negation with the "``~``" unary operator:
``
>>> rows = db(~(db.person.name == 'Alex') | (db.person.id > 3)).select()
>>> for row in rows:
        print row.id, row.name
2 Bob
3 Carl
``:code

------
Due to Python restrictions in overloading "``and``" and "``or``" operators, these cannot be used in forming queries.  The binary operators "``&``" and "``|``" must be used instead. Note that these operators (unlike "``and``" and "``or``") have higher precedence than comparison operators, so the "extra" parentheses in the above examples are mandatory. Similarly, the unary operator "``~``" has higher precedence than comparison operators, so ``~``-negated comparisons must also be parenthesized.
------

It is also possible to build queries using in-place logical operators:

``
>>> query = db.person.name != 'Alex'
>>> query &= db.person.id > 3
>>> query |= db.person.name == 'John'
``

#### ``count``, ``isempty``, ``delete``, ``update``

You can count records in a set:

``count``:inxx ``isempty``:inxx

``
>>> print db(db.person.id > 0).count()
3
``:code

Notice that ``count`` takes an optional ``distinct`` argument which defaults to False, and it works very much like the same argument for ``select``. ``count`` has also a ``cache`` argument that works very much like the equivalent argument of the ``select`` method.

Sometimes you may need to check if a table is empty. A more efficient way than counting is using the ``isempty`` method:

``
>>> print db(db.person.id > 0).isempty()
False
``:code

or equivalently:

``
>>> print db(db.person).isempty()
False
``:code

You can delete records in a set:

``delete``:inxx
``
>>> db(db.person.id > 3).delete()
``:code

And you can update all records in a set by passing named arguments corresponding to the fields that need to be updated:

``update``:inxx
``
>>> db(db.person.id > 3).update(name='Ken')
``:code

#### Expressions

The value assigned an update statement can be an expression. For example consider this model
``
>>> db.define_table('person',
                    Field('name'),
                    Field('visits', 'integer', default=0))

>>> db(db.person.name == 'Massimo').update(visits = db.person.visits + 1)
``:code

The values used in queries can also be expressions
``
>>> db.define_table('person',
                    Field('name'),
                    Field('visits', 'integer', default=0),
                    Field('clicks', 'integer', default=0))

>>> db(db.person.visits == db.person.clicks + 1).delete()
``:code

#### ``case`` ``case``:inxx

An expression can contain a case clause for example:

``
>>> db.define_table('person', Field('name'))
>>> condition = db.person.name.startswith('M')
>>> yes_or_no = condition.case('Yes', 'No')
>>> for row in db().select(db.person.name, yes_or_no):
...     print row.person.name,  row(yes_or_no)
Max Yes
John No
``:code

#### ``update_record``

``update_record``:inxx
web2py also allows updating a single record that is already in memory using ``update_record``

``
>>> row = db(db.person.id == 2).select().first()
>>> row.update_record(name='Curt')
``:code

``update_record`` should not be confused with

``
>>> row.update(name='Curt')
``:code

because for a single row, the method ``update`` updates the row object but not the database record, as in the case of ``update_record``.

It is also possible to change the attributes of a row (one at a time) and then call ``update_record()`` without arguments to save the changes:

``
>>> row = db(db.person.id > 2).select().first()
>>> row.name = 'Curt'
>>> row.update_record() # saves above change
``:code

The ``update_record`` method is available only if the table's ``id`` field is included in the select, and ``cacheable`` is not set to ``True``.

#### Inserting and updating from a dictionary

A common issue consists of needing to insert or update records in a table where the name of the table, the field to be updated, and the value for the field are all stored in variables. For example: ``tablename``, ``fieldname``, and ``value``.

The insert can be done using the following syntax:

``
db[tablename].insert(**{fieldname:value})
``:code

The update of record with given id can be done with: ``_id``:inxx

``
db(db[tablename]._id == id).update(**{fieldname:value})
``:code

Notice we used ``table._id`` instead of ``table.id``. In this way the query works even for tables with a field of type "id" which has a name other than "id".


#### ``first`` and ``last``
``first``:inxx ``last``:inxx

Given a Rows object containing records:

``
>>> rows = db(query).select()
>>> first_row = rows.first()
>>> last_row = rows.last()
``:code

are equivalent to
``
>>> first_row = rows[0] if len(rows)>0 else None
>>> last_row = rows[-1] if len(rows)>0 else None
``:code

Notice, ``first()`` and ``last()`` allow you to obtain obviously the first and last record present in your query, but this won't mean that these records are going to be the first or last inserted records. In case you want the first or last record inputted in a given table don't forget to use ``orderby=db.table_name.id``. If you forget you will only get the first and last record returned by your query which are often in a random order determined by the backend query optimiser.

#### ``as_dict`` and ``as_list``
``as_list``:inxx ``as_dict``:inxx

A Row object can be serialized into a regular dictionary using the ``as_dict()`` method and a Rows object can be serialized into a list of dictionaries using the ``as_list()`` method. Here are some examples:
``
>>> rows = db(query).select()
>>> rows_list = rows.as_list()
>>> first_row_dict = rows.first().as_dict()
``:code

These methods are convenient for passing Rows to generic views and or to store Rows in sessions (since Rows objects themselves cannot be serialized since contain a reference to an open DB connection):
``
>>> rows = db(query).select()
>>> session.rows = rows  # not allowed!
>>> session.rows = rows.as_list()  # allowed!
``:code

#### Combining rows

Row objects can be combined at the Python level. Here we assume:

``
>>> print rows1
person.name
Max
Tim
>>> print rows2
person.name
John
Tim
``


You can do an intersection of the records in two set of rows:

``
>>> rows3 = rows1 & rows2
>>> print rows3
name
Tim
``:code

You can do a union of the records removing duplicates:

``
>>> rows3 = rows1 | rows2
>>> print rows3
name
Max
Tim
John
``:code

#### ``find``, ``exclude``, ``sort``
``find``:inxx ``exclude``:inxx ``sort``:inxx

Some times you need to perform two selects and one contains a subset of a previous select. In this case it is pointless to access the database again. The ``find``, ``exclude`` and ``sort`` objects allow you to manipulate a Rows object and generate another one without accessing the database. More specifically:
- ``find`` returns a new set of Rows filtered by a condition and leaves the original unchanged.
- ``exclude`` returns a new set of Rows filtered by a condition and removes them from the original Rows.
- ``sort`` returns a new set of Rows sorted by a condition and leaves the original unchanged.

All these methods take a single argument, a function that acts on each individual row.

Here is an example of usage:
``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name='John')
>>> db.person.insert(name='Max')
>>> db.person.insert(name='Alex')
>>> rows = db(db.person).select()
>>> for row in rows.find(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
3
>>> for row in rows.exclude(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
2
>>> for row in rows.sort(lambda row: row.name):
        print row.name
Alex
John
``:code

They can be combined:
``
>>> rows = db(db.person).select()
>>> rows = rows.find(lambda row: 'x' in row.name
                     ).sort(lambda row: row.name)
>>> for row in rows:
        print row.name
Alex
Max
``:code

Sort takes an optional argument ``reverse=True`` with the obvious meaning.

The ``find`` method has an optional limitby argument with the same syntax and functionality as the Set select ``method``.



### Other methods

#### ``update_or_insert``
``update_or_insert``:inxx

Some times you need to perform an insert only if there is no record with the same values as those being inserted.
This can be done with

``
db.define_table('person',
                Field('name'),
                Field('birthplace'))

db.person.update_or_insert(name='John', birthplace='Chicago')
``:code

The record will be inserted only if there is no other user called John born in Chicago.

You can specify which values to use as a key to determine if the record exists. For example:
``
db.person.update_or_insert(db.person.name == 'John',
                           name='John',
                           birthplace='Chicago')
``:code

and if there is John his birthplace will be updated else a new record will be created.

The selection criteria in the example above is a single field. 
It can also be a query, such as 
``
db.person.update_or_insert((db.person.name == 'John') & (db.person.birthplace == 'Chicago'),
                           name='John',
                           birthplace='Chicago',
                           pet='Rover')
``:code

#### ``validate_and_insert``, ``validate_and_update``

``validate_and_insert``:inxx ``validate_and_update``:inxx

The function

``
ret = db.mytable.validate_and_insert(field='value')
``:code

works very much like

``
id = db.mytable.insert(field='value')
``:code

except that it calls the validators for the fields before performing the insert and bails out if the validation does not pass. If validation does not pass the errors can be found in ``ret.errors``. ``ret.errors`` holds a key-value mapping where each key is the field name whose validation failed, and the value of the key is the result from the validation error (much like ``form.errors``). If it passes, the id of the new record is in ``ret.id``. Mind that normally validation is done by the form processing logic so this function is rarely needed.

Similarly

``
ret = db(query).validate_and_update(field='value')
``:code

works very much the same as

``
num = db(query).update(field='value')
``:code

except that it calls the validators for the fields before performing the update. Notice that it only works if query involves a single table. The number of updated records can be found in ``ret.updated`` and errors will be ``ret.errors``.

#### ``smart_query`` (experimental)

There are times when you need to parse a query using natural language such as

``
name contain m and age greater than 18
``

The DAL provides a method to parse this type of queries:

``
search = 'name contain m and age greater than 18'
rows = db.smart_query([db.person], search).select()
``

The first argument must be a list of tables or fields that should be allowed in the search. It raises a ``RuntimeError`` if the search string is invalid. This functionality can be used to build RESTful interfaces (see chapter 10) and it is used internally by the ``SQLFORM.grid`` and ``SQLFORM.smartgrid``.

In the smartquery search string, a field can be identified by fieldname only and or by tablename.fieldname. Strings may be delimited by double quotes if they contain spaces.

### Computed fields
``compute``:inxx

DAL fields may have a ``compute`` attribute. This must be a function (or lambda) that takes a Row object and returns a value for the field. When a new record is modified, including both insertions and updates, if a value for the field is not provided, web2py tries to compute from the other field values using the ``compute`` function. Here is an example:
``
>>> db.define_table('item',
                    Field('unit_price','double'),
                    Field('quantity','integer'),
                    Field('total_price',
                          compute=lambda r: r['unit_price'] * r['quantity']))

>>> r = db.item.insert(unit_price=1.99, quantity=5)
>>> print r.total_price
9.95
``:code

Notice that the computed value is stored in the db and it is not computed on retrieval, as in the case of virtual fields, described later. Two typical applications of computed fields are:
- in wiki applications, to store the processed input wiki text as HTML, to avoid re-processing on every request
- for searching, to compute normalized values for a field, to be used for searching.

Computed fields are evaluated in the order in which they are defined in the table definition. A computed field can refer to previously defined computed fields (new after v 2.5.1)

### Virtual fields

``virtual fields``:inxx

Virtual fields are also computed fields (as in the previous subsection) but they differ from those because they are ''virtual'' in the sense that they are not stored in the db and they are computed each time records are extracted from the database. They can be used to simplify the user's code without using additional storage but they cannot be used for searching.

#### New style virtual fields

web2py provides a new and easier way to define virtual fields and lazy virtual fields. This section is marked experimental because they APIs may still change a little from what is described here.

Here we will consider the same example as in the previous subsection. In particular we consider the following model:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))
``:code

One can define a ``total_price`` virtual field as

``
>>> db.item.total_price = \
        Field.Virtual('total_price',
                      lambda row: row.item.unit_price * row.item.quantity)
``:code

i.e. by simply defining a new field ``total_price`` to be a ``Field.Virtual``. The only argument of the constructor is a function that takes a row and returns the computed values.

A virtual field defined as the one above is automatically computed for all records when the records are selected:

``
>>> for row in db(db.item).select():
        print row.total_price
``

It is also possible to define method fields which are calculated on-demand, when called.
For example:

``
>>> db.item.discounted_total = \
        Field.Method(lambda row, discount=0.0:
                     row.item.unit_price * row.item.quantity * (1.0 - discount / 100))
``:code

In this case ``row.discounted_total`` is not a value but a function. The function takes the same arguments as the function passed to the ``Method`` constructor except for ``row`` which is implicit (think of it as ``self`` for rows objects).

The lazy field in the example above allows one to compute the total price for each ``item``:

``
>>> for row in db(db.item).select(): print row.discounted_total()
``

And it also allows to pass an optional ``discount`` percentage (15%):

``
>>> for row in db(db.item).select():
        print row.discounted_total(15)
``

Virtual and Method fields can also be defined in place when a table is defined:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'),
                    Field.Virtual('total_price',
                                  lambda row: ...),
                    Field.Method('discounted_total',
                                 lambda row, discount=0.0: ...))
``:code


------
Mind that virtual fields do not have the same attributes as the other fields (default, readable, requires, etc).  In older versions of web2py they do not appear in the list of ``db.table.fields`` and they require a special approach to display in SQLFORM.grid and SQLFORM.smartgrid. See the discussion on grids and virtual fields in the Forms chapter.
------

#### Old style virtual fields

In order to define one or more virtual fields, you can also define a container class, instantiate it and link it to a table or to a select. For example, consider the following table:

``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))
``:code

One can define a ``total_price`` virtual field as
``
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price*self.item.quantity
>>> db.item.virtualfields.append(MyVirtualFields())
``:code

Notice that each method of the class that takes a single argument (self) is a new virtual field. ``self`` refers to each one row of the select. Field values are referred by full path as in ``self.item.unit_price``. The table is linked to the virtual fields by appending an instance of the class to the table's ``virtualfields`` attribute.

Virtual fields can also access recursive fields as in
``
>>> db.define_table('item',
                    Field('unit_price', 'double'))

>>> db.define_table('order_item',
                    Field('item', 'reference item'),
                    Field('quantity', 'integer'))
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.order_item.item.unit_price * self.order_item.quantity

>>> db.order_item.virtualfields.append(MyVirtualFields())
``:code

Notice the recursive field access ``self.order_item.item.unit_price`` where ``self`` is the looping record.

They can also act on the result of a JOIN
``
>>> db.define_table('item',
                    Field('unit_price', 'double'))

>>> db.define_table('order_item',
                    Field('item', 'reference item'),
                    Field('quantity', 'integer'))

>>> rows = db(db.order_item.item == db.item.id).select()
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price * self.order_item.quantity

>>> rows.setvirtualfields(order_item=MyVirtualFields())

>>> for row in rows:
        print row.order_item.total_price
``:code

Notice how in this case the syntax is different. The virtual field accesses both ``self.item.unit_price`` and ``self.order_item.quantity`` which belong to the join select. The virtual field is attached to the rows of the table using the ``setvirtualfields`` method of the rows object. This method takes an arbitrary number of named arguments and can be used to set multiple virtual fields, defined in multiple classes, and attach them to multiple tables:
``
>>> class MyVirtualFields1(object):
        def discounted_unit_price(self):
            return self.item.unit_price * 0.90
>>> class MyVirtualFields2(object):
        def total_price(self):
            return self.item.unit_price * self.order_item.quantity

        def discounted_total_price(self):
            return self.item.discounted_unit_price * self.order_item.quantity

>>> rows.setvirtualfields(item=MyVirtualFields1(),
                          order_item=MyVirtualFields2())

>>> for row in rows:
        print row.order_item.discounted_total_price
``:code

Virtual fields can be ''lazy''; all they need to do is return a function and access it by calling the function:
``
>>> db.define_table('item',
                    Field('unit_price', 'double'),
                    Field('quantity', 'integer'))

>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            def lazy(self=self):
                return self.item.unit_price * self.item.quantity
            return lazy

>>> db.item.virtualfields.append(MyVirtualFields())

>>> for item in db(db.item).select():
        print item.lazy_total_price()
``:code

or shorter using a lambda function:
``
>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            return lambda self=self: self.item.unit_price * self.item.quantity
``:code

### One to many relation
``one to many``:inxx

To illustrate how to implement one to many relations with the web2py DAL, define another table "thing" that refers to the table "person" which we redefine here:
``
>>> db.define_table('person',
                    Field('name'),
                    format='%(name)s')

>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id', 'reference person'),
                    format='%(name)s')
``:code

Table "thing" has two fields, the name of the thing and the owner of the thing. The "owner_id" field id a reference field. A reference type can be specified in two equivalent ways:

``
Field('owner_id', 'reference person')
Field('owner_id', db.person)
``:code

The latter is always converted to the former. They are equivalent except in the case of lazy tables, self references or other types of cyclic references where the former notation is the only allowed notation.

When a field type is another table, it is intended that the field reference the other table by its id. In fact, you can print the actual type value and get:
``
>>> print db.thing.owner_id.type
reference person
``:code

Now, insert three things, two owned by Alex and one by Bob:
``
>>> db.thing.insert(name='Boat', owner_id=1)
1
>>> db.thing.insert(name='Chair', owner_id=1)
2
>>> db.thing.insert(name='Shoes', owner_id=2)
3
``:code

You can select as you did for any other table:
``
>>> for row in db(db.thing.owner_id == 1).select():
        print row.name
Boat
Chair
``:code

Because a thing has a reference to a person, a person can have many things, so a record of table person now acquires a new attribute thing, which is a Set, that defines the things of that person. This allows looping over all persons and fetching their things easily:

``referencing``:inxx
``
>>> for person in db().select(db.person.ALL):
        print person.name
        for thing in person.thing.select():
            print '    ', thing.name
Alex
     Boat
     Chair
Bob
     Shoes
Carl
``:code

#### Inner joins

Another way to achieve a similar result is by using a join, specifically an INNER JOIN. web2py performs joins automatically and transparently when the query links two or more tables as in the following example:

``Rows``:inxx ``inner join``:inxx ``join``:inxx
``
>>> rows = db(db.person.id == db.thing.owner_id).select()

>>> for row in rows:
        print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
``:code

Observe that web2py did a join, so the rows now contain two records, one from each table, linked together. Because the two records may have fields with conflicting names, you need to specify the table when extracting a field value from a row. This means that while before you could do:
``
row.name
``:code

and it was obvious whether this was the name of a person or a thing, in the result of a join you have to be more explicit and say:
``
row.person.name
``:code

or:
``
row.thing.name
``:code

There is an alternative syntax for INNER JOINS:
``
>>> rows = db(db.person).select(join=db.thing.on(db.person.id == db.thing.owner_id))

>>> for row in rows:
    print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
``:code

While the output is the same, the generated SQL in the two cases can be different. The latter syntax removes possible ambiguities when the same table is joined twice and aliased:

``
>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id1', 'reference person'),
                    Field('owner_id2', 'reference person'))

>>> rows = \
        db(db.person).select(join=[db.person.with_alias('owner_id1').on(db.person.id == db.thing.owner_id1),
                                   db.person.with_alias('owner_id2').on(db.person.id == db.thing.owner_id2)])
``

The value of ``join`` can be list of ``db.table.on(...)`` to join.

#### Left outer join

Notice that Carl did not appear in the list above because he has no things. If you intend to select on persons (whether they have things or not) and their things (if they have any), then you need to perform a LEFT OUTER JOIN. This is done using the argument "left" of the select command. Here is an example:

``Rows``:inxx ``left outer join``:inxx ``outer join``:inxx
``
>>> rows = db().select(db.person.ALL, db.thing.ALL,
                       left=db.thing.on(db.person.id == db.thing.owner_id))

>>> for row in rows:
        print row.person.name, 'has', row.thing.name

Alex has Boat
Alex has Chair
Bob has Shoes
Carl has None
``:code

where:
``
left = db.thing.on(...)
``:code

does the left join query. Here the argument of ``db.thing.on`` is the condition required for the join (the same used above for the inner join). In the case of a left join, it is necessary to be explicit about which fields to select.

Multiple left joins can be combined by passing a list or tuple of ``db.mytable.on(...)`` to the  ``left`` attribute.

#### Grouping and counting

When doing joins, sometimes you want to group rows according to certain criteria and count them. For example, count the number of things owned by every person. web2py allows this as well. First, you need a count operator. Second, you want to join the person table with the thing table by owner. Third, you want to select all rows (person + thing), group them by person, and count them while grouping:

``grouping``:inxx
``
>>> count = db.person.id.count()
>>> for row in db(db.person.id == db.thing.owner_id
                  ).select(db.person.name, count, groupby=db.person.name):
        print row.person.name, row[count]

Alex 2
Bob 1
``:code

Notice the ``count`` operator (which is built-in) is used as a field. The only issue here is in how to retrieve the information. Each row clearly contains a person and the count, but the count is not a field of a person nor is it a table. So where does it go? It goes into the storage object representing the record with a key equal to the query expression itself. The count method of the Field object has an optional ``distinct`` argument. When set to ``True`` it specifies that only distinct values of the field in question are to be counted.

### Many to many
``many-to-many``:inxx
In the previous examples, we allowed a thing to have one owner but one person could have many things. What if Boat was owned by Alex and Curt? This requires a many-to-many relation, and it is realized via an intermediate table that links a person to a thing via an ownership relation.

Here is how to do it:
``
>>> db.define_table('person',
                    Field('name'))

>>> db.define_table('thing',
                    Field('name'))

>>> db.define_table('ownership',
                    Field('person', 'reference person'),
                    Field('thing', 'reference thing'))
``:code

the existing ownership relationship can now be rewritten as:
``
>>> db.ownership.insert(person=1, thing=1)  # Alex owns Boat
>>> db.ownership.insert(person=1, thing=2)  # Alex owns Chair
>>> db.ownership.insert(person=2, thing=3)  # Bob owns Shoes

``:code

Now you can add the new relation that Curt co-owns Boat:
``
>>> db.ownership.insert(person=3, thing=1)  # Curt owns Boat too

``:code

Because you now have a three-way relation between tables, it may be convenient to define a new set on which to perform operations:
``
>>> persons_and_things = db((db.person.id == db.ownership.person) &
                            (db.thing.id == db.ownership.thing))
``:code

Now it is easy to select all persons and their things from the new Set:
``
>>> for row in persons_and_things.select():
        print row.person.name, row.thing.name

Alex Boat
Alex Chair
Bob Shoes
Curt Boat
``:code

Similarly, you can search for all things owned by Alex:
``
>>> for row in persons_and_things(db.person.name == 'Alex').select():
        print row.thing.name

Boat
Chair
``:code

and all owners of Boat:
``
>>> for row in persons_and_things(db.thing.name == 'Boat').select():
        print row.person.name

Alex
Curt
``:code

A lighter alternative to Many 2 Many relations is tagging. Tagging is discussed in the context of the ``IS_IN_DB`` validator. Tagging works even on database backends that do not support JOINs like the Google App Engine NoSQL.

### ``list:<type>`` and ``contains``
``list:string``:inxx
``list:integer``:inxx
``list:reference``:inxx
``contains``:inxx
``multiple``:inxx
``tags``:inxx

web2py provides the following special field types:

``
list:string
list:integer
list:reference <table>
``:code

They can contain lists of strings, of integers and of references respectively.

On Google App Engine NoSQL ``list:string`` is mapped into ``StringListProperty``, the other two are mapped into ``ListProperty(int)``. On relational databases they are mapped into text fields which contain the list of items separated by ``|``. For example ``[1,2,3]`` is mapped into ``|1|2|3|``.

For lists of string the items are escaped so that any ``|`` in the item is replaced by a ``||``. Anyway this is an internal representation and it is transparent to the user.

You can use ``list:string``, for example, in the following way:

``
>>> db.define_table('product',
                    Field('name'),
                    Field('colors', 'list:string'))

>>> db.product.colors.requires=IS_IN_SET(('red', 'blue', 'green'))

>>> db.product.insert(name='Toy Car', colors=['red', 'green'])

>>> products = db(db.product.colors.contains('red')).select()

>>> for item in products:
        print item.name, item.colors

Toy Car ['red', 'green']
``:code

``list:integer`` works in the same way but the items must be integers.

As usual the requirements are enforced at the level of forms, not at the level of ``insert``.

------
For ``list:<type>`` fields the ``contains(value)`` operator maps into a non trivial query that checks for lists containing the ``value``.  The ``contains`` operator also works for regular ``string`` and ``text`` fields and it maps into a ``LIKE '%value%'``.
------

The ``list:reference`` and the ``contains(value)`` operator are particularly useful to de-normalize many-to-many relations. Here is an example:

``
>>> db.define_table('tag',
                    Field('name'),
                    format='%(name)s')

>>> db.define_table('product',
                    Field('name'),
                    Field('tags', 'list:reference tag'))

>>> a = db.tag.insert(name='red')

>>> b = db.tag.insert(name='green')

>>> c = db.tag.insert(name='blue')

>>> db.product.insert(name='Toy Car', tags=[a, b, c])

>>> products = db(db.product.tags.contains(b)).select()

>>> for item in products:
        print item.name, item.tags

Toy Car [1, 2, 3]

>>> for item in products:
        print item.name, db.product.tags.represent(item.tags)

Toy Car red, green, blue
``:code

Notice that a ``list:reference tag`` field get a default constraint

``
requires = IS_IN_DB(db, 'tag.id', db.tag._format, multiple=True)
``:code

that produces a ``SELECT/OPTION`` multiple drop-box in forms.

Also notice that this field gets a default ``represent`` attribute which represents the list of references as a comma-separated list of formatted references. This is used in read forms and ``SQLTABLE``s.

-----
While ``list:reference`` has a default validator and a default representation, ``list:integer`` and ``list:string`` do not. So these two need an ``IS_IN_SET`` or an ``IS_IN_DB`` validator if you want to use them in forms.
-----


### Other operators

web2py has other operators that provide an API to access equivalent SQL operators.
Let's define another table "log" to store security events, their event_time and severity, where the severity is an integer number.

``date``:inxx ``datetime``:inxx ``time``:inxx
``
>>> db.define_table('log', Field('event'),
                           Field('event_time', 'datetime'),
                           Field('severity', 'integer'))
``:code

As before, insert a few events, a "port scan", an "xss injection" and an "unauthorized login".
For the sake of the example, you can log events with the same event_time but with different severities (1, 2, and 3 respectively).
``
>>> import datetime
>>> now = datetime.datetime.now()
>>> print db.log.insert(
        event='port scan', event_time=now, severity=1)
1
>>> print db.log.insert(
        event='xss injection', event_time=now, severity=2)
2
>>> print db.log.insert(
        event='unauthorized login', event_time=now, severity=3)
3
``:code

#### ``like``, ``ilike``, ``regexp``, ``startswith``, ``endswith``, ``contains``, ``upper``, ``lower``

``like``:inxx ``ilike``:inxx ``startswith``:inxx ``endswith``:inxx ``regexp``:inxx
``contains``:inxx ``upper``:inxx ``lower``:inxx

Fields have a like operator that you can use to match strings:

``
>>> for row in db(db.log.event.like('port%')).select():
        print row.event
port scan
``:code

Here "port%" indicates a string starting with "port". The percent sign character, "%", is a wild-card character that means "any sequence of characters".

The like operator maps to the LIKE word in ANSI-SQL. LIKE is case-sensitive in most databases, and depends on the collation of the database itself. The ``like`` method is hence case-sensitive but it can be made case-insensitive with

``
db.mytable.myfield.like('value', case_sensitive=False)
``:code


web2py also provides some shortcuts:

``
db.mytable.myfield.startswith('value')
db.mytable.myfield.endswith('value')
db.mytable.myfield.contains('value')
``:code

which are roughly equivalent respectively to

``
db.mytable.myfield.like('value%')
db.mytable.myfield.like('%value')
db.mytable.myfield.like('%value%')
``:code

Notice that ``contains`` has a special meaning for ``list:<type>`` fields and it was discussed in a previous section.

The ``contains`` method can also be passed a list of values and an optional boolean argument ``all`` to search for records that contain all values:

``
db.mytable.myfield.contains(['value1', 'value2'], all=True)
``
or any value from the list
``
db.mytable.myfield.contains(['value1', 'value2'], all=False)
``

There is a also a ``regexp`` method that works like the ``like`` method but allows regular expression syntax for the look-up expression. It is only supported by PostgreSQL, MySQL, Oracle and SQLite (with different degree of support).

The ``upper`` and ``lower`` methods allow you to convert the value of the field to upper or lower case, and you can also combine them with the like operator:

``upper``:inxx ``lower``:inxx
``
>>> for row in db(db.log.event.upper().like('PORT%')).select():
        print row.event

port scan
``:code

#### ``year``, ``month``, ``day``, ``hour``, ``minutes``, ``seconds``
``hour``:inxx ``minutes``:inxx ``seconds``:inxx ``day``:inxx ``month``:inxx ``year``:inxx

The date and datetime fields have day, month and year methods. The datetime and time fields have hour, minutes and seconds methods. Here is an example:

``
>>> for row in db(db.log.event_time.year() == 2013).select():
        print row.event

port scan
xss injection
unauthorized login
``:code

#### ``belongs``

The SQL IN operator is realized via the belongs method which returns true when the field value belongs to the specified set (list or tuples):

``belongs``:inxx
``
>>> for row in db(db.log.severity.belongs((1, 2))).select():
        print row.event

port scan
xss injection
``:code

The DAL also allows a nested select as the argument of the belongs operator. The only caveat is that the nested select has to be a ``_select``, not a ``select``, and only one field has to be selected explicitly, the one that defines the set.

``nested select``:inxx
``
>>> bad_days = db(db.log.severity == 3)._select(db.log.event_time)

>>> for row in db(db.log.event_time.belongs(bad_days)).select():
        print row.event

port scan
xss injection
unauthorized login
``:code

In those cases where a nested select is required and the look-up field is a reference we can also use a query as argument. For example:

``
db.define_table('person', Field('name'))
db.define_table('thing',
                Field('name'),
                Field('owner_id', 'reference thing'))

db(db.thing.owner_id.belongs(db.person.name == 'Jonathan')).select()
``:code

In this case it is obvious that the next select only needs the field referenced by the ``db.thing.owner_id`` field so we do not need the more verbose ``_select`` notation.

``nested_select``:inxx

A nested select can also be used as insert/update value but in this case the syntax is different:

``
lazy = db(db.person.name == 'Jonathan').nested_select(db.person.id)

db(db.thing.id == 1).update(owner_id = lazy)
``:code

In this case ``lazy`` is a nested expression that computes the ``id`` of person "Jonathan". The two lines result in one single SQL query.

#### ``sum``, ``avg``, ``min``, ``max`` and ``len``

``sum``:inxx ``avg``:inxx ``min``:inxx ``max``:inxx
Previously, you have used the count operator to count records. Similarly, you can use the sum operator to add (sum) the values of a specific field from a group of records. As in the case of count, the result of a sum is retrieved via the store object:
``
>>> sum = db.log.severity.sum()
>>> print db().select(sum).first()[sum]
6
``:code

You can also use ``avg``, ``min``, and ``max`` to the average, minimum, and maximum value respectively for the selected records. For example:

``
>>> max = db.log.severity.max()
>>> print db().select(max).first()[max]
3
``:code

``.len()`` computes the length of a string, text or boolean fields.

Expressions can be combined to form more complex expressions. For example here we are computing the sum of the length of all the severity strings in the logs, increased of one:

``
>>> sum = (db.log.severity.len() + 1).sum()
>>> print db().select(sum).first()[sum]
``:code

#### Substrings

One can build an expression to refer to a substring. For example, we can group things whose name starts with the same three characters and select only one from each group:

``
db(db.thing).select(distinct = db.thing.name[:3])
``:code


#### Default values with ``coalesce`` and ``coalesce_zero``

There are times when you need to pull a value from database but also need a default values if the value for a record is set to NULL. In SQL there is a keyword, ``COALESCE``, for this. web2py has an equivalent ``coalesce`` method:

``
>>> db.define_table('sysuser', Field('username'), Field('fullname'))
>>> db.sysuser.insert(username='max', fullname='Max Power')
>>> db.sysuser.insert(username='tim', fullname=None)
print db(db.sysuser).select(db.sysuser.fullname.coalesce(db.sysuser.username))
"COALESCE(sysuser.fullname, sysuser.username)"
Max Power
tim
``

Other times you need to compute a mathematical expression but some fields have a value set to None while it should be zero.
``coalesce_zero`` comes to the rescue by defaulting None to zero in the query:

``
>>> db.define_table('sysuser', Field('username'), Field('points'))
>>> db.sysuser.insert(username='max', points=10)
>>> db.sysuser.insert(username='tim', points=None)
>>> print db(db.sysuser).select(db.sysuser.points.coalesce_zero().sum())
"SUM(COALESCE(sysuser.points,0))"
10
``

### Generating raw sql
``raw SQL``:inxx

Sometimes you need to generate the SQL but not execute it. This is easy to do with web2py since every command that performs database IO has an equivalent command that does not, and simply returns the SQL that would have been executed. These commands have the same names and syntax as the functional ones, but they start with an underscore:

Here is ``_insert`` ``_insert``:inxx
``
>>> print db.person._insert(name='Alex')
INSERT INTO person(name) VALUES ('Alex');
``:code

Here is ``_count`` ``_count``:inxx
``
>>> print db(db.person.name == 'Alex')._count()
SELECT count(*) FROM person WHERE person.name='Alex';
``:code

Here is ``_select`` ``_select``:inxx
``
>>> print db(db.person.name == 'Alex')._select()
SELECT person.id, person.name FROM person WHERE person.name='Alex';
``:code

Here is ``_delete`` ``_delete``:inxx
``
>>> print db(db.person.name == 'Alex')._delete()
DELETE FROM person WHERE person.name='Alex';
``:code

And finally, here is ``_update`` ``_update``:inxx
``
>>> print db(db.person.name == 'Alex')._update()
UPDATE person SET  WHERE person.name='Alex';
``:code

-----
Moreover you can always use ``db._lastsql`` to return the most recent
SQL code, whether it was executed manually using executesql or was SQL
generated by the DAL.
-----

### Exporting and importing data
``export``:inxx ``import``:inxx

#### CSV (one Table at a time)

When a Rows object is converted to a string it is automatically
serialized in CSV:

``csv``:inxx
``
>>> rows = db(db.person.id == db.thing.owner_id).select()
>>> print rows

person.id, person.name, thing.id, thing.name, thing.owner_id
1, Alex, 1, Boat, 1
1, Alex, 2, Chair, 1
2, Bob, 3, Shoes, 2
``:code

You can serialize a single table in CSV and store it in a file "test.csv":
``
>>> open('test.csv', 'wb').write(str(db(db.person.id).select()))
``:code

This is equivalent to

``
>>> rows = db(db.person.id).select()
>>> rows.export_to_csv_file(open('test.csv', 'wb'))
``:code

You can read the CSV file back with:
``
>>> db.person.import_from_csv_file(open('test.csv', 'r'))
``:code

When importing, web2py looks for the field names in the CSV header. In this example, it finds two columns: "person.id" and "person.name". It ignores the "person." prefix, and it ignores the "id" fields. Then all records are appended and assigned new ids. Both of these operations can be performed via the appadmin web interface.

#### CSV (all tables at once)

In web2py, you can backup/restore an entire database with two commands:

To export:
``
>>> db.export_to_csv_file(open('somefile.csv', 'wb'))
``:code

To import:
``
>>> db.import_from_csv_file(open('somefile.csv', 'rb'))
``:code

This mechanism can be used even if the importing database is of a different type than the exporting database. The data is stored in "somefile.csv" as a CSV file where each table starts with one line that indicates the tablename, and another line with the fieldnames:
``
TABLE tablename
field1, field2, field3, ...
``:code

Two tables are separated ``\r\n\r\n``. The file ends with the line
``
END
``:code

The file does not include uploaded files if these are not stored in the database. In any case it is easy enough to zip the "uploads" folder separately.

When importing, the new records will be appended to the database if it is not empty. In general the new imported records will not have the same record id as the original (saved) records but web2py will restore references so they are not broken, even if the id values may change.

If a table contains a field called
"uuid", this field will be used to identify duplicates.  Also, if an
imported record has the same "uuid" as an existing record, the
previous record will be updated.

#### CSV and remote database synchronization

Consider the following model:
``
db = DAL('sqlite:memory:')
db.define_table('person',
                Field('name'),
                format='%(name)s')
db.define_table('thing',
                Field('owner_id', 'reference person'),
                Field('name'),
                format='%(name)s')

if not db(db.person).count():
    id = db.person.insert(name="Massimo")
    db.thing.insert(owner_id=id, name="Chair")
``:code

Each record is identified by an ID and referenced by that ID. If you
have two copies of the database used by distinct web2py installations,
the ID is unique only within each database and not across the databases.
This is a problem when merging records from different databases.

In order to make a record uniquely identifiable across databases, they
must:
- have a unique id (UUID),
- have an event_time (to figure out which one is more recent if multiple copies),
- reference the UUID instead of the id.

This can be achieved without modifying web2py. Here is what to do:

Change the above model into:

``
db.define_table('person',
                Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
                Field('modified_on', 'datetime', default=request.now),
                Field('name'),
                format='%(name)s')

db.define_table('thing',
                Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
                Field('modified_on', 'datetime', default=request.now),
                Field('owner_id', length=64),
                Field('name'),
                format='%(name)s')

db.thing.owner_id.requires = IS_IN_DB(db,'person.uuid','%(name)s')

if not db(db.person.id).count():
    id = uuid.uuid4()
    db.person.insert(name="Massimo", uuid=id)
    db.thing.insert(owner_id=id, name="Chair")
``:code

-------
Notice that in the above table definitions, the default value for the two ``uuid`` fields is set to a lambda function, which returns a UUID (converted to a string). The lambda function is called once for each record inserted, ensuring that each record gets a unique UUID, even if multiple records are inserted in a single transaction.
-------

Create a controller action to export the database:

``
def export():
    s = StringIO.StringIO()
    db.export_to_csv_file(s)
    response.headers['Content-Type'] = 'text/csv'
    return s.getvalue()
``:code

Create a controller action to import a saved copy of the other database and sync records:

``
def import_and_sync():
    form = FORM(INPUT(_type='file', _name='data'), INPUT(_type='submit'))
    if form.process().accepted:
        db.import_from_csv_file(form.vars.data.file,unique=False)
        # for every table
        for table in db.tables:
            # for every uuid, delete all but the latest
            items = db(db[table]).select(db[table].id,
                                         db[table].uuid,
                                         orderby=db[table].modified_on,
                                         groupby=db[table].uuid)
            for item in items:
                db((db[table].uuid==item.uuid) &
                   (db[table].id!=item.id)).delete()
    return dict(form=form)
``:code

Optionally you should create an index manually to make the search by uuid faster.


``XML-RPC``:inxx
Alternatively, you can use XML-RPC to export/import the file.

If the records reference uploaded files, you also need to export/import the content of the uploads folder. Notice that files therein are already labeled by UUIDs so you do not need to worry about naming conflicts and references.

#### HTML and XML (one Table at a time)

``Rows objects``:inxx
Rows objects also have an ``xml`` method (like helpers) that serializes it to XML/HTML:

``HTML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print rows.xml()
<table>
  <thead>
    <tr>
      <th>person.id</th>
      <th>person.name</th>
      <th>thing.id</th>
      <th>thing.name</th>
      <th>thing.owner_id</th>
    </tr>
  </thead>
  <tbody>
    <tr class="even">
      <td>1</td>
      <td>Alex</td>
      <td>1</td>
      <td>Boat</td>
      <td>1</td>
    </tr>
    ...
  </tbody>
</table>
``:code

``Rows custom tags``:inxx
If you need to serialize the Rows in any other XML format with custom tags, you can easily do that using the universal TAG helper and the * notation:
``XML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print TAG.result(*[TAG.row(*[TAG.field(r[f], _name=f) for f in db.person.fields]) for r in rows])

<result>
  <row>
    <field name="id">1</field>
    <field name="name">Alex</field>
  </row>
  ...
</result>
``:code

#### Data representation

``export_to_csv_file``:inxx
The ``export_to_csv_file`` function accepts a keyword argument named ``represent``. When ``True`` it will use the columns ``represent`` function while exporting the data instead of the raw data.

``colnames``:inxx
The function also accepts a keyword argument named ``colnames`` that should contain a list of column names one wish to export. It defaults to all columns.

Both ``export_to_csv_file`` and ``import_from_csv_file`` accept keyword arguments that tell the csv parser the format to save/load the files:
- ``delimiter``: delimiter to separate values (default ',')
- ``quotechar``: character to use to quote string values (default to double quotes)
- ``quoting``: quote system (default ``csv.QUOTE_MINIMAL``)

Here is some example usage:
``
>>> import csv
>>> rows = db(query).select()
>>> rows.export_to_csv_file(open('/tmp/test.txt', 'w'),
                            delimiter='|',
                            quotechar='"',
                            quoting=csv.QUOTE_NONNUMERIC)
``:code

Which would render something similar to
``
"hello"|35|"this is the text description"|"2013-03-03"
``:code

For more information consult the official Python documentation ``quoteall``:cite

### Caching selects

The select method also takes a cache argument, which defaults to None. For caching purposes, it should be set to a tuple where the first element is the cache model (cache.ram, cache.disk, etc.), and the second element is the expiration time in seconds.

In the following example, you see a controller that caches a select on the previously defined db.log table. The actual select fetches data from the back-end database no more frequently than once every 60 seconds and stores the result in cache.ram. If the next call to this controller occurs in less than 60 seconds since the last database IO, it simply fetches the previous data from cache.ram.

``cache select``:inxx
``
def cache_db_select():
    logs = db().select(db.log.ALL, cache=(cache.ram, 60))
    return dict(logs=logs)
``:code

``cacheable``:inxx

The ``select`` method has an optional ``cacheable`` argument, normally set to ``False``. When ``cacheable=True`` the resulting ``Rows`` is serializable but The ``Row``s lack ``update_record`` and ``delete_record`` methods.

If you do not need these methods you can speed up selects a lot by setting the cacheable attribute:

``
rows = db(query).select(cacheable=True)
``:code

When the ``cache`` argument is set but ``cacheable=False`` (default) only the database results are cached, not the actual Rows object. When the ``cache`` argument is used in conjunction with ``cacheable=True`` the entire Rows object is cached and this results in much faster caching:

``
rows = db(query).select(cache=(cache.ram,3600),cacheable=True)
``:code

### Self-Reference and aliases

``self reference``:inxx
``alias``:inxx
It is possible to define tables with fields that refer to themselves, here is an example:
``reference table``:inxx
``
db.define_table('person',
                Field('name'),
                Field('father_id', 'reference person'),
                Field('mother_id', 'reference person'))
``:code

Notice that the alternative notation of using a table object as field type will fail in this case, because it uses a variable ``db.person`` before it is defined:
``
db.define_table('person',
                Field('name'),
                Field('father_id', db.person),  # wrong!
                Field('mother_id', db.person))  # wrong!
``:code

In general ``db.tablename`` and ``"reference tablename"`` are equivalent field types, but the latter is the only one allowed for self.references.

``with_alias``:inxx
If the table refers to itself, then it is not possible to perform a JOIN to select a person and its parents without use of the SQL "AS" keyword. This is achieved in web2py using the ``with_alias``. Here is an example:
``
>>> Father = db.person.with_alias('father')
>>> Mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father_id=1, mother_id=2)
3
>>> rows = db().select(db.person.name, Father.name, Mother.name,
                       left=(Father.on(Father.id == db.person.father_id),
                             Mother.on(Mother.id == db.person.mother_id)))

>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name

Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

Notice that we have chosen to make a distinction between:
- "father_id": the field name used in the table "person";
- "father": the alias we want to use for the table referenced by the above field; this is communicated to the database;
- "Father": the variable used by web2py to refer to that alias.

The difference is subtle, and there is nothing wrong in using the same name for the three of them:
``
db.define_table('person',
                Field('name'),
                Field('father', 'reference person'),
                Field('mother', 'reference person'))

>>> father = db.person.with_alias('father')
>>> mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father=1, mother=2)
3
>>> rows = db().select(db.person.name, father.name, mother.name,
                       left=(father.on(father.id==db.person.father),
                             mother.on(mother.id==db.person.mother)))

>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name

Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

But it is important to have the distinction clear in order to build correct queries.

### Advanced features

#### Table inheritance
``inheritance``:inxx

It is possible to create a table that contains all the fields from another table. It is sufficient to pass the other table in place of a field to ``define_table``. For example
``
db.define_table('person', Field('name'))
db.define_table('doctor', db.person, Field('specialization'))
``:code

``dummy table``:inxx
It is also possible to define a dummy table that is not stored in a database in order to reuse it in multiple other places. For example:

``
signature = db.Table(db, 'signature',
                     Field('created_on', 'datetime', default=request.now),
                     Field('created_by', db.auth_user, default=auth.user_id),
                     Field('updated_on', 'datetime', update=request.now),
                     Field('updated_by', db.auth_user, update=auth.user_id))

db.define_table('payment', Field('amount', 'double'), signature)
``:code

This example assumes that standard web2py authentication is enabled.

Notice that if you use ``Auth`` web2py already creates one such table for you:

``
auth = Auth(db)
db.define_table('payment', Field('amount', 'double'), auth.signature)
``

When using table inheritance, if you want the inheriting table to inherit validators, be sure to define the validators of the parent table before defining the inheriting table.

#### ``filter_in`` and ``filter_out``
``filter_in``:inxx ``filter_out``:inxx

It is possible to define a filter for each field to be called before a value is inserted into the database for that field and after a value is retrieved from the database.

Imagine for example that you want to store a serializable Python data structure in a field in the json format. Here is how it could be accomplished:

``
>>> from simplejson import loads, dumps
>>> db.define_table('anyobj',
                    Field('name'),
                    Field('data', 'text'))

>>> db.anyobj.data.filter_in = lambda obj, dumps=dumps: dumps(obj)
>>> db.anyobj.data.filter_out = lambda txt, loads=loads: loads(txt)
>>> myobj = ['hello', 'world', 1, {2: 3}]
>>> id = db.anyobj.insert(name='myobjname', data=myobj)
>>> row = db.anyobj(id)
>>> row.data
['hello', 'world', 1, {2: 3}]
``:code

Another way to accomplish the same is by using a Field of type ``SQLCustomType``, as discussed later.

#### callbacks on record insert, delete and update

``_before_insert``:inxx
``_after_insert``:inxx
``_before_update``:inxx
``_after_update``:inxx
``_before_delete``:inxx
``_after_delete``:inxx

Web2py provides a mechanism to register callbacks to be called before and/or after insert, update and delete of records.

Each table stores six lists of callbacks:

``
db.mytable._before_insert
db.mytable._after_insert
db.mytable._before_update
db.mytable._after_update
db.mytable._before_delete
db.mytable._after_delete
``:code

You can register callback function by appending it the corresponding function to one of those lists. The caveat is that depending on the functionality, the callback has different signature.

This is best explained via some examples.

``
>>> db.define_table('person', Field('name'))
>>> def pprint(*args): print args
>>> db.person._before_insert.append(lambda f: pprint(f))
>>> db.person._after_insert.append(lambda f, id: pprint(f, id))
>>> db.person._before_update.append(lambda s, f: pprint(s, f))
>>> db.person._after_update.append(lambda s, f: pprint(s, f))
>>> db.person._before_delete.append(lambda s: pprint(s))
>>> db.person._after_delete.append(lambda s: pprint(s))
``:code

Here ``f`` is a dict of fields passed to insert or update, ``id`` is the id of the newly inserted record, ``s`` is the Set object used for update or delete.

``
>>> db.person.insert(name='John')
({'name': 'John'},)
({'name': 'John'}, 1)
>>> db(db.person.id==1).update(name='Tim')
(<Set (person.id = 1)>, {'name': 'Tim'})
(<Set (person.id = 1)>, {'name': 'Tim'})
>>> db(db.person.id==1).delete()
(<Set (person.id = 1)>,)
(<Set (person.id = 1)>,)
``:code

The return values of these callback should be ``None`` or ``False``. If any of the ``_before_*`` callback returns a ``True`` value it will abort the actual insert/update/delete operation.

``update_naive``:inxx

Some times a callback may need to perform an update in the same or a different table and one wants to avoid callbacks calling themselves recursively.

For this purpose there the Set objects have an ``update_naive`` method that works like ``update`` but ignores before and after callbacks.

##### Database cascades
Database schema can define relationships which trigger deletions of related records, known as cascading. The DAL is not informed when a record is deleted due to a cascade. So an ``on_delete`` trigger will not be called due a cascade-deletion. 

[[versioning]]
#### Record versioning
``_enable_record_versioning``:inxx

It is possible to ask web2py to save every copy of a record when the record is individually modified. There are different ways to do it and it can be done for all tables at once using the syntax:

``
auth.enable_record_versioning(db)
``:code

this requires Auth and it is discussed in the chapter about authentication.
It can also be done for each individual table as discussed below.

Consider the following table:

``
db.define_table('stored_item',
                Field('name'),
                Field('quantity', 'integer'),
                Field('is_active', 'boolean',
                      writable=False, readable=False, default=True))
``:code

Notice the hidden boolean field called ``is_active`` and defaulting to
True.

We can tell web2py to create a new table (in the same or a different database) and store all previous versions of each record in the table, when modified.

This is done in the following way:
``
db.stored_item._enable_record_versioning()
``:code

or in a more verbose syntax:

``
db.stored_item._enable_record_versioning(archive_db=db,
                                         archive_name='stored_item_archive',
                                         current_record='current_record',
                                         is_active='is_active')
``

The ``archive_db=db`` tells web2py to store the archive table in the same database as the ``stored_item`` table. The ``archive_name`` sets the name for the archive table. The archive table has the same fields as the original table ``stored_item`` except that unique fields are no longer unique (because it needs to store multiple versions) and has an extra field which name is specified by ``current_record`` and which is a reference to the current record in the ``stored_item`` table.

When records are deleted, they are not really deleted. A deleted record is copied in the ``stored_item_archive`` table (like when it is modified) and the ``is_active`` field is set to False. By enabling record versioning web2py sets a ``custom_filter`` on this table that hides all records in table ``stored_item`` where the ``is_active`` field is set to False. The ``is_active`` parameter in the ``_enable_record_versioning`` method allows to specify the name of the field used by the ``custom_filter`` to determine if the field was deleted or not.

``custom_filter``s are ignored by the appadmin interface.

#### Common fields and multi-tenancy
``common fields``:inxx
``multi tenancy``:inxx

``db._common_fields`` is a list of fields that should belong to all the tables. This list can also contain tables and it is understood as all fields from the table. For example occasionally you find yourself in need to add a signature to all your tables but the ```auth`` tables. In this case, after you ``db.define_tables()`` but before defining any other table, insert

``
db._common_fields.append(auth.signature)
``

One field is special: "request_tenant".
This field does not exist but you can create it and add it to any of your tables (or all of them):

``
db._common_fields.append(Field('request_tenant',
                               default=request.env.http_host,
                               writable=False))
``

For every table with a field called ``db._request_tenant``, all records for all queries are always automatically filtered by:

``
db.table.request_tenant == db.table.request_tenant.default
``:code

and for every record inserted, this field is set to the default value.
In the example above we have chosen
``
default = request.env.http_host
``
i.e. we have chose to ask our app to filter all tables in all queries with
``
db.table.request_tenant == request.env.http_host
``

This simple trick allow us to turn any application into a multi-tenant application. i.e. even if we run one instance of the app and we use one single database, if the app is accessed under two or more domains (in the example the domain name is retrieved from ``request.env.http_host``) the visitors will see different data depending on the domain. Think of running multiple web stores under different domains with one app and one database.

You can turn off multi tenancy filters using: ``ignore_common_filters``:inxx
``
rows = db(query, ignore_common_filters=True).select()
``:code

#### Common filters

A common filter is a generalization of the above multi-tenancy idea.
It provides an easy way to prevent repeating of the same query.
Consider for example the following table:

``
db.define_table('blog_post',
                Field('subject'),
                Field('post_text', 'text'),
                Field('is_public', 'boolean'),
                common_filter = lambda query: db.blog_post.is_public==True)
``

Any select, delete or update in this table, will include only public blog posts. The attribute can also be changed in controllers:

``
db.blog_post._common_filter = lambda query: db.blog_post.is_public == True
``

It serves both as a way to avoid repeating the "db.blog_post.is_public==True" phrase in each blog post search, and also as a security enhancement, that prevents you from forgetting to disallow viewing of non-public posts.

In case you actually do want items left out by the common filter (for example, allowing the admin to see non-public posts), you can either remove the filter:
``
db.blog_post._common_filter = None
``
or ignore it:
``
db(query, ignore_common_filters=True).select(...)
``

#### Custom ``Field`` types (experimental)

``SQLCustomType``:inxx

Aside for using ``filter_in`` and ``filter_out``, it is possible to define new/custom field types.
For example we consider here a field that contains binary data in compressed form:

``
from gluon.dal import SQLCustomType
import zlib

compressed = SQLCustomType(type ='text',
                           native='text',
                           encoder=(lambda x: zlib.compress(x or '')),
                           decoder=(lambda x: zlib.decompress(x)))

db.define_table('example', Field('data', type=compressed))
``:code

``SQLCustomType`` is a field type factory. Its ``type`` argument must be one of the standard web2py types. It tells web2py how to treat the field values at the web2py level. ``native`` is the type of the field as far as the database is concerned. Allowed names depend on the database engine. ``encoder`` is an optional transformation function applied when the data is stored and ``decoder`` is the optional reversed transformation function.

This feature is marked as experimental. In practice it has been in web2py for a long time and it works but it can make the code not portable, for example when the native type is database specific. It does not work on Google App Engine NoSQL.

#### Using DAL without define tables

The DAL can be used from any Python program simply by doing this:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite',folder='path/to/app/databases')
``:code

i.e. import the DAL, Field, connect and specify the folder which contains the .table files (the app/databases folder).

To access the data and its attributes we still have to define all the tables we are going to access with ``db.define_tables(...)``.

If we just need access to the data but not to the web2py table attributes, we get away without re-defining the tables but simply asking web2py to read the necessary info from the metadata in the .table files:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite', folder='path/to/app/databases', auto_import=True))
``:code

This allows us to access any ``db.table`` without need to re-define it.

#### PostGIS, SpatiaLite, and MS Geo (experimental)

``PostGIS``:inxx ``StatiaLite``:inxx ``Geo Extensions``:inxx
``geometry``:inxx ``geoPoint``:inxx ``geoLine``:inxx ``geoPolygon``:inxx

The DAL supports geographical APIs using PostGIS (for PostgreSQL), spatialite (for SQLite), and MSSQL and Spatial Extensions. This is a feature that was sponsored by the Sahana project and implemented by Denes Lengyel.

DAL provides geometry and geography fields types and the following functions:

``st_asgeojson``:inxx ``st_astext``:inxx ``st_contains``:inxx
``st_distance``:inxx ``st_equals``:inxx ``st_intersects``:inxx ``st_overlaps``:inxx
``st_simplify``:inxx ``st_touches``:inxx ``st_within``:inxx

``
st_asgeojson (PostGIS only)
st_astext
st_contains
st_distance
st_equals
st_intersects
st_overlaps
st_simplify (PostGIS only)
st_touches
st_within
st_x
st_y
``

Here are some examples:

``
from gluon.dal import DAL, Field, geoPoint, geoLine, geoPolygon
db = DAL("mssql://user:pass@host:db")
sp = db.define_table('spatial', Field('loc', 'geometry()'))
``:code

Below we insert a point, a line, and a polygon:
``
sp.insert(loc=geoPoint(1, 1))
sp.insert(loc=geoLine((100, 100), (20, 180), (180, 180)))
sp.insert(loc=geoPolygon((0, 0), (150, 0), (150, 150), (0, 150), (0, 0)))
``:code

Notice that
``
rows = db(sp.id > 0).select()
``:code

Always returns the geometry data serialized as text.
You can also do the same more explicitly using ``st_astext()``:

``
print db(sp.id>0).select(sp.id, sp.loc.st_astext())
spatial.id,spatial.loc.STAsText()
1, "POINT (1 2)"
2, "LINESTRING (100 100, 20 180, 180 180)"
3, "POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

You can ask for the native representation by using ``st_asgeojson()`` (in PostGIS only):

``
print db(sp.id>0).select(sp.id, sp.loc.st_asgeojson().with_alias('loc'))
spatial.id,loc
1, [1, 2]
2, [[100, 100], [20 180], [180, 180]]
3, [[[0, 0], [150, 0], [150, 150], [0, 150], [0, 0]]]
``:code

(notice an array is a point, an array of arrays is a line, and an array of array of arrays is a polygon).

Here are example of how to use geographical functions:

``
query = sp.loc.st_intersects(geoLine((20, 120), (60, 160)))
query = sp.loc.st_overlaps(geoPolygon((1, 1), (11, 1), (11, 11), (11, 1), (1, 1)))
query = sp.loc.st_contains(geoPoint(1, 1))
print db(query).select(sp.id, sp.loc)
spatial.id, spatial.loc
3,"POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

Computed distances can also be retrieved as floating point numbers:

``
dist = sp.loc.st_distance(geoPoint(-1,2)).with_alias('dist')
print db(sp.id>0).select(sp.id, dist)
spatial.id, dist
1 2.0
2 140.714249456
3 1.0
``:code

#### Copy data from one db into another

Consider the situation in which you have been using the following database:

``
db = DAL('sqlite://storage.sqlite')
``

and you wish to move to another database using a different connection string:

``
db = DAL('postgres://username:password@localhost/mydb')
``

Before you switch, you want to move the data and rebuild all the metadata for the new database. We assume the new database to exist but we also assume it is empty.

Web2py provides a script that does this work for you:

``
cd web2py
python scripts/cpdb.py \\
   -f applications/app/databases \\
   -y 'sqlite://storage.sqlite' \\
   -Y 'postgres://username:password@localhost/mydb' \\
   -d ../gluon
``

After running the script you can simply switch the connection string in the model and everything should work out of the box. The new data should be there.

This script provides various command line options that allows you to move data from one application to another, move all tables or only some tables, clear the data in the tables. For more info try:

``
python scripts/cpdb.py -h
``

#### Note on new DAL and adapters

The source code of the Database Abstraction Layer was completely rewritten in 2010. While it stays backward compatible, the rewrite made it more modular and easier to extend. Here we explain the main logic.

The file "gluon/dal.py" defines, among other, the following classes.

``
ConnectionPool
BaseAdapter extends ConnectionPool
Row
DAL
Reference
Table
Expression
Field
Query
Set
Rows
``

Their use has been explained in the previous sections, except for ``BaseAdapter``. When the methods of a ``Table`` or ``Set`` object need to communicate with the database they delegate to methods of the adapter the task to generate the SQL and or the function call.

For example:

``
db.mytable.insert(myfield='myvalue')
``

calls

``
Table.insert(myfield='myvalue')
``

which delegates the adapter by returning:

``
db._adapter.insert(db.mytable, db.mytable._listify(dict(myfield='myvalue')))
``

Here ``db.mytable._listify`` converts the dict of arguments into a list of ``(field,value)`` and calls the ``insert`` method of the ``adapter``. ``db._adapter`` does more or less the following:

``
query = db._adapter._insert(db.mytable, list_of_fields)
db._adapter.execute(query)
``

where the first line builds the query and the second executes it.

``BaseAdapter`` defines the interface for all adapters.

"gluon/dal.py" at the moment of writing this book, contains the following adapters:

``
SQLiteAdapter extends BaseAdapter
JDBCSQLiteAdapter extends SQLiteAdapter
MySQLAdapter extends BaseAdapter
PostgreSQLAdapter extends BaseAdapter
JDBCPostgreSQLAdapter extends PostgreSQLAdapter
OracleAdapter extends BaseAdapter
MSSQLAdapter extends BaseAdapter
MSSQL2Adapter extends MSSQLAdapter
MSSQL3Adapter extends MSSQLAdapter
MSSQL4Adapter extends MSSQLAdapter
FireBirdAdapter extends BaseAdapter
FireBirdEmbeddedAdapter extends FireBirdAdapter
InformixAdapter extends BaseAdapter
DB2Adapter extends BaseAdapter
IngresAdapter extends BaseAdapter
IngresUnicodeAdapter extends IngresAdapter
GoogleSQLAdapter extends MySQLAdapter
NoSQLAdapter extends BaseAdapter
GoogleDatastoreAdapter extends NoSQLAdapter
CubridAdapter extends MySQLAdapter (experimental)
TeradataAdapter extends DB2Adapter (experimental)
SAPDBAdapter extends BaseAdapter (experimental)
CouchDBAdapter extends NoSQLAdapter (experimental)
IMAPAdapter extends NoSQLAdapter (experimental)
MongoDBAdapter extends NoSQLAdapter (experimental)
VerticaAdapter extends MSSQLAdapter (experimental)
SybaseAdapter extends MSSQLAdapter (experimental)
``

which override the behavior of the ``BaseAdapter``.

Each adapter has more or less this structure:

``
class MySQLAdapter(BaseAdapter):

    # specify a diver to use
    driver = globals().get('pymysql', None)

    # map web2py types into database types
    types = {
        'boolean': 'CHAR(1)',
        'string': 'VARCHAR(%(length)s)',
        'text': 'LONGTEXT',
        ...
        }

    # connect to the database using driver
    def __init__(self, db, uri, pool_size=0, folder=None, db_codec ='UTF-8',
                 credential_decoder=lambda x:x, driver_args={},
                 adapter_args={}):
        # parse uri string and store parameters in driver_args
        ...
        # define a connection function
        def connect(driver_args=driver_args):
            return self.driver.connect(**driver_args)
        # place it in the pool
        self.pool_connection(connect)
        # set optional parameters (after connection)
        self.execute('SET FOREIGN_KEY_CHECKS=1;')
        self.execute("SET sql_mode='NO_BACKSLASH_ESCAPES';")

   # override BaseAdapter methods as needed
   def lastrowid(self, table):
        self.execute('select last_insert_id();')
        return int(self.cursor.fetchone()[0])

``:code

Looking at the various adapters as example should be easy to write new ones.

When ``db`` instance is created:

``
db = DAL('mysql://...')
``

the prefix in the uri string defines the adapter. The mapping is defined in the following dictionary also in "gluon/dal.py":

``
ADAPTERS = {
    'sqlite': SQLiteAdapter,
    'spatialite': SpatiaLiteAdapter,
    'sqlite:memory': SQLiteAdapter,
    'spatialite:memory': SpatiaLiteAdapter,
    'mysql': MySQLAdapter,
    'postgres': PostgreSQLAdapter,
    'postgres:psycopg2': PostgreSQLAdapter,
    'postgres:pg8000': PostgreSQLAdapter,
    'postgres2:psycopg2': NewPostgreSQLAdapter,
    'postgres2:pg8000': NewPostgreSQLAdapter,
    'oracle': OracleAdapter,
    'mssql': MSSQLAdapter,
    'mssql2': MSSQL2Adapter,
    'mssql3': MSSQL3Adapter,
    'mssql4' : MSSQL4Adapter,
    'vertica': VerticaAdapter,
    'sybase': SybaseAdapter,
    'db2': DB2Adapter,
    'teradata': TeradataAdapter,
    'informix': InformixAdapter,
    'informix-se': InformixSEAdapter,
    'firebird': FireBirdAdapter,
    'firebird_embedded': FireBirdAdapter,
    'ingres': IngresAdapter,
    'ingresu': IngresUnicodeAdapter,
    'sapdb': SAPDBAdapter,
    'cubrid': CubridAdapter,
    'jdbc:sqlite': JDBCSQLiteAdapter,
    'jdbc:sqlite:memory': JDBCSQLiteAdapter,
    'jdbc:postgres': JDBCPostgreSQLAdapter,
    'gae': GoogleDatastoreAdapter, # discouraged, for backward compatibility
    'google:datastore': GoogleDatastoreAdapter,
    'google:datastore+ndb': GoogleDatastoreAdapter,
    'google:sql': GoogleSQLAdapter,
    'couchdb': CouchDBAdapter,
    'mongodb': MongoDBAdapter,
    'imap': IMAPAdapter
}
``:code

the uri string is then parsed in more detail by the adapter itself.

For any adapter you can replace the driver with a different one:

``
import MySQLdb as mysqldb
from gluon.dal import MySQLAdapter
MySQLAdapter.driver = mysqldb
``
i.e. ``mysqldb`` has to be ''that module'' with a .connect() method.
You can specify optional driver arguments and adapter arguments:

``
db =DAL(..., driver_args={}, adapter_args={})
``


### Gotchas

#### SQLite
SQLite does not support dropping and altering columns. That means that web2py migrations will work up to a point. If you delete a field from a table, the column will remain in the database but will be invisible to web2py. If you decide to reinstate the column, web2py will try re-create it and fail. In this case you must set ``fake_migrate=True`` so that metadata is rebuilt without attempting to add the column again. Also, for the same reason, **SQLite** is not aware of any change of column type. If you insert a number in a string field, it will be stored as string. If you later change the model and replace the type "string" with type "integer", SQLite will continue to keep the number as a string and this may cause problem when you try to extract the data.

SQLite doesn't have a boolean type. web2py internally maps booleans to a 1 character string, with 'T' and 'F' representing True and False. The DAL handles this completely; the abstraction of a true boolean value works well.
But if you are updating the SQLite table with SQL directly, be aware of the web2py implementation, and avoid using 0 and 1 values.

#### MySQL

MySQL does not support multiple ALTER TABLE within a single transaction. This means that any migration process is broken into multiple commits. If something happens that causes a failure it is possible to break a migration (the web2py metadata are no longer in sync with the actual table structure in the database). This is unfortunate but it can be prevented (migrate one table at the time) or it can be fixed a posteriori (revert the web2py model to what corresponds to the table structure in database, set ``fake_migrate=True`` and after the metadata has been rebuilt, set ``fake_migrate=False`` and migrate the table again).

#### Google SQL

Google SQL has the same problems as MySQL and more. In particular table metadata itself must be stored in the database in a table that is not migrated by web2py. This is because Google App Engine has a read-only file system. Web2py migrations in Google:SQL combined with the MySQL issue described above can result in metadata corruption. Again, this can be prevented (by migrating the table at once and then setting migrate=False so that the metadata table is not accessed any more) or it can fixed a posteriori (by accessing the database using the Google dashboard and deleting any corrupted entry from the table called ``web2py_filesystem``.

#### MSSQL (Microsoft SQL Server)
``limitby``:inxx
MSSQL < 2012 does not support the SQL OFFSET keyword. Therefore the database cannot do pagination. When doing a ``limitby=(a,b)`` web2py will fetch the first ``b`` rows and discard the first ``a``. This may result in a considerable overhead when compared with other database engines.
If you're using MSSQL >= 2005, the recommended prefix to use is ``mssql3://`` which provides a method to avoid the issue of fetching the entire non-paginated resultset. If you're on MSSQL >= 2012, use ``mssql4://`` that uses the ``OFFSET ... ROWS ... FETCH NEXT ... ROWS ONLY`` construct to support natively pagination without performance hits like other backends.
The ``mssql://`` uri also enforces (for historical reasons) the use of ``text`` columns, that are superseeded in more recent versions (from 2005 onwards) by ``varchar(max)``. ``mssql3://`` and ``mssql4://`` should be used if you don't want to face some limitations of the - officially deprecated - ``text`` columns

MSSQL has problems with circular references in tables that have ONDELETE CASCADE. This is an MSSQL bug and you work around it by setting the ondelete attribute for all reference fields to "NO ACTION". 
You can also do it once and for all before you define tables:

``
db = DAL('mssql://....')
for key in ['reference', 'reference FK']:
    db._adapter.types[key]=db._adapter.types[key].replace('%(on_delete_action)s', 'NO ACTION')
``:code

MSSQL also has problems with arguments passed to the DISTINCT keyword and therefore
 while this works,

``
db(query).select(distinct=True)
``

this does not

``
db(query).select(distinct=db.mytable.myfield)
``

#### Oracle

Oracle also does not support pagination. It does not support neither the OFFSET nor the LIMIT keywords. Web2py achieves pagination by translating a ``db(...).select(limitby=(a,b))`` into a complex three-way nested select (as suggested by official Oracle documentation). 
This works for simple select but may break for complex selects involving aliased fields and or joins.

#### Google NoSQL (Datastore)
Google NoSQL (Datastore) does not allow joins, left joins, aggregates, expression, OR involving more than one table, the ‘like’ operator searches in "text" fields. 

Transactions are limited and not provided automatically by web2py (you need to use the Google API ``run_in_transaction`` which you can look up in the Google App Engine documentation online). 

Google also limits the number of records you can retrieve in each one query (1000 at the time of writing). On the Google datastore record IDs are integer but they are not sequential. 
While on SQL the "list:string" type is mapped into a "text" type, on the Google Datastore it is mapped into a ``ListStringProperty``. Similarly "list:integer" and "list:reference" are mapped into "ListProperty". This makes searches for content inside these fields types are more efficient on Google NoSQL than on SQL databases.
