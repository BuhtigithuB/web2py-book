## Databázová abstrakční vrstva
``DAL``:inxx

### Úvodem

Web2py obsahuje databázovou abstrakční vrstvu (DAL), což je API rozhraní, které přiřazuje objekty Pythonu databázovým objektům, jako jsou dotazy (queries), tabulky nebo záznamy (věty, records). DAL dynamicky v reálném čase sestavuje dialekt SQL jazyka pro zvolený databázový back-end, takže nemusíte psát SQL kód a učit se odchylky syntaxe SQL dialektů. Aplikace bude přenositelná na různé databázové stroje. Seznam podporovaných databází je v tabulce níže. Podívejte se na Web2py stránku nebo zkuste zjistit ve Web2py Google Group diskuzi, zda existuje další požadovaný driver. Google NoSQL propíráme jako speciální případ v Kapitole 13.

Binární distribuce pro Windows pracuje ihned (out of the box) s SQLite a MySQL. Binární distribuce pro Mac pracuje ihned s SQLite.
Chcete-li používat jiný databázový back-end, použijte Web2py ze zdrojových kódů (sources) a pokud není driver přímo k dispozici, doinstalujte jej (jeho Python ovladač, typicky pomocí pip).
``database drivers``:inxx

Jakmile je nainstalován potřebný driver, spusťte Weby2py (ze zdrojových kódů) a driver se zobrazí jako dostupný. Zde je seznam možných driverů:

``DAL``:inxx ``SQLite``:inxx ``MySQL``:inxx ``PostgresSQL``:inxx ``Oracle``:inxx ``MSSQL``:inxx ``FireBird``:inxx ``DB2``:inxx ``Informix``:inxx ``Sybase``:inxx ``Teradata``:inxx ``MongoDB``:inxx ``CouchDB``:inxx ``SAPDB``:inxx ``Cubrid``:inxx

----------
databáze | ovladač
SQLite | sqlite3 nebo pysqlite2 nebo zxJDBC ``zxjdbc``:cite  (pro Jython)
PostgreSQL | psycopg2 ``psycopg2``:cite  nebo pg8000 ``pg8000``:cite nebo zxJDBC ``zxjdbc``:cite  (pro Jython)
MySQL | pymysql ``pymysql``:cite nebo MySQLdb ``mysqldb``:cite
Oracle | cx_Oracle ``cxoracle``:cite
MSSQL | pyodbc ``pyodbc``:cite
FireBird | kinterbasdb ``kinterbasdb``:cite nebo fdb nebo pyodbc
DB2 | pyodbc ``pyodbc``:cite
Informix | informixdb ``informixdb``:cite
Ingres | ingresdbi ``ingresdbi``:cite
Cubrid | cubriddb ``cubridb``:cite
Sybase | Sybase ``Sybase``:cite
Teradata | pyodbc ``Teradata``:cite 
SAPDB    | sapdb ``SAPDB``:cite
MongoDB | pymongo ``pymongo``:cite
IMAP | imaplib ``IMAP``:cite
---------

``sqlite3``, ``pymysql``, ``pg8000`` a ``imaplib`` jsou k dispozici jako součást Web2py. Podpora MongoDB je experimentální. IMAP umožňuje používat DAL pro přístup k IMAP.

Web2py má následující třídy, které tvoří DAL:

**DAL** reprezentuje databázové připojení (connection). Například:
``DAL``:inxx ``sqlite``:inxx
``
db = DAL('sqlite://storage.db')
``:code

``define_table``:inxx
**Table** reprezentuje databázovou tabulku. Table neinstanciujete přímo. Místo toho používáte ``DAL.define_table``, která vytvoří instanci Table.
``
db.define_table('mytable', Field('myfield'))
``:code

Nejdůležitější metody třídy Table jsou:
``insert``:inxx
``truncate``:inxx
``drop``:inxx
``import_from_csv_file``:inxx
``.insert``, ``.truncate``, ``.drop`` a ``.import_from_csv_file``.

``Field``:inxx
**Field** reprezentuje databázové pole. Instanciuje se předáním jako argument pro metodu ``DAL.define_table``.

``Rows``:inxx
``Row``:inxx
**DAL Rows** (řádky, záznamy) je objekt, který vrátí výběr z databáze (select). Představme si ho jako seznam ``Row`` (jednotlivých řádků, záznamů):
``
rows = db(db.mytable.myfield!=None).select()
``:code

``Row``:inxx
**Row** obsahuje jednotlivá pole (jejich hodnoty).
``
for row in rows:
    print row.myfield
``:code

``Query``:inxx
**Query** je objekt, který odpovídá SQL klauzuli "where":
``
myquery = (db.mytable.myfield != None) | (db.mytable.myfield > 'A')
``:code

``Set``:inxx
**Set** je objekt, který odpovídá vymezuje množinu záznamů (zatímco v Rows objektu jsou již konkrétní vybrané záznamy). Nejdůležitější metody Set objektu jsou ``count``, ``select``, ``update`` a ``delete``. Například:
``
myset = db(myquery)
rows = myset.select()
myset.update(myfield='nejakahodnota')
myset.delete()
``:code

``Expression``:inxx

**Expression** znamená něco jako ``orderby`` nebo ``groupby`` výrazy. Třída Expression je tvořena pomocí Field. Tady je příklad:
``
myorder = db.mytable.myfield.upper() | db.mytable.id
db().select(db.table.ALL, orderby=myorder)
``:code

### Řetězce připojení (connection strings)
``connection strings``:inxx

Připojení k databázi je vytvořeno instanciováním DAL třídy:
``
>>> db = DAL('sqlite://storage.db', pool_size=0)
``:code
``db`` není klíčové slovo. Je to obyčejná lokální proměnná pro objekt připojení ``DAL`` a můžete ji tedy pojmenovat jakkoli. Konstruktor ``DAL`` vyžaduje jeden povinný argument, což je connection string (řetězec připojení). Je to vlastně ve Web2py jediný kód, který závisí na zvoleném databázovém back-endu. Zde jsou příklady řetězců připojení pro jednotlivé databáze (ve všech případech předpokládáme, že databáze běží na localhost na svém standardním portu a má jméno "test"):

-------------
**SQLite**     | ``sqlite://test.db``
**MySQL**      | ``mysql://username:password@localhost/test``
**PostgreSQL** | ``postgres://username:password@localhost/test``
**MSSQL**      | ``mssql://username:password@localhost/test``
**FireBird**   | ``firebird://username:password@localhost/test``
**Oracle**     | ``oracle://username/password@test``
**DB2**        | ``db2://username:password@test``
**Ingres**     | ``ingres://username:password@localhost/test``
**Sybase**     | ``sybase://username:password@localhost/test``
**Informix**   | ``informix://username:password@test``
**Teradata**   | ``teradata://DSN=dsn;UID=user;PWD=pass;DATABASE=test``
**Cubrid**     | ``cubrid://username:password@localhost/test``
**SAPDB**      | ``sapdb://username:password@localhost/test``
**IMAP**       | ``imap://user:password@server:port``
**MongoDB**    | ``mongodb://username:password@localhost/test``
**Google/SQL** | ``google:sql://project:instance/database``
**Google/NoSQL** | ``google:datastore``
-------------

V SQLite je databáze tvořena jediným souborem a ten je vytvořen automaticky, jestliže zatím neexistuje. Tento soubor je vždy uzamčen po dobu každého přístupu. V případě MySQL, PostgreSQL, MSSQL, FireBird, Oracle, DB2, Ingres a Informix je potřeba prázdnou databázi "test" mimo Web2py (ve vhodném databázovém manažeru). Jakmile je vytvořeno připojení k databázi, Web2py v ní bude vytvářet nebo rušit tabulky, nebo měnit jejich strukturu.

Je také možné jako řetězec připojení použít ``None``. V tom případě se DAL nepřipojí k žádné databázi, ale API je i tak k dispozici pro testování. Příklady uvádíme v Kapitole 7.

Někdy také může být potřeba sestavovat SQL příkazy jako kdybyste připojení měli, aniž by bylo skutečné připojení vytvořeno. To lze udělat takto:

``
db = DAL('...', do_connect=False)
``:code

Pak můžete volat ``_select``, ``_insert``, ``_update`` nebo ``_delete`` a tím sestavit SQL příkaz, ale nelze volat ``select``, ``insert``, ``update`` nebo ``delete``. Ve většině případů lze použít ``do_connect=False`` dokonce bez požadovaného databázového ovladače.

Web2py defaultně používá kódování znaků utf8. Pokud byste pracovali s databází, která ukládá v jiném kódování, použijte volitelný parametr ``db_codec``:

``
db = DAL('...', db_codec='latin1')
``:code

Jinak dostanete chyby UnicodeDecodeError.

#### Sdílení připojení (Connection pooling)
``connection pooling``:inxx

DAL konstructor má druhý parametr ``pool_size`` s defaultní hodnotou: nula.

Vzhledem k tomu, že docela dlouho trvá vytvořit připojení pro každý request (přístup na server), Web2py implementuje mechanismus Sdílení připojení. Poté, co je připojení vytvořeno, stránka je sestavena a předána, a transakce je potvrzena, tak se připojení, místo aby bylo uzavřeno, ponechá v poolu (v zásobě otevřených připojení). Jakmile přijde následující http request, Web2py zkusí vzít volné připojení z poolu a použít jej pro novou transakci. Teprve není-li v zásobě žádné volné připojení, vytvoří se připojení nové.

Po startu serveru je zásobník (pool) prázdný. Pool za provozu roste až na minimum z hodnoty ``pool_size`` a ze skutečného nejvyššího počtu souběžných přístupů. Dejme tomu, že ``pool_size=10``, ale na náš server nikdy nepřišlo více než 5 souběžných požadavků - pool poté bude obsahovat 5 připojení. Při nastavení ``pool_size=0`` se sdílení připojení nepoužije.

Připojení v poolu mohou být využita z různých vláken (threads), přesněji řečeno z různých, ale nikoli současně běžících vláken. Jeden Web2py proces má jen jeden zásobník připojení (pokud se používá).

Parametr ``pool_size`` je ignorován, pokud pracujeme s SQLite nebo s Google App Engine.
V případě SQLite je tomu tak proto, že by uvedený postup neznamenal žádný přínos (úsporu času).

#### Chyby připojení

Jestliže se Web2py nepodaří připojit k databázi, počká 1 vteřinu a zkusí to znova, postupně pětkrát, než ohlásí chybu. Při sdílení připojení (connection pooling) se může stát, že otevřené, ale nějakou dobu nepoužívané připojení uzavře databázový stroj. Tato situace je opakovanými pokusy o připojení rovněž řešena.

#### Replikované databáze

Jako první argument ``DAL(...)`` lze uvést i seznam URI (adres). V takovém případě se Web2py pokusí připojit ke každé z nich. Obvyklým účelem je možnost použití více databázových serverů a rozložení zátěže mezi ně. Typický use case (scénář použití) je zde:

``
db = DAL(['mysql://...1','mysql://...2','mysql://...3'])
``:code


V tom případě se DAL pokusí připojit k první databázi, když se to nepovede, tak ke druhé, a nakonec ke třetí databázi. To lze použít k distribuci zátěže v master-slave konfiguraci. Povíme si o tom více v Kapitole 13 při probírání otázek škálovatelnosti (scalability).

### Vyhrazená slova
``reserved Keywords``:inxx

``check_reserved`` je další argument, který můžeme předat DAL konstruktoru. Tím požadujeme, aby se zkontrolovalo, že názvy tabulek a sloupců nekolidují s vyhrazenými SQL jmény v cílových databázových strojích. Defaultní hodnota je None.

Možná hodnota parametru je seznam (list) jmen databázových adaptérů.

Jméno adaptéru je totéž jako řetězec, který se používá v DAL řetězci připojení (connection stringu). Takže chcete-li zkontrolovat kolize vůči vyhraženým jménům PostgreSQL a MSSQL databází, zadáte řetězec takto:
``
db = DAL('sqlite://storage.db',
         check_reserved=['postgres', 'mssql'])
``:code

DAL pak jména zkontroluje v zadaném pořadí.

Navíc jsou k dispozici volby "all" nebo "common". Zadáte-li "all", kontrola se provede proti všem známým rezervovaným jménům všech SQL strojů. Zadáte-li "common", zkontrolují se jen hlavní klíčová slova jako ``SELECT``, ``INSERT``, ``UPDATE``, apod.

Můžete také zkontrolovat kolize nevyhražených jmen. V takovém případě přidejte ``_nonreserved`` ke jménu adaptéru. Např.:
``
check_reserved=['postgres', 'postgres_nonreserved']
``:code

Dále uvedené adaptéry podporují kontrolu vyhrazených jmen:

-----
**PostgreSQL** | ``postgres(_nonreserved)``
**MySQL** | ``mysql``
**FireBird** | ``firebird(_nonreserved)``
**MSSQL** | ``mssql``
**Oracle** | ``oracle``
-----

### ``DAL``, ``Table``, ``Field``

S DAL API (s rozhraním databázové vrstvy) můžete experimentovat pomocí Web2py shellu. Příklad: python web2py.py -a "<heslo>" -M -S "<aplikace>"

Zápisem "db<enter>" zjistěte, zda je vytvořeno spojení, a/nebo jej vytvořte znova, např. do SQLite:

``
>>> db = DAL('sqlite://storage.db')
``:code

Nyní je připojení vytvořeno a uloženo v proměnné ``db``.

Kdykoli můžete zjistit řetězec připojení (connection string):
``_uri``:inxx
``
>>> print db._uri
sqlite://storage.db
``:code

a jméno databáze:
``_dbname``:inxx
``
>>> print db._dbname
sqlite
``:code

Connection string je pojmenován ``_uri``, protože se jedná o instanci Uniform Resource Identifier.

DAL umožňuje současná připojení do stejné databáze nebo do více různých databází, dokonce i do databází různých typů. Ale nyní uvažujme jediné připojení, protože to je obvyklý případ.

``define_table``:inxx ``Field``:inxx
``type``:inxx ``length``:inxx ``default``:inxx ``requires``:inxx ``required``:inxx ``unique``:inxx
``notnull``:inxx ``ondelete``:inxx ``uploadfield``:inxx ``uploadseparate``:inxx ``migrate``:inxx ``sql.log``:inxx

Nejdůležitější metoda DAL je ``define_table``:
``
>>> db.define_table('osoba', Field('jmeno'))
``:code

Ta definuje, ukládá a vrací objekt třídy ``Table`` (Tabulka), v tomto případě zvaný "osoba", což je tabulka s polem (sloupcem) "jmeno". Tento objekt je nadále dostupný i jako ``db.person``, takže obvykle není potřeba si návratovou hodnotu uložit.

V seznamu polí neuvádějte "id", protože Web2py vytvoří toto pole vždy automaticky. Každá tabulka bude mít pole "id" defaultně. Je to integer auto-inkrementované pole (obvykle počínaje od 1) pro unikátní identifikaci každého záznamu a pro reference (odkazy) mezi tabulkami datového modelu. Neboli "id" je primární klíč tabulky. (Poznámka: číslování id od 1 závisí na databázovém stroji. Např. tomu tak není na Google App Engine NoSQL.)

``named id field``:inxx
Lze také definovat jinak pojmenované pole typu ``type='id'`` a Web2py takové pole použije stejně jako auto-increment id. To se nedoporučuje používat, ale můžete to potřebovat, pokud se budete připojovat k tabulkám jiné (ne Web2py) aplikace (legacy tables). S nějakými omezeními je možné použít i jiné primární klíče, což bude probráno v oddíle o použití tabulek jiných aplikací.

Tabulky lze definovat jen jednou, avšak je možné re-definovat existující tabulku:

``
db.define_table('osoba', Field('jmeno'))
db.define_table('osoba', Field('jmeno'), redefine=True)
``:code

To může spustit mograci (automatický převod struktury tabulky), pokud se seznam polí proti původnímu změnil.

----------
Protože tabulky obvykle definujeme v modelu a modely se vykonávají dříve než controller, je časté, že některá tabulka je definována, aniž bude při konkrétním přístupu potřeba. Proto je potřeba zrychlit zpracování odloženým definováním tabulky. Tomu se říká lazy definice (doslova: líné). Vytvoříme je nastavením ``DAL(...,lazy_tables=True)``. Tabulky (jim odpovídající datové struktury) se skutečně vytvoří jen tehdy a až tehdy, když se do nich bude přistupovat.
----------


### Reprezentace záznamu

Je nepovinné, ale doporučuje se zadat výraz pro reprezentaci jednotlivého záznamu (tedy určit, jak má být zobrazen uživateli):
``
>>> db.define_table('osoba', Field('jmeno'), format='%(jmeno)s')
``:code

or
``
>>> db.define_table('osoba', Field('jmeno'), format='%(jmeno)s %(id)s')
``:code

nebo i složitěji pomocí funkce:
``
>>> db.define_table('osoba', Field('jmeno'),
       format=lambda r: r.jmeno or 'anonymní uživatel')
``:code

Atribut format se používá ze dvou důvodů:
- Aby reprezentoval odkazované záznamy ve výběrových drop-down prvcích.
- Aby inicioval atribut ``db.jinatabulka.osoba.represent`` pro všechny tabulky, které jsou pomocí cizího klíče na tuto tabulku vázány. To znamená, že v tabulkovém zobrazení (např. pomocí SQLTABLE) se nezobrazí odkazovaná id, ale záznam tak, jak jej chcete (nastavením: format=) vidět.


``Field constructor``:inxx
Zde jsou defaultní hodnoty parametrů konstruktoru třídy Field:
``
Field(name, 'string', length=None, default=None,
      required=False, requires='<default>',
      ondelete='CASCADE', notnull=False, unique=False,
      uploadfield=True, widget=None, label=None, comment=None,
      writable=True, readable=True, update=None, authorize=None,
      autodelete=False, represent=None, compute=None,
      uploadfolder=os.path.join(request.folder,'uploads'),
      uploadseparate=None,uploadfs=None)
``:code

Ne všechny argumenty jsou relevantní pro kterýkoli typ pole.
Např. "uploadfield" a "authorize" zase jen k polím typu "upload". "ondelete" má význam jen pro pole typů "reference" nebo "upload".
- ``length`` určuje maximální délku (počet znaků) "string", "password" nebo "upload" pole.  Jestliže ``length`` není zadáno, použije se defaultní hodnota, jenže pro ni není zaručována zpětná kompatibilita. ''Abyste zabránili nechtěným migracím při upgradu verze, doporučujeme zadat length vždy explicitně.''
- ``default`` nastavuje defaultní hodnotu pole. Ta se použije při INSERTu, jestliže hodnota pole není v příkazu určena explicitně. Také se použije pro předvyplnění formulářů, vytvořených podle tabulky pomocí SQLFORM. Nemusí to být jen pevná hodnota, ale také to může být funkce (typicky lambda funkce), která vrátí hodnotu potřebného typu. V takovém případě se funkce zavolá pro každý jednotlivý záznam, i když je vkládáno více záznamů v jediné transakci.
- ``required`` říká DAL vrstvě, že není dovolen INSERT do tabulky, dokud hodnota pole není explicitně určena.
- ``requires`` určuje validátor nebo seznam validátorů. Nepoužívá jej DAL, ale používá jej formulář: SQLFORM. Defaultní validátory podle typu pole uvádí následující tabulka:

----------
**typ pole** | **defaultní validátor**
``string`` | ``IS_LENGTH(length)`` default length je 512
``text`` | ``IS_LENGTH(65536)``
``blob`` | ``None``
``boolean`` | ``None``
``integer`` | ``IS_INT_IN_RANGE(-1e100, 1e100)``
``double`` | ``IS_FLOAT_IN_RANGE(-1e100, 1e100)``
``decimal(n,m)`` | ``IS_DECIMAL_IN_RANGE(-1e100, 1e100)``
``date`` | ``IS_DATE()``
``time`` | ``IS_TIME()``
``datetime`` | ``IS_DATETIME()``
``password`` | ``None``
``upload`` | ``None``
``reference <tabulka>``  | ``IS_IN_DB(db, tabulka.pole, format)``
``list:string`` | ``None``
``list:integer`` | ``None``
``list:reference <tabulka>`` | ``IS_IN_DB(db, tabulka.pole, format, multiple=True)``
``json`` | ``IS_JSON()``
``bigint`` | ``None``
``big-id`` | ``None``
``big-reference`` | ``None``
---------

Decimal vyžaduje a vrací hodnoty stejně jako ``Decimal`` objekty z Python modulu ``decimal``. SQLite s nimi neumí pracovat, proto interně je typ ``decimal`` zpracován jako ``double``. (n,m) udávají počet číslic celkem a počet číslic za desetinnou tečkou.

``big-id`` a ``big-reference`` podporují jen některé databázové stroje a jsou experimentální. Není důvod je normálně použít, vyjma legacy tables (práce s tabulkami jiných aplikací). DAL konstruktor má parametr ``bigint_id`` a jestliže mu předáme ``True``, změní pole typů ``id`` a ``reference`` na ``big-id`` a ``big-reference``.

``list:<type>`` pole jsou zvláštní případ. Jsou určena k využití jistých výhod denormalizačních vlastností NoSQL (v případě Google App Engine NoSQL polí typu ``ListProperty`` a ``StringListProperty``) a tyto vlastnosti dávají k dispozici i podporovaným relačním databázím. V relačních databázích jsou "list" pole uložena jako pole typu ``text``. Položky v nich jsou odděleny pomocí ``|`` s tím, že případný znak ``|`` uvnitř položky je escapován na ``||``. O těchto typech polí pojednáme ve zvláštním oddíle.

``json`` pole může ukládat jakýkoli json serializovatelný objekt. Je speciálně vytvořeno kvůli práci s MongoDB a v rámci přenositelnosti jej mají k dispozici i ostatní databázové adaptéry.

-------
``requires=...`` je použit na úrovni formulářů, ``required=True`` na úrovni DAL (pro insert), kdežto ``notnull``, ``unique`` a ``ondelete`` na úrovni databáze. Mohou se někdy zdát redundantní, ale je potřeba pamatovat na uvedený rozdíl.
-------

``ondelete``:inxx

- ``ondelete`` se přeloží do SQL příkazu na klauzuli "ON DELETE". Defaultně je nastaven na "CASCADE". Tím je databázi řečeno, aby, když maže záznam, prošla a zrušila i záznamy jiných tabulek, jejichž cizí klíč na rušený záznam odkazuje. Takové chování potlačíme nastavením ``ondelete`` na "NO ACTION" nebo "SET NULL".
- ``notnull=True`` přidá do SQL příkazu klauzuli "NOT NULL". Blokuje přidávání null hodnot do příslušného pole databáze.
- ``unique=True`` se přeloží v SQL příkazu na klauzuli "UNIQUE" a tím způsobí kontrolu, že hodnoty v tomto poli (sloupci) tabulky jsou unikátní. To je rovněž zajištěno na úrovni databáze.

- ``uploadfield`` má význam jen pro pole typu "upload". Pole typu "upload" ukládá normálně jen jméno souboru, zatímco samotný soubor ukládá jinam, defaultně do adresáře "uploads/" souborového systému. Jestliže je nastaveno ``uploadfield``, pak je soubor uložen do blob pole té samé tabulky a hodnota ``uploadfield`` určuje jméno tohoto blob pole. Probereme to podrobněji v souvislosti s SQLFORM.
- ``uploadfolder`` je defaultně "uploads/" adresář aplikace. Pokud se uploadované soubory ukládají jako samostatné soubory (tedy nikoli do blob pole), můžete určit odlišnou cestu, kam soubory ukládat. Například:
``
Field(...,uploadfolder=os.path.join(request.folder,'static/temp'))
``:code
bude soubory ukládat do "web2py/applications/mojeaplikace/static/temp".
- ``uploadseparate``, pokud bude nastaveno na True, způsobí ukládání souborů do separátních podadresářů adresáře pro upload. Tím je optimalizováno ukládání velkého množství souborů. POZOR: Není dovoleno hodnotu ``uploadseparate`` změnit dodatečně: rozbili byste tím odkazy na existující uploadované soubory. Pokud by se to stalo, je možné soubory přesunout a tím problém opravit, ale tento postup zde neuvádíme.
- ``uploadfs`` umožňuje uvést jiný souborový systém pro ukládání souborů, včetně Amazon S3 úložiště (storage) nebo vzdáleného (remote) SFTP úložiště. K tomu musí být instalován PyFileSystem. ``uploadfs`` musí odkazovat na ``PyFileSystem``. ``PyFileSystem``:inxx ``uploadfs``:idxx

- ``widget`` musí být některý z dostupných widget objektů, včetně uživatelsky přidaných widgetů, například: ``SQLFORM.widgets.string.widget``. Seznam widgetů, které jsou k dispozici, bude probrán později. Každý typ pole má přiřazen svůj defaultní widget.
- ``label`` je řetězec (nebo něco, co může být na řetězec serializováno, např. HTML helper), který se použije pro pojmenování pole v automaticky generovaných formulářích.
- ``comment``  je řetězec (nebo opět to, co může být na řetězec serializováno, např. HTML helper) s poznámkou nebo komentářem pro toto pole, což se zobrazí v automaticky generovaných formulářích typicky napravo od input prvku.
- ``writable`` říká, zda je prvek, zobrazený ve formulářích, přístupný pro změny (zápis).
- ``readable`` určuje zobrazení prvku ve formulářích. Pole, které není readable ani writable, se nezobrazí ve formulářích pro přidání záznamu a editaci.
- ``update`` znamená defaultní hodnotu, která přepíše obsah pole při ukládání záznamu.
- ``compute`` je volitelná funkce, která se vykoná při ukládání (insert nebo update) do záznamu - do pole se uloží výsledek této funkce. Jako argument je funkci předán slovník (dictionary) aktuálního záznamu. V něm ovšem chybí položky s aktuálními hodnotami "compute" polí.
- ``authorize`` lze použít k vynucení kontroly přístupu pro jednotlivé pole, a sice pouze u "upload" polí. Podrobněji to probereme u Autentikace a Autorizace.
- ``autodelete`` určuje, zda bude při mazání záznamu automaticky smazán i separátně uložený soubor. Vztahuje se pouze k "upload" polím.
- ``represent`` může být None nebo to může být funkce, která hodnotu pole převede na jinou reprezentaci pro uživatele. Příklady:
``
db.mojetabulka.jmeno.represent = lambda jmeno, row: jmeno.capitalize()
db.mojetabulka.jine_id.represent = lambda id, row: row.myfield
db.mojetabulka.nejake_uploadpole.represent = lambda value, row: \
    A('stáhnout', _href=URL('download', args=value))
``:code

``blob``:inxx
"blob" pole jsou rovněž speciální. Defaultně jsou binární data zakódována do base64 a pak teprve uložena do pole v databázi, a obdobně jsou dekódována při čtení z databáze. Tím se sice zabere o 25% více místa, ale dává to dvě výhody. V průměru to sníží množství dat, přenesených mezi Web2py a databázovým serverem, a komunikace tím nebude závislá na specifických escape konvencích (ošetření problematických znaků) konkrétního back-endu (databázového stroje).

Většina atributů polí může být změněna i dodatečně, později než jsou definována pole:

``
db.define_table('osoba', Field('jmeno', default=''), format='%(jmeno)s')
db.osoba._format = '%(jmeno)s/%(id)s'
db.osoba.jmeno.default = 'anonym'
``
(poznamenejme, že atributy tabulek prefixujeme podtržítkem, aby se zabránilo možnému konfliktu se jmény polí).

Můžete získat seznam (list) tabulek, definovaných pro dané databázové připojení:

``tables``:inxx
``
>>> print db.tables
['osoba']
``:code

Podobně můžete získat seznam polí, definovaných pro danou tabulku:

``fields``:inxx
``
>>> print db.osoba.fields
['id', 'jmeno']
``:code


Typ tabulky - instance třídy Table:

``Table``:inxx
``
>>> print type(db.person)
<class 'gluon.sql.Table'>
``:code

jiný způsob, jak použít tabulku:
``
>>> print type(db['osoba'])
<class 'gluon.sql.Table'>
``:code

Podobně můžete přistupovat k polím více způsoby:
``
>>> print type(db.osoba.jmeno)
<class 'gluon.sql.Field'>
>>> print type(db.osoba['jmeno'])
<class 'gluon.sql.Field'>
>>> print type(db['osoba']['jmeno'])
<class 'gluon.sql.Field'>
``:code

Pro konkrétní pole můžete získat aktuální nastavení jeho atributů:
``
>>> print db.osoba.jmeno.type
string
>>> print db.osoba.jmeno.unique
False
>>> print db.osoba.jmeno.notnull
False
>>> print db.osoba.jmeno.length
32
``:code

a to včetně tabulky, do které patří (jako odkaz nebo jako řetězec) a včetně připojení, k němuž definice patří:
``
>>> db.osoba.jmeno._table == db.osoba
True
>>> db.osoba.jmeno._tablename == 'osoba'
True
>>> db.osoba.jmeno._db == db
True
``:code

Pole má také metody. Některé se používají pro sestavení dotazů (queries) a seznámíme se s nimi je později.
Speciální metodou objektu pole je ``validate``, které provede validátor(y) pole.

``
print db.osoba.jmeno.validate('Honza')
``

což vrátí vektor (tuple) ``(value, error)``. ``error`` je ``None``, jestliže validace prošla bez chyb.

### Migrace
``migrations``:inxx

``define_table`` kontroluje, zda příslušná tabulka existuje nebo ne. Pokud ne, sestaví SQL příkaz pro její vytvoření a provede jej. Jestliže tabulka už existuje, ale liší se od zadané definice, sestaví se SQL příkaz pro úpravu struktury tabulky a provede se. Pokud pole nezměnilo název, ale změnilo typ, dojde k pokusu převést data. (Pokud tomuto chcete zabránit, je potřeba redefinovat strukturu tabulky postupně dvakrát, napoprvé odstraněním pole (a uložených dat v něm) a napodruhé jeho opětovným nadefinováním, takže bude vytvořeno jako prázdné.) Pokud tabulka existuje ve správné struktuře, neprovede se nic, ale ve všech případech se vytvoří objekt ``db.person`` jako reprezentace databázové tabulky.

Toto chování označujeme jako "migrace". Web2py loguje všechny migrace a pokusy o ni do souboru "databases/sql.log".

Prvním argumentem ``define_table`` je vždy jméno tabulky. Následující další nepojmenované argumenty jsou definice polí (Field). Funkce má také nepovinný pojmenovaný parametr "migrate":
``
>>> db.define_table('osoba', Field('jmeno'), migrate='osoba.table')
``:code

Jméno, uvedené v "migrate", je jméno souboru (který vznikne ve složce "databases" aplikace) a v němž si Web2py udržuje své interní informace o této tabulce. Tyto soubory jsou velice důležité a nikdy je neodstraňujte, dokud v databázi existují odpovídající tabulky. Jen v případě, že jste tabulku už zrušili (drop) lze smazat i soubor s interními informacemi a jménem zrušené tabulky (pokud by zůstal).
Defaultně je "migrate" nastaveno na True. Web2py pak sestaví jméno souboru s interními informacemi z hashe připojení a ze jména tabulky.
Nastavíme-li migrate=False, není migrace povolena a Web2py předpokládá, že tabulka existuje v databázi a že má správnou strukturu, a sice alespoň ta pole, která jsou uvedena v ``define_table``.

Je doporučeno migrační tabulky explicitně pojmenovat. Migrační tabulky dvou různých tabulek v aplikaci pochopitelně nesmějímít stejná jména.

Také DAL třída má parametr "migrate", který určuje defaultní hodnotu pro "migrate" při volání ``define_table``. Například:
``
>>> db = DAL('sqlite://storage.db', migrate=False)
``:code

nastaví defaultní hodnotu "migrate" na False při každém volání ``db.define_table``, v němž chybí argument "migrate". Kontrola existence a struktury tabulky je tím potlačena.

------
Poznamenáváme, že Web2py migruje jen nové sloupce, odstraněné sloupce a změny v typu sloupce (to poslední s výjimkou SQLite). Web2py nemigruje změněné atributy jako ``default``, ``unique``, ``notnull``, nebo ``ondelete``.
------

Migrace lze striktně zakázat všem tabulkám najednou:

``
db = DAL(...,migrate_enabled=False)
``

To je doporučené chování a nastavení, jestliže dvě aplikace sdílejí stejnou databázi. Jen jedna z nich by měla mít povoleny migrace a provádět je, druhá (nebo ostatní) by měly mít migrace zakázané.

### Oprava poškozených migrací
``fake_migrate``:inxx

Existují dva běžné problémy s migracemi a k nim vhodné postupy, jak je řešit.

Jeden problém je specifický pro SQLite. SQLite nevynucuje typy sloupců a neumí fyzicky odstranit sloupec. Znamená to, že máte-li pole (sloupec) typu string a odstraníte jej, z databáze fyzicky odstraněn nebude, pouze se nepoužívá. Jestliže stejně pojmenovaný sloupec přidáte znovu s jiným typem, dejme tomu datetime (nebo to celé provedete v jediném migračním kroku), skončíte s datetime sloupcem, který ve své části obsahuje řetězce (string). Web2py neví, co z databáze obdrží, takže zkusí načíst záznamy a havaruje.

Jestliže Web2py vrátí chybu v gluon.sql.parse funkci při načtení (select) záznamů, jedná se o tento problém: poškozená data ve sloupci v důsledku uvedeného problematického chování.

Řešením je aktualizovat v opětovně přidaném (nebo v jediném kroku přetypovaném) sloupci data hodnotou správného typu. Např. můžeme přetypovaný sloupec ve všech záznamech aktualizovat hodnotou None.

Jiný problém je obecnější, ale typický pro MySQL. MySQL nedovoluje více než jeden ALTER TABLE v transakci. Znamená to, že Web2py musí rozdělit složitější transakce na jednoduché transakční kroky (s nějvýše jedním ALTER TABLE) a každý z nich commitovat individuálně. Může tak dojít k tomu, že zatímco část celého postupu projde a je commitována, následující část selže a zanechá tak Web2py v poškozeném stavu. Z jakého důvodu může dílčí transakce havarovat? Protože například zahrnuje změnu typu string na datetime a spolu s ní se Web2py pokusí data převést a havaruje při převodu. Co to pro Web2py bude dále znamenat? Bude zmatené v tom, jaká přesně struktura tabulky zůstala v databázi.

Opravu lze provést zakázáním migrací a povolením falešných (fake) migrací:
``
db.define_table(...., migrate=False, fake_migrate=True)
``:code

Tím se přebudují Web2py metadata (migrační tabulka) podle definice tabulky. Vyzkoušejte různé definice a zjistěte, která pracuje správně (definici před migrací a definici po migraci). Jakmile budete úspěšní, odstraňte zase ``fake_migrate=True`` atribut.

Před pokusem o opravu poškozené migrace je vhodné si zálohovat soubory "applications/yourapp/databases/*.table".

Migrační problémy lze také opravit pro všechny tabulky najednou:

``
db = DAL(..., fake_migrate_all=True)
``:code

To havaruje, jestliže model obsahuje tabulky, které v databázi právě neexistují, ale může to při řešení problému pomoci.

### ``insert``

Do určené tabulky můžete vkládat záznamy:

``insert``:inxx
``
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
``:code

Insert vrátí unikátní "id" záznamu, který byl právě vytvořen.

Můžete vymazat obsah tabulky neboli zrušit všechny záznamy a resetovat čítač přidělovaných id:

``truncate``:inxx
``
>>> db.person.truncate()
``:code

Když nyní zopakujete přidání, čítač začne znova přidělovat id od 1 (což je back-end specifické a neplatí na Google NoSQL):
``
>>> db.person.insert(name="Alex")
1
``:code

Poznamenejme, že pro ``truncate`` lze zadat argumenty, např. říci SQLite, že má restartovat čítač id.

``
db.person.truncate('RESTART IDENTITY CASCADE')
``:code

Argumenty jsou v cílovém SQL a tím pádem specifické pro konkrétní databázový stroj.

``bulk_insert``:inxx
Web2py má také metodu pro hromadný import (bulk_insert)
``
>>> db.person.bulk_insert([{'name':'Alex'}, {'name':'John'}, {'name':'Tim'}])
[3,4,5]
``:code

Argumentem je seznam (list) slovníků (dictionary). Pro každý slovník ze seznamu se importuje jeden záznam. Jako seznam (list) jsou vrácena id přidaných záznamů. Na podporovaných relačních databázích není tento import žádným přínosem ve srovnání s cyklem a prováděním jednotlivých importů. Ale na Google App Engine NoSQL dosáhnete podstatného zvýšení rychlosti.

### ``commit`` a ``rollback``

create, drop, insert, truncate, delete, nebo update operace nejsou potvrzeny, dokud nepoužijete příkaz commit:

``commit``:inxx
``
>>> db.commit()
``:code

Ověřte si to tak, že přidáte záznam:
``
>>> db.person.insert(name="Bob")
2
``:code

a revertujete, neboli budete ignorovat všechny operace, provedené od předchozího commitu:

``rollback``:inxx
``
>>> db.rollback()
``:code

Když nyní zopakujete insert, čítač znova vrátí 2, protože předchozí insert byl odvolán.
``
>>> db.person.insert(name="Bob")
2
``:code

Kód v modelech, controllerech a view je obalen Web2py kódem, který vypadá zhruba takto:
``
try:
     provést všechny modely, funkci controlleru a view
except:
     rollback všech připojení
     zaznamenat stack volání do logu
     vystavit chybový ticket uživateli
else:
     commit všech připojení
     uložit cookies, sessiony a vrátit sestavenou stránku
``:code

Z toho důvodu není potřebné explicitně ve Web2py volat ``commit`` nebo ``rollback``, leda tehdy, když chcete potvrzovat změny dat po menších krocích.

### Hrubé SQL příkazy (Raw SQL)

#### Měření času provedení příkazů

Web2py automaticky měří čas vykonání SQL příkazů. Proměnná ``db._timings`` je seznam (list) vektorů (tuple). Každý vektor obsahuje cílový SQL příkaz, jak byl předán databázovému driveru, a čas, který zabralo jeho vykonání. Z diagnostických důvodů si toto můžete zobrazit ve view pomocí toolbaru:

``
{{=response.toolbar()}}
``

#### ``executesql``

DAL umožňuje vykonávat i explicitní SQL příkazy (tedy nejen automaticky sestavené SQL příkazy).

``executesql``:inxx
``
>>> print db.executesql('SELECT * FROM person;')
[(1, u'Massimo'), (2, u'Massimo')]
``:code

V takovém případě DAL neparsuje a nepřevádí vrácené hodnoty a formát tak závisí na konkrétním databázovém ovladači. Takové použití obvykle není potřeba pro SELECTy, ale je typické např. pro SQL příkazy pro práci s indexy.
``executesql`` má 4 volitelné argumenty: ``placeholders``, ``as_dict``, ``fields`` a ``colnames``.
``placeholders`` je volitelná sekvence hodnot pro náhradu zástupných symbolů (placeholders) ve vašem SQL příkazu nebo (pokud to databázový stroj podporuje) slovník (dictionary) s klíči, které odpovídají zástupným symbolům v SQL příkaze.

Jestliže ``as_dict`` je nastaveno na True, výsledný kurzor, jak jej vrátí DB driver, bude zkonvertován na seznam (list) slovníků (dictionary), jejichž klíči jsou jména db polí. Výsledky, vrácené při ``as_dict = True`` jsou stejné, jako když aplikujete **.as_list()** na výsledky normálního DAL selectu.
``
[{field1: value1, field2: value2}, {field1: value1b, field2: value2b}]
``:code

V opačném případě (defaultně) získáte seznam (list) vektorů (tuple).

Argument ``fields`` je seznam (list) DAL Field objektů, které odpovídají polím, vráceným z databáze. Field objekty mají být použity v definici jedné nebo více DAL tabulek. ``fields`` seznam může také obsahovat jako položku DAL tabulku nebo tabulky, a to místo polí nebo spolu s dalšími poli. Chceme-li uvést jedinou tabulku, nemusí ani být parametrem seznam, ale přímo objekt tabulky. Tam, kde je použit objekt tabulky, získají se Field objekty z definice tabulky.

Místo určení argumentu ``fields`` lze použít argument ``colnames``, což je seznam (list) jmen polí ve formátu jmenotabulky.jmenopole. I v tomto případě by měly být použity pole a tabulky, které jsou definovány v DAL objektu připojení (obvykle pojmenovaném: db).

Je i možné zadat obojí současně, ``fields`` i příslušné ``colnames``. V takovém případě může kromě Fields objektů argument ``fields`` obsahovat DAL Expression
objekty (výrazy). K Field objektům ve "fields" odpovídající ``colnames`` musí stále být ve formátu jmenotabulky.jmenopole, kdežto pro Expression objekty může být použito libovolné jméno.

``fields`` a ``colnames`` musí být ve stejném pořadí, jako jsou pole ve výsledném kurzoru, jak jej vrátí databáze.

Poznamenejme ještě, že DAL Table objekty, na které odkazují ``fields`` a ``colnames``, mohou být fiktivní (dummy) tabulky, které nemusí představovat žádnou skutečnou tabulku v databázi.


#### ``_lastsql``

Ať už byl SQL příkaz zadán manuálně nebo byl sestaven pomocí DAL, vždy můžete zjistit poslední provedený SQL příkaz pomocí ``db._lastsql``. To je užitečné při ladění:

``_lastdb``:inxx
``
>>> rows = db().select(db.person.ALL)
>>> print db._lastsql
SELECT person.id, person.name FROM person;
``:code

-------
Web2py nikdy nesestavuje SQL příkazy s operátorem "*" - místo toho je vždy explicitní při tvorbě seznamu požadovaných polí.
-------

### ``drop``

Celou tabulku můžete smazat:

``drop``:inxx
``
>>> db.person.drop()
``:code

Poznámka pro SQLite: Web2py nevytvoří tabulku znova (migrací při příštím přístupu), dokud v databases/ adresáři nezrušíte odpovídající .table soubor (s metadaty zrušené tabulky). 

### Indexy

DAL API zatím nemá příkaz pro vytváření indexů, takže právě k tomu je vhodné použít příkaz ``executesql``. Existence indexů (zahrnutá do DAL) by totiž mohla příliš zkomplikovat problematiku migrací, takže je lepší s indexy manipulovat explicitně.

Tady je příklad, jak [[vytvořit index pomocí SQL příkazu v SQLite http://www.sqlite.org/lang_createindex.html]]:
``
>>> db = DAL('sqlite://storage.db')
>>> db.define_table('person', Field('name'))
>>> db.executesql('CREATE INDEX IF NOT EXISTS myidx ON person (name);')
``:code

Jiné databázové dialekty mají velice podobnou syntaxim ale např. nemusí rozumět volitelné direktivě "IF NOT EXISTS".

### Databáze jiných aplikací (legacy databases) a "tabulky s klíčem"

Web2py se může připojit k databázím jiných aplikací jen za určitých podmínek.

Nejsnazší je to při splnění těchto podmínek:
- Každá tabulka má unikátní auto-inkrement integer pole, pojmenované "id"
- Záznamy jsou z jiných tabulek odkazovány výhradně pomocí tohoto "id" pole.

Když se připojujete k existující tabulce (k tabulce, kterou nevytvořila tato Web2py aplikace), vždy nastavte ``migrate=False``.

Jestliže cizí tabulka má auto-increment integer pole, ale to se jmenuje jinak než "id", Web2py k ní může i tak přistupovat, ale do definice tabulky musíte explicitně přidat ``Field('....','id')``, kde .... je jméno auto-increment integer pole.

``keyed table``:inxx

Jestliže cizí tabulka používá primární klíč, tvořený jinak než jako jediné auto-inkrementální integer pole, můžete využít "tabulku s klíčem" ("keyed table"), například:
``
db.define_table('account',
    Field('accnum','integer'),
    Field('acctype'),
    Field('accdesc'),
    primarykey=['accnum', 'acctype'],
    migrate=False)
``:code

- ``primarykey`` je seznam jmen polí, které tvoří primární klíč.
- Všechna primarykey pole budou mít nastavení ``NOT NULL``, i když to explicitně nespecifikujete.
- "Tabulky s klíčem" mohou odkazovat pomocí cizích klíčů zase jen na tabulky s klíčem (s argumentem "primarykey").
- Odkazující pole musí používat formát ``reference tablename.fieldname``.
- Funkce ``update_record`` není k dispozici pro Rows objekt "tabulek s klíčem".

-------
Tabulky s klíčem jsou podporovány jen pro některé back-endy (DB2, MS-SQL, Ingres, Informix, případně později přidané). Nezaručujeme tedy, že ``primarykey`` atribut je použitelný pro jakoukoli cizí tabulku a libovolný db back-end.
Řešením v takovém případě může být vytvoření view, které má auto-inkrementální id pole.


### Distribuované transakce
``distributed transactions``:inxx

------
V době psaní je tato vlastnost podporována jen pro PostgreSQL, MySQL a Firebird, protože mají API pro dvoufázové commity.
------

Předpokládejme, že máte dvě (nebo více) připojení k různým PostgreSQL databázím, například:
``
db_a = DAL('postgres://...')
db_b = DAL('postgres://...')
``:code

V modelu nebo controlleru je můžete commitovat současně, pomocí:
``
DAL.distributed_transaction_commit(db_a, db_b)
``:code

Dojde-li k chybě, tato metoda revertuje vše a vyhodí ``Exception``.

Normálně, po návratu z akce (funkce) controlleru, jsou-li použita dvě připojení současně a nezavoláte tuto funkci, Web2py je obě připojení commituje, jenže separátně. Znamená to, že jeden z commitů může být úspěšný a druhý selhat. Distribuovaná transakce (explicitní zavolání popsané funkce) takové situaci zabrání.

### Více o uploadech

Uvažujme následující model:
``
>>> db.define_table('myfile',
    Field('image', 'upload', default='path/'))
``:code

Pro 'upload' pole může být jeho default nastaven na cestu (absolutní nebo relativní k aktuální aplikaci) a defaultní image se vytvoří jako kopie souboru na zadané cestě. Vytvoří se nová samostatná kopie pro každý přidaný záznam, v němž nebude upload (v příkladu pojmenovaný: image) zadán.

Normálně je vložení uploadu zajištěno automaticky pomocí SQLFORM nebo crud formuláře (což je rovněž varianta SQLFORM), ale někdy již máte soubor v souborovém systému a chcete ho uploadovat programově. To je možné provést takto:
``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename))
``:code

Je také možné vložit soubor ještě jednodušším způsobem, kdy insert metoda zavolá store() automaticky:

``
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=stream)
``:code

V tomto případě bude jméno souboru zjištěno ze stream objektu.

Metoda ``store`` upload pole má dva parametry: stream a jméno souboru. Založí nový soubor, pojmenovaný jako dočasný, a sice v uploads/ adresáři, není-li zadáno jinak, a obsah souboru zkopíruje do tohoto nového dočasného souboru. Vrátí jméno souboru, které je nakonec uloženo do ``image`` pole tabulky ``db.myfile``.

Poznamenejme, že jestliže má být soubor uložen ne jako samostatný soubor, ale do blob pole databáze, metoda ``store()`` soubor do blob pole neuloží - a to proto, že je zavolána dříve než insert(). Soubor tedy v tomto případě musíme uložit do blob pole explicitně:
``
>>> db.define_table('myfile',
        Field('image', 'upload', uploadfield='image_file'),
        Field('image_file', 'blob'))
>>> stream = open(filename, 'rb')
>>> db.myfile.insert(image=db.myfile.image.store(stream, filename),
        image_file=stream.read())
``:code


Opakem ``.store`` je ``.retrieve``:

``
>>> row = db(db.myfile).select().first()
>>> (filename, stream) = db.myfile.image.retrieve(row.image)
>>> import shutil
>>> shutil.copyfileobj(stream,open(filename,'wb'))
``

### ``Query``, ``Set``, ``Rows``

Uvažujme znova tabulku, definovanou výše, prázdnou (např. po truncate() nebo po drop() následovaném opětovným vytvořením tabulky mechanismem migrace). Vložme do ní tři záznamy:
``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name="Alex")
1
>>> db.person.insert(name="Bob")
2
>>> db.person.insert(name="Carl")
3
``:code

Odkaz na tabulku můžete pochopitelně uložit do proměnné, dejme tomu pojmenované ``person``:

``Table``:inxx
``
>>> person = db.person
``:code

Stejně si můžete do proměnné uložit odkaz na pole, např. pojmenujme proměnnou opět stejně se jménem pole:

``Field``:inxx
``
>>> name = person.name
``:code

Vytvořme si dotaz (query) pomocí operátorů (jako jsou ==, !=, <, >, <=, >=, like nebo belongs) a tento dotaz můžeme opět uložit do proměnné - pojmenujme ji třeba ``q``:

``Query``:inxx
``
>>> q = name=='Alex'
``:code

Zavoláním ``db`` s parametrem <dotaz> definujete objekt Set - sadu (množinu) záznamů (set of records). Zase si ji uložíme do proměnné, pojmenované např. ``s``:

``Set``:inxx
``
>>> s = db(q)
``:code

Poznamenejme, že zatím nebyl vykonán žádný SQL příkaz do databáze. DAL + Query jen připravily objekt pro sadu záznamů, které tomuto dotazu (query) vyhovují.
Web2py z dotazu vyhodnotí, do které tabulky (nebo tabulek) dotaz zasahuje. Není potom potřeba tabulky explicitně určit.

### ``select``

Se Set objektem, který máme nyní přístupný pod proměnnou ``s``, můžete získat skutečné záznamy z databáze příkazem ``select``:

``Rows``:inxx ``select``:inxx
``
>>> rows = s.select()
``:code

``Row``:inxx
Dostaneme iterovatelný objekt třídy ``gluon.sql.Rows``, jehož položkami (prvky) jsou objekty Row. ``gluon.sql.Row`` objekty se chovají jako slovníky (dictionary), ale s jejich prvky můžete také pracovat jako s atributy: row['jmeno'] nebo row.jmeno. Rows objekt (sada záznamů) je read-only, do Row objekt (jednotlivý záznam) lze i měnit.

Rows objekt umožňuje v cyklu procházet jednotlivé záznamy výsledku SELECTu, např. můžete vypsat hodnoty polí pro jednotlivé záznamy:
``
>>> for row in rows:
        print row.id, row.name
1 Alex
``:code

Všechny kroky můžete spojit do jediného příkazu:
``
>>> for row in db(db.person.name=='Alex').select():
        print row.name
Alex
``:code

``ALL``:inxx

Metodě select můžete předat argumenty. Všechny nepojmenované argumenty budou chápany jako pole, které chcete z databáze získat (fetch). Např. můžete explicitně chtít pole "id" a "name":
``
>>> for row in db().select(db.person.id, db.person.name):
        print row.name
Alex
Bob
Carl
``:code

Atribut tabulky ALL vám umožňuje požadovat všechna pole tabulky:
``
>>> for row in db().select(db.person.ALL):
        print row.name
Alex
Bob
Carl
``:code

Všimněte si, že jsme nezadali žádný dotaz (query) jako argument db(). Web2py pochopí, že jestliže chcete všechna pole tabulky person bez uvedení nějaké dodatečné informace, tak chcete získat všechny záznamy této tabulky.

Ekvivalentní alternativní syntaxe je tato:
``
>>> for row in db(db.person.id > 0).select():
        print row.name
Alex
Bob
Carl
``:code

V tomto případě Web2py naopak pochopí, že chcete-li všechny záznamy tabulky person (id > 0) a neuvádíte žádnou informaci navíc (o požadovaných polích), má vrátit všechna pole této tabulky.

Vezměme nyní jeden řádek výsledku (záznam, větu tabulky):

``
row = rows[0]
``

Můžete získat obsah polí více ekvivalentními způsoby:

``
>>> row.name
Alex
>>> row['name']
Alex
>>> row('person.name')
Alex
``

Poslední uvedená syntaxe (s kulatými závorkami) je zvláště praktická, když chcete získat výraz místo pole. Ukážeme si to později.

Můžete nastavit
``
rows.compact = False
``
tím zakážete notaci
``
row.name
``
a místo ní povolíte méně stručnou notaci:
``
row.person.name
``
To je ovšem méně obvyklé a málokdy potřebné.

#### Zkratky (shortcuts)
``DAL shortcuts``:inxx

DAL nabízí různé zkratky ke zjednodušení kódu.
Podívejme se na ně:
``
myrecord = db.mytable[id]
``:code

Vrátí záznam (Row objekt) se zadaným ``id``. Pokud ``id`` v tabulce neexistuje, vrátí ``None``. Je to ekvivalentní této úplné syntaxi:

``
myrecord = db(db.mytable.id==id).select().first()
``:code

Můžete zrušit záznam pomocí id, takto:

``
del db.mytable[id]
``:code

což je ekvivalent úplného znění:

``
db(db.mytable.id==id).delete()
``:code

Záznam můžete vložit i takto:

``
db.mytable[0] = dict(myfield='somevalue')
``:code

což je ekvivalent

``
db.mytable.insert(myfield='somevalue')
``:code

a v obou případech přidá záznam, přičemž v prvním případě předáme hodnoty jako slovník (dictionary).

Můžete aktualizovat záznam:

``
db.mytable[id] = dict(myfield='somevalue')
``:code

což je ekvivalent úplné syntaxe

``
db(db.mytable.id==id).update(myfield='somevalue')
``:code

#### Získání ``Row``

Další praktická syntaxe je tato:

``
record = db.mytable(id)
record = db.mytable(db.mytable.id==id)
record = db.mytable(id,myfield='somevalue')
``:code

Je poněkud podobná syntaxi ``db.mytable[id]``, ale je flexibilnější a bezpečnější. V obou případech může být id zadáno jako integer nebo jako string (1, '1'). Ale chování se liší, pokud není vůbec zadáno číslo: db.mytable['a'] vyvolá výjimku, kdežto db.mytable('a') vrátí None (čili: neexistuje požadovaný záznam). Ve druhé variantě (s kulatými závorkami) tedy výjimka nevznikne nikdy. Druhá varianta také umožňuje zadat více podmínek, které musí být splněny současně. Pokud nejsou, je rovněž vráceno ``None``.

#### Rekurzivní ``select``
``recursive selects``:inxx

Uvažujme předcházející tabulku "person" a novou tabulku "thing", která se na záznamy z "person" odkazuje:
``
>>> db.define_table('thing',
        Field('name'),
        Field('owner_id', 'reference person'))
``:code

proveďme jednoduchý select() z této tabulky:
``
>>> things = db(db.thing).select()
``:code

což, jak jsme si ukázali, je totéž jako:

``
>>> things = db(db.thing._id>0).select()
``:code

``._id`` znamená primární klíč tabulky. Pokud se nejedná o tabulku jiné aplikace, ale o standardní tabulku Web2py aplikace, tak ``db.thing._id`` je totéž jako ``db.thing.id``, což jsme použili ve výkladu dříve a budeme tak uvádět i ve většině ostatních příkladů v této knize. ``_id``:inxx


Pro každý záznam (Row), získaný z tabulky things, je nejen možné získat pole z této tabulky (thing), ale také z relačně navázaných tabulek (rekursivně):
``
>>> for thing in things: print thing.name, thing.owner_id.name
``:code

Nicméně zde ``thing.owner_id.name`` vyvolá jeden SQL select pro každý záznam ve things, což je samozřejmě velice neefektivní. Doporučujeme proto používat joiny místo rekurzivních selectů všude, kde je to možné. Přesto tato vlastnost může být pohodlná a praktická, pokud pracujeme jen s jednotlivým záznamem nebo velmi málo záznamy.

Funguje to i zpětně, pro směr relace 1->m, tedy můžete zjistit věci (things), které patří některé osobě (person):

``
person =  db.person(id)
for thing in person.thing.select(orderby=db.thing.name):
    print person.name, 'owns', thing.name
``:code

``person.thing`` je zkratka (shortcut) pro

``
db(db.thing.owner_id==person.id)
``:code

tedy pro sadu (Set) záznamů z ``thing``, které jsou vázány na zadanou osobu ``person``. Tato zkratka ale selže, jestliže odkazovaná tabulka (things) má více klíčů, které propojují záznamy do řídící tabulky (preson). V takovém případě musíte být konkrétnější a použít druhou syntaxi (celý Dotaz).


#### Serializace ``Rows`` do views

Mějme následující akci index(), která na základě dotazu query vytvoří Rows objekt (sekvenci záznamů).
``SQLTABLE``:inxx

``
def index()
    return dict(rows = db(query).select())
``:code

Výsledek můžete zobrazit ve view touto jednoduchou syntaxí:
``
{{extend 'layout.html'}}
<h1>Záznamy</h1>
{{=rows}}
``:code

To je ekvivalentní zápisu:
``
{{extend 'layout.html'}}
<h1>Records</h1>
{{=SQLTABLE(rows)}}
``:code

``SQLTABLE`` konvertuje Rows objekt na HTML tabulku s hlavičkou se jmény sloupců a s jedním řádkem pro každý záznam. Řádky jsou doplněny střídavě o css class "even" resp. "odd". Interně je Rows objekt nejprve konvertován na SQLTABLE objekt (nazeměňujte s Table) a pak je serializován. Hodnoty z databáze jsou při tom zformátovány pomocí validátorů a escapovány pro HTML výstup.

Je také možné a někdy užitečné zavolat SQLTABLE explicitně.

SQLTABLE konstruktor má následující volitelné argumenty:

- ``linkto`` URL nebo akce pro vytvoření odkazů z polí typu reference (default je None)
- ``upload`` URL nebo akce pro umožnění downloadu souborů, které byly uploadovány (default je None)
- ``headers`` slovník (dictionary), který mapuje jména polí na hlavičky sloupců (default je ``{}``); případně to může být příkaz/povel (aktuálně podporujeme: ``headers='fieldname:capitalize'``)
- ``truncate`` počet znaků, na které budou zkráceny dlouhé údaje v tabulce
- ``columns`` seznam jmen polí, které se promítnou jako sloupce (ve formátu tablename.fieldname), neuvedené se nezobrazí (default je zobrazit vše)
- ``**attributes`` atributy pro TABLE objekt

Příklad:
``
{{extend 'layout.html'}}
<h1>Záznamy</h1>
{{=SQLTABLE(rows,
     headers='fieldname:capitalize',
     truncate=100,
     upload=URL('download'))
}}
``:code

``SQLFORM.grid``:inxx ``SQLFORM.smartgrid``:inxx

------
``SQLTABLE`` může být užitečná, ale někdy můžeme potřebovat více. ``SQLFORM.grid`` je rozšíření SQLTABLE s vyhledáváním a stránkováním a s možností vytvářet záznamy, editovat je nebo rušit. ``SQLFORM.smartgrid`` je další zobecnění, které kromě předchozího nabízí podobně funkční odkazy na záznamy relačně závislých tabulek.
------

Tady je příklad použití ``SQLFORM.grid``:

``
def index():
    return dict(grid=SQLFORM.grid(query))
``:code

a odpovídající view:

``
{{extend 'layout.html'}}
{{=grid}}
``

``SQLFORM.grid`` a ``SQLFORM.smartgrid`` by měly být preferovány před ``SQLTABLE``. Podrobněji je probíráme v kapitole 7.

#### ``orderby``, ``groupby``, ``limitby``, ``distinct``, ``having``

Příkaz ``select`` může mít pět volitelných argumentů: orderby, groupby, limitby, left a cache. Zde si vysvětlíme první tři z nich.

Takto můžete získané záznamy setřídit podle sloupce name:

``orderby``:inxx ``groupby``:inxx ``having``:inxx
``
>>> for row in db().select(
        db.person.ALL, orderby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

Můžete změnit pořadí na opačné (všimněte si znaku ~ (tilda)):
``
>>> for row in db().select(
        db.person.ALL, orderby=~db.person.name):
        print row.name
Carl
Bob
Alex
``:code

Můžete použít náhodné pořadí:
``
>>> for row in db().select(
        db.person.ALL, orderby='<random>'):
        print row.name
Carl
Alex
Bob
``:code

-----
Použití ``orderby='<random>'`` není podporováno na Google NoSQL. Ale samozřejmě můžete importovat externí modul:
``
import random
rows=db(...).select().sort(lambda row: random.random())
``:code
-----

Můžete setřídit podle dalších sloupců při shodě předchozích, tak, že je uvedete spojené znakem "|":
``
>>> for row in db().select(
        db.person.ALL, orderby=db.person.name|db.person.id):
        print row.name
Carl
Bob
Alex
``:code

Použijete-li spolu s ``orderby`` také ``groupby``, můžete sloučit záznamy se stejnou hodnotou zadaného pole (to je back-end specifické a není podporováno na Google NoSQL):
``
>>> for row in db().select(
        db.person.ALL,
        orderby=db.person.name, groupby=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

Můžete použít ``having`` spolu s ``groupby`` pro podmíněné seskupení.

``
>>> print db(query1).select(db.person.ALL, groupby=db.person.name, having=query2)
``

query1 vybírá záznamy, které se mají zobrazit, kdežto query2 záznamy, které se mají seskupit.

``distinct``:inxx

Argumentem ``distinct=True`` můžete zadat. že chcete, aby každý vrácený záznam byl unikátní (uvést duplicitní záznamy jen jednou). Je to totéž jako groupovat (seskupit) podle všech výstupních polí, s tím rozdílem, že nemusíte zadat pořadí (orderby). Rozumné použití znamená, že jednak pomocí ALL nevyberete všechna pole, jednak nevyberete pole id explicitně, protože v obou případech by id způsobilo, že každý výstupní záznam by byl unikátní a výsledek by se tedy nelišil bez ohledu na nastavení distinct.

Příklad:
``
>>> for row in db().select(db.person.name, distinct=True):
        print row.name
Alex
Bob
Carl
``:code

Jako ``distinct`` můžete uvést také výraz, např.:
``
>>> for row in db().select(db.person.name, distinct=db.person.name):
        print row.name
Alex
Bob
Carl
``:code

Pomocí limitby=(min, max) můžete vybrat část výstupních záznamů s offset od min (včetně) do max (bez něj). Např. zde chceme získat jen první dva záznamy:

``limitby``:inxx
``
>>> for row in db().select(db.person.ALL, limitby=(0, 2)):
        print row.name
Alex
Bob
``:code


#### Logické oprátory

Dotazy (query) lze kombinovat do složitějších. Binární AND (logický součin, splněný při splnění obou dílčích podmínek) zajišťuje operátor "``&``":

``and``:inxx ``or``:inxx ``not``:inxx
``
>>> rows = db((db.person.name=='Alex') & (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
4 Alex
``:code

Binární OR (logický součet, splněný při splnění aspoň jedné z dílčích podmínek) zajišťuje operátor "``|``":
``
>>> rows = db((db.person.name=='Alex') | (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
1 Alex
``:code

Můžete negovat dotaz změnou operátoru za opačný, typicky pomocí operátoru "``!=``":
``
>>> rows = db((db.person.name!='Alex') | (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
2 Bob
3 Carl
``:code

Negovat dotaz (nebo dílčí podmínku složeného dotazu) můžete také unárním operátorem "``~``". Ubární operátor se aplikuje na za ním následující výraz:
``
>>> rows = db(~(db.person.name=='Alex') | (db.person.id>3)).select()
>>> for row in rows: print row.id, row.name
2 Bob
3 Carl
``:code

------
Nelze použít operátory "``and``" a "``or``", a to vzhledem k omezením Pythonu z hlediska možnosti je předefinovat. Právě proto je potřeba používat při skládání dotazů operátory "``&``" a "``|``". Tyto operátory mají větší přednost než operátory porovnání (na rozdíl od "``and``" a "``or``"), takže je nutné používat "extra" závorky navíc kolem vzájemně spojovaných dílčích dotazů. Podobně i operátor "``~``" má větší přednost než operátory pro porovnání, takže i negovaný dílčí dotaz je potřeba uzavřít do závorky.
------

Dotazy lze také měnit pomocí in-place logických operátorů:

``
>>> query = db.person.name!='Alex'
>>> query &= db.person.id>3
>>> query |= db.person.name=='John'
``

#### ``count``, ``isempty``, ``delete``, ``update``

Spočítat záznamy můžete metodou count():

``count``:inxx ``isempty``:inxx

``
>>> print db(db.person.id > 0).count()
3
``:code

``count`` má také volitelný argument ``distinct`` (defaultně False), který se chová identicky jako u metody ``select``. ``count`` má dále argument ``cache``, který se také chová stejně jako u ``select`` metody.

Někdy potřebujete zjistit, zda je tabulka (případně sada (set)) prázdná. Efektivnější než počítat záznamy je použít metodu ``isempty``:

``
>>> print db(db.person.id > 0).isempty()
False
``:code

ekvivalentní, ale kratší, je:

``
>>> print db(db.person).isempty()
False
``:code

Můžete smazat záznamy vybrané sady:

``delete``:inxx
``
>>> db(db.person.id > 3).delete()
``:code

Můžete aktualizovat všechny záznamy sady předáním pojmenovaných argumentů metodě ``update``. Jména parametrů musí být identická se jmény aktualizovaných polí:

``update``:inxx
``
>>> db(db.person.id > 3).update(name='Ken')
``:code

#### Výrazy

Hodnota, předaná argumentu metody ``update``, může být také výraz. Např.
``
>>> db.define_table('person',
        Field('name'),
        Field('visits', 'integer', default=0))
>>> db(db.person.name == 'Massimo').update(
        visits = db.person.visits + 1)
``:code

Výraz se vyhodnotí pro každý záznam sady zvlášť.
Také dotazy mohou obsahovat výrazy:
``
>>> db.define_table('person',
        Field('name'),
        Field('visits', 'integer', default=0),
        Field('clicks', 'integer', default=0))
>>> db(db.person.visits == db.person.clicks + 1).delete()
``:code

#### ``case`` ``case``:inxx

Výraz může obsahovat kluzuli ``case``, např.:

``
>>> db.define_table('person', Field('name'))
>>> condition = db.person.name.startswith('M')
>>> yes_or_no = condition.case('Yes','No')
>>> for row in db().select(db.person.name, yes_or_no):
...     print row.person.name,  row(yes_or_no)
Max Yes
John No
``:code

#### ``update_record``

``update_record``:inxx
Web2py poskytuje také metodu pro aktualizaci jednoho záznamu současně v paměti i v databázi, ``update_record``:

``
>>> row = db(db.person.id==2).select().first()
>>> row.update_record(name='Curt')
``:code

Nezaměňujte ``update_record`` s metodou ``update`` (aplikovanou na jediný řádek)

``
>>> row.update(name='Curt')
``:code

protože metoda ``update`` aktualizuje jen row objekt (kopii záznamu v paměti), ale ne záznam v databázi.

Je však možné měnit atributy row objektu popsaným způsobem pomocí ``update`` nebo jednotlivě pomocí přiřazení, a pak zavolat ``update_record()`` bez argumentů, což uloží provedené změny:

``
>>> row = db(db.person.id > 2).select().first()
>>> row.name = 'Curt'
>>> row.update_record() # uloží dříve provedenou změnu
``:code

Metoda ``update_record`` je k dispozici JEN TEHDY, pokud je v selectu (tedy i v záznamu) pole ``id`` a pokud není nastaveno ``cacheable`` na ``True``.

#### Vkládání a aktualizace pomocí slovníku (dictionary)

Občas je potřeba vkládat nebo aktualizovat záznamy v tabulce, kdy jméno tabulky, jména dotčených polí a ukládané hodnoty jsou uloženy v proměnných. Např. ``tablename`` (proměnná se jménem tabulky), ``fieldname`` (proměnná se jménem pole, kam ukládáme), and ``value`` (proměnná s hodnotou, která má být uložena).

Pro vkládání upravíme syntaxi takto:

``
db[tablename].insert(**{fieldname:value})
``:code

Aktualizace záznamu s určeným id: ``_id``:inxx

``
db(db[tablename]._id==id).update(**{fieldname:value})
``:code

Použití ``table._id`` místo ``table.id`` je obecnější - dotaz pak funguje i pro případ cizích tabulek s polem typu "id", které je ale pojmenováno jinak.


#### ``first`` a ``last``
``first``:inxx ``last``:inxx

Z Rows objektu můžeme jako Row objekt získat první nebo poslední záznam:

``
>>> rows = db(query).select()
>>> first_row = rows.first()
>>> last_row = rows.last()
``:code

což je ekvivalent zápisu:
``
>>> first_row = rows[0] if len(rows)>0 else None
>>> last_row = rows[-1] if len(rows)>0 else None
``:code

#### ``as_dict`` a ``as_list``
``as_list``:inxx ``as_dict``:inxx

Row objekt (1 záznam) může být převeden na standardní slovník (dictionary) pomocí metody ``as_dict()``. Rows object (sekvence záznamů) může být převeden na standardní seznam (list) pomocí metody ``as_list()``:
``
>>> rows = db(query).select()
>>> rows_list = rows.as_list()
>>> first_row_dict = rows.first().as_dict()
``:code

Tyto metody mohou být užitečné ((?? for passing Rows to generic views a ??)) chceme-li uložit Rows objekt do sessions (protože Rows objekt nemůže být přímo serializován, obsahuje totiž odkaz na otevřené db připojení):
``
>>> rows = db(query).select()
>>> session.rows = rows # not allowed!
>>> session.rows = rows.as_list() # allowed!
``:code

#### Skládání Rows objektů

Rows objekty je možné slučovat na úrovni Pythonu. Dejme tomu, že máme tyto dva Rows objekty:

``
>>> print rows1
person.name
Max
Tim
>>> print rows2
person.name
John
Tim
``

Můžeme je spojit do jediného objektu:

``
>>> rows3 = rows1 & rows2
>>> print rows3
name
Max
Tim
John
Tim
``:code

Můžeme je spojit do jediného objektu s vyloučením duplicit:

``
>>> rows3 = rows1 | rows2
>>> print rows3
name
Max
Tim
John
``:code

#### ``find``, ``exclude``, ``sort``
``find``:inxx ``exclude``:inxx ``sort``:inxx

Někdy je potřeba provést dva selecty, přičemž druhý zahrnuje podmnožinu předchozího selectu. V takovém případě není vhodné zatěžovat zbytečně databázi dalším dotazem. Metody ``find``, ``exclude`` a ``sort`` umožňují pracovat s Rows objektem a vytvořit z jeho (všech nebo některých) záznamů jiný, bez přístupu do databáze:
- ``find`` vrátí nový Rows objekt s vybranými záznamy podle podmínky; originál zůstane beze změny.
- ``exclude`` vrátí nový Rows objekt s vybranými záznamy podle podmínky; z původního Rows objektu budou vyhovující záznamy odstraněny.
- ``sort`` vrátí nový Rows objekt setříděný podle zadaného výrazu; originál zůstane beze změny.

Všechny metody mají shodný parametr: funkci, která se provede nad každým jednotlivým řádkem.

Příklad:
``
>>> db.define_table('person', Field('name'))
>>> db.person.insert(name='John')
>>> db.person.insert(name='Max')
>>> db.person.insert(name='Alex')
>>> rows = db(db.person).select()
>>> for row in rows.find(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
3
>>> for row in rows.exclude(lambda row: row.name[0]=='M'):
        print row.name
Max
>>> print len(rows)
2
>>> for row in rows.sort(lambda row: row.name):
        print row.name
Alex
John
``:code

Další příklad - volání dvou metod v jednom příkazu:
``
>>> rows = db(db.person).select()
>>> rows = rows.find(
        lambda row: 'x' in row.name).sort(
            lambda row: row.name)
>>> for row in rows:
        print row.name
Alex
Max
``:code

``Sort`` může mít parametr ``reverse=True`` pro opačné setřídění.

Metoda ``find`` má volitelný argument ``limitby`` se stejnou syntaxí a funkcí jako u ``select`` metody objektu Set.



### Další metody

#### ``update_or_insert``
``update_or_insert``:inxx

Někdy potřebujete vložit záznam jen tehdy, pokud ještě neexistuje záznam s danými hodnotami.
To je možné provést takto:

``
db.define_table('person', Field('name'), Field('birthplace'))
db.person.update_or_insert(name='John', birthplace='Chicago')
``:code

Záznam se založí jen když dosud neevidujeme žádného Johna z Chicaga.

Jiná verze může být užitečnější: Jako poziční (nepojmenovaný) první parametr můžeme uvést Dotaz (query), pomocí něhož se určí, zda vhodný záznam existuje nebo ne:
``
db.person.update_or_insert(db.person.name=='John',
     name='John', birthplace='Chicago')
``:code

Jestliže John je v evidenci, aktualizuje se jeho místo narození. V opačném případě se vytvoří nový záznam.

Jiný příklad se složitějším dotazem:
``
db.person.update_or_insert((db.person.name=='John') & (db.person.birthplace=='Chicago'),
     name='John', birthplace='Chicago', pet='Rover')
``:code

#### ``validate_and_insert``, ``validate_and_update``

``validate_and_insert``:inxx ``validate_and_update``:inxx

Funkce

``
ret = db.mytable.validate_and_insert(field='value')
``:code

pracuje podobně jako

``
id = db.mytable.insert(field='value')
``:code

s tím rozdílem, že nejprve validuje zadaná pole a nevykoná insert(), jestliže validace selže. Jestliže validace selžou, problémy lze nalézt ve vráceném objektu v ``ret.error``. Jestliže validace projdou a záznam se uloží, přidělené id je v ``ret.id``. Vzhledem k tomu, že validace se provádí automaticky během logiky zpracování formuláře, není tato funkce běžně potřeba.

Obdobně

``
ret = db(query).validate_and_update(field='value')
``:code

pracuje podobně jako

``
num = db(query).update(field='value')
``:code

ale s provedením validace předem. To funguje jedině tehdy, když dotaz (query) pracuje jen s jedinou tabulkou (bez joinů). Počet ovlivněných záznamů lze pak najít v ``res.updated`` a chyby v ``ret.errors``.

#### ``smart_query`` (experimentální)

Umožňuje parsovat dotaz v přirozeném jazyce (anglicky)

``
name contain m and age greater than 18
``

DAL umí parsovat takovýto dotaz:

``
search = 'name contain m and age greater than 18'
rows = db.smart_query([db.person], search).select()
``

První argument musí být seznam tabulek nebo polí, které jsou v dotazu dovoleny. Při chybě v řetězci ``search`` je vyvolán ``RuntimeError``. Tato funkcionalita může být použita při tvorbě RESTful interface (více v kapitole 10) a je interně použita u ``SQLFORM.grid`` a ``SQLFORM.smartgrid``.

V řetězci pro smart_query může být pole určeno jen jako samotné jméno pole (fieldname) nebo ve formátu tablename.fieldname. Řetězce lze omezit uvozovkami (double quotes), jestliže obsahují mezery.

### Vypočtená pole (computed fields)
``compute``:inxx

DAL pole může mít atribut ``compute``. Musí to být funkce (často lambda funkce), která vezme parametr Row objekt a vrátí hodnotu pole. Když je vkládán nový záznam nebo aktualizován existující, pak není-li hodnota pole explicitně uvedena, Web2py ji určí výpočtem pomocí ``compute`` funkce:
``
>>> db.define_table('item',
        Field('unit_price','double'),
        Field('quantity','integer'),
        Field('total_price',
            compute=lambda r: r['unit_price']*r['quantity']))
>>> r = db.item.insert(unit_price=1.99, quantity=5)
>>> print r.total_price
9.95
``:code

Poznamenejme, že vypočtená hodnota je uložena v poli v databázi a nepočítá se až při získání (načtení) hodnoty, jako je tomu u virtuálních polí, která popisujeme níže. Dvě typická použití "vypočtených polí" jsou:
- ve wiki aplikacích pro uložení textu ve verzi symbolického jazyka do verze HTML, aby nebylo nutné překládat symbolický jazyk do HTML až později, při zobrazení článku.
- při vyhledávání, k určení normalizované verze hodnoty pro vyhledávání.

### Virtuální pole

``virtual fields``:inxx

Virtuální pole jsou rovněž počítaná pole, ale liší se tím, že jsou "virtuální" v tom smyslu, že ve skutečnosti neexistují v databázi. Jsou vypočtena vždy až v okamžiku, kdy je záznam získán z databáze. Mohou být použita ke zjednodušení kódu, který se záznamem pracuje, ale nelze podle nich (z pochopitelných důvodů) vyhledávat.

#### Nový styl virtuálních polí

Novější verze Web2py poskytují nový snadnější způsob definice virtuálních polí a virtuálních polí s odloženým vyčíslením (lazy virtual fields). Tuto sekci považujeme ještě za experimentální, protože API těchto virtuálních polí (příkazy pro práci s nimi) se ještě může změnit vůči popisu, který uvádíme zde.

Zde uvažujme stejný příklad jako v minulém odstavci, konkrétně tento model:

``
>>> db.define_table('item',
        Field('unit_price','double'),
        Field('quantity','integer'),
``:code

Můžeme definovat virtuální pole ``total_price`` takto

``
>>> db.item.total_price = Field.Virtual(
    lambda row: row.item.unit_price*row.item.quantity)
``:code

Jediný argument konstruktoru pro ``Field.Virtual`` je funkce, která pro zadaný Row objekt vrátí hodnotu virtuálního pole.

Takto definované virtuální pole je automaticky spočteno pro všechny záznamy poté, co jsou získány select-em:

``
>>> for row in db(db.item).select(): print row.total_price
``

Je také možné virtruální pole definovat jako metodu, která spočte hodnotu až když ji budeme opravdu potřebovat.
Například:

``
>>> db.item.discounted_total = Field.Method(lambda row, discount=0.0: \
       row.item.unit_price*row.item.quantity*(1.0-discount/100))
``:code

V tomto případě ``row.discounted_total`` není hodnota, ale funkce. Tato funkce má takové parametry, jaké jsme uvedli při definici, ale s výjimkou prvního parametru (row), který se při volání neuvádí a je předán implicitně (představme si jej jako obdobu pythonovského "self" pro row objekt virtuální metody).

Lazy pole (s odloženým vyčíslením) ve výše uvedeném příkladu dovoluje spočítat výslednou cenu každé položky ``item``:

``
>>> for row in db(db.item).select(): print row.discounted_total()
``

přičemž současně lze zadat volitelnou slevu jako argument ``discount``, např. 15%:

``
>>> for row in db(db.item).select(): print row.discounted_total(15)
``

Virtuální pole Virtual a Method nemusíme definovat až dodatečně, ale můžeme je také definovat přímo jako součást definice tabulky:

``
>>> db.define_table('item',
        Field('unit_price', 'double'),
        Field('quantity', 'integer'),
        Field.Virtual('total_price', lambda row: ...),
        Field.Method('discounted_total', lambda row, discount=0.0: ...))
``:code


------
Pamatujte, že virtuální pole nemají stejné atributy jako ostatní pole (default, readable, requires, apod.). Ve starších verzích Web2py chyběly v seznamu ``db.table.fields``. Vyžadují zvláštní ošetření při použití v SQLFORM.grid a SQLFORM.smartgrid - více o gridu a virtuálních polích je diskutováno v kapitole o Formulářích.
------

#### Old style virtual fields

In order to define one or more virtual fields, you can also define a container class, instantiate it and link it to a table or to a select. For example, consider the following table:

``
>>> db.define_table('item',
        Field('unit_price','double'),
        Field('quantity','integer'),
``:code

One can define a ``total_price`` virtual field as
``
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price*self.item.quantity
>>> db.item.virtualfields.append(MyVirtualFields())
``:code

Notice that each method of the class that takes a single argument (self) is a new virtual field. ``self`` refers to each one row of the select. Field values are referred by full path as in ``self.item.unit_price``. The table is linked to the virtual fields by appending an instance of the class to the table's ``virtualfields`` attribute.

Virtual fields can also access recursive fields as in
``
>>> db.define_table('item',
        Field('unit_price','double'))
>>> db.define_table('order_item',
        Field('item','reference item'),
        Field('quantity','integer'))
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.order_item.item.unit_price \
                * self.order_item.quantity
>>> db.order_item.virtualfields.append(MyVirtualFields())
``:code

Notice the recursive field access ``self.order_item.item.unit_price`` where ``self`` is the looping record.

They can also act on the result of a JOIN
``
>>> db.define_table('item',
        Field('unit_price','double'))
>>> db.define_table('order_item',
        Field('item','reference item'),
        Field('quantity','integer'))
>>> rows = db(db.order_item.item==db.item.id).select()
>>> class MyVirtualFields(object):
        def total_price(self):
            return self.item.unit_price \
                * self.order_item.quantity
>>> rows.setvirtualfields(order_item=MyVirtualFields())
>>> for row in rows: print row.order_item.total_price
``:code

Notice how in this case the syntax is different. The virtual field accesses both ``self.item.unit_price`` and ``self.order_item.quantity`` which belong to the join select. The virtual field is attached to the rows of the table using the ``setvirtualfields`` method of the rows object. This method takes an arbitrary number of named arguments and can be used to set multiple virtual fields, defined in multiple classes, and attach them to multiple tables:
``
>>> class MyVirtualFields1(object):
        def discounted_unit_price(self):
            return self.item.unit_price*0.90
>>> class MyVirtualFields2(object):
        def total_price(self):
            return self.item.unit_price \
                * self.order_item.quantity
        def discounted_total_price(self):
            return self.item.discounted_unit_price \
                * self.order_item.quantity
>>> rows.setvirtualfields(
        item=MyVirtualFields1(),
        order_item=MyVirtualFields2())
>>> for row in rows:
        print row.order_item.discounted_total_price
``:code

Virtual fields can be ''lazy''; all they need to do is return a function and access it by calling the function:
``
>>> db.define_table('item',
        Field('unit_price','double'),
        Field('quantity','integer'),
>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            def lazy(self=self):
                return self.item.unit_price \
                    * self.item.quantity
            return lazy
>>> db.item.virtualfields.append(MyVirtualFields())
>>> for item in db(db.item).select():
        print item.lazy_total_price()
``:code

or shorter using a lambda function:
``
>>> class MyVirtualFields(object):
        def lazy_total_price(self):
            return lambda self=self: self.item.unit_price \
                * self.item.quantity
``:code

### One to many relation
``one to many``:inxx

To illustrate how to implement one to many relations with the web2py DAL, define another table "thing" that refers to the table "person" which we redefine here:
``
>>> db.define_table('person',
                    Field('name'),
                    format='%(name)s')
>>> db.define_table('thing',
                    Field('name'),
                    Field('owner_id', 'reference person'),
                    format='%(name)s')
``:code

Table "thing" has two fields, the name of the thing and the owner of the thing. The "owner_id" field id a reference field. A reference type can be specified in two equivalent ways:

``
Field('owner_id', 'reference person')
Field('owner_id', db.person)
``:code

The latter is always converted to the former. They are equivalent except in the case of lazy tables, self references or other types of cyclic references where the former notation is the only allowed notation.

When a field type is another table, it is intended that the field reference the other table by its id. In fact, you can print the actual type value and get:
``
>>> print db.thing.owner_id.type
reference person
``:code

Now, insert three things, two owned by Alex and one by Bob:
``
>>> db.thing.insert(name='Boat', owner_id=1)
1
>>> db.thing.insert(name='Chair', owner_id=1)
2
>>> db.thing.insert(name='Shoes', owner_id=2)
3
``:code

You can select as you did for any other table:
``
>>> for row in db(db.thing.owner_id==1).select():
        print row.name
Boat
Chair
``:code

Because a thing has a reference to a person, a person can have many things, so a record of table person now acquires a new attribute thing, which is a Set, that defines the things of that person. This allows looping over all persons and fetching their things easily:

``referencing``:inxx
``
>>> for person in db().select(db.person.ALL):
        print person.name
        for thing in person.thing.select():
            print '    ', thing.name
Alex
     Boat
     Chair
Bob
     Shoes
Carl
``:code

#### Inner joins

Another way to achieve a similar result is by using a join, specifically an INNER JOIN. web2py performs joins automatically and transparently when the query links two or more tables as in the following example:

``Rows``:inxx ``inner join``:inxx ``join``:inxx
``
>>> rows = db(db.person.id==db.thing.owner_id).select()
>>> for row in rows:
        print row.person.name, 'has', row.thing.name
Alex has Boat
Alex has Chair
Bob has Shoes
``:code

Observe that web2py did a join, so the rows now contain two records, one from each table, linked together. Because the two records may have fields with conflicting names, you need to specify the table when extracting a field value from a row. This means that while before you could do:
``
row.name
``:code

and it was obvious whether this was the name of a person or a thing, in the result of a join you have to be more explicit and say:
``
row.person.name
``:code

or:
``
row.thing.name
``:code

There is an alternative syntax for INNER JOINS:
``
>>> rows = db(db.person).select(join=db.thing.on(db.person.id==db.thing.owner_id))
>>> for row in rows:
    print row.person.name, 'has', row.thing.name
Alex has Boat
Alex has Chair
Bob has Shoes
``:code

While the output is the same, the generated SQL in the two cases can be different. The latter syntax removes possible ambiguities when the same table is joined twice and aliased:

``
>>> db.define_table('thing',
        Field('name'),
        Field('owner_id1','reference person'),
        Field('owner_id2','reference person'))
>>> rows = db(db.person).select(
    join=[db.person.with_alias('owner_id1').on(db.person.id==db.thing.owner_id1).
          db.person.with_alias('owner_id2').on(db.person.id==db.thing.owner_id2)])
``

The value of ``join`` can be list of ``db.table.on(...)`` to join.

#### Left outer join

Notice that Carl did not appear in the list above because he has no things. If you intend to select on persons (whether they have things or not) and their things (if they have any), then you need to perform a LEFT OUTER JOIN. This is done using the argument "left" of the select command. Here is an example:

``Rows``:inxx ``left outer join``:inxx ``outer join``:inxx
``
>>> rows=db().select(
        db.person.ALL, db.thing.ALL,
        left=db.thing.on(db.person.id==db.thing.owner_id))
>>> for row in rows:
        print row.person.name, 'has', row.thing.name
Alex has Boat
Alex has Chair
Bob has Shoes
Carl has None
``:code

where:
``
left = db.thing.on(...)
``:code

does the left join query. Here the argument of ``db.thing.on`` is the condition required for the join (the same used above for the inner join). In the case of a left join, it is necessary to be explicit about which fields to select.

Multiple left joins can be combined by passing a list or tuple of ``db.mytable.on(...)`` to the  ``left`` attribute.

#### Grouping and counting

When doing joins, sometimes you want to group rows according to certain criteria and count them. For example, count the number of things owned by every person. web2py allows this as well. First, you need a count operator. Second, you want to join the person table with the thing table by owner. Third, you want to select all rows (person + thing), group them by person, and count them while grouping:

``grouping``:inxx
``
>>> count = db.person.id.count()
>>> for row in db(db.person.id==db.thing.owner_id).select(
        db.person.name, count, groupby=db.person.name):
        print row.person.name, row[count]
Alex 2
Bob 1
``:code

Notice the ``count`` operator (which is built-in) is used as a field. The only issue here is in how to retrieve the information. Each row clearly contains a person and the count, but the count is not a field of a person nor is it a table. So where does it go? It goes into the storage object representing the record with a key equal to the query expression itself. The count method of the Field object has an optional ``distinct`` argument. When set to ``True`` it specifies that only distinct values of the field in question are to be counted.

### Many to many
``many-to-many``:inxx
In the previous examples, we allowed a thing to have one owner but one person could have many things. What if Boat was owned by Alex and Curt? This requires a many-to-many relation, and it is realized via an intermediate table that links a person to a thing via an ownership relation.

Here is how to do it:
``
>>> db.define_table('person',
                    Field('name'))
>>> db.define_table('thing',
                    Field('name'))
>>> db.define_table('ownership',
                    Field('person', 'reference person'),
                    Field('thing', 'reference thing'))
``:code

the existing ownership relationship can now be rewritten as:
``
>>> db.ownership.insert(person=1, thing=1) # Alex owns Boat
>>> db.ownership.insert(person=1, thing=2) # Alex owns Chair
>>> db.ownership.insert(person=2, thing=3) # Bob owns Shoes

``:code

Now you can add the new relation that Curt co-owns Boat:
``
>>> db.ownership.insert(person=3, thing=1) # Curt owns Boat too

``:code

Because you now have a three-way relation between tables, it may be convenient to define a new set on which to perform operations:
``
>>> persons_and_things = db(
        (db.person.id==db.ownership.person) \
        & (db.thing.id==db.ownership.thing))
``:code

Now it is easy to select all persons and their things from the new Set:
``
>>> for row in persons_and_things.select():
        print row.person.name, row.thing.name
Alex Boat
Alex Chair
Bob Shoes
Curt Boat
``:code

Similarly, you can search for all things owned by Alex:
``
>>> for row in persons_and_things(db.person.name=='Alex').select():
        print row.thing.name
Boat
Chair
``:code

and all owners of Boat:
``
>>> for row in persons_and_things(db.thing.name=='Boat').select():
        print row.person.name
Alex
Curt
``:code

A lighter alternative to Many 2 Many relations is tagging. Tagging is discussed in the context of the ``IS_IN_DB`` validator. Tagging works even on database backends that do not support JOINs like the Google App Engine NoSQL.

### ``list:<type>``, and ``contains``
``list:string``:inxx
``list:integer``:inxx
``list:reference``:inxx
``contains``:inxx
``multiple``:inxx
``tags``:inxx

web2py provides the following special field types:

``
list:string
list:integer
list:reference <table>
``:code

They can contain lists of strings, of integers and of references respectively.

On Google App Engine NoSQL ``list:string`` is mapped into ``StringListProperty``, the other two are mapped into ``ListProperty(int)``. On relational databases they all mapped into text fields which contain the list of items separated by ``|``. For example ``[1,2,3]`` is mapped into ``|1|2|3|``.

For lists of string the items are escaped so that any ``|`` in the item is replaced by a ``||``. Anyway this is an internal representation and it is transparent to the user.

You can use ``list:string``, for example, in the following way:

``
>>> db.define_table('product',
        Field('name'),
        Field('colors','list:string'))
>>> db.product.colors.requires=IS_IN_SET(('red','blue','green'))
>>> db.product.insert(name='Toy Car',colors=['red','green'])
>>> products = db(db.product.colors.contains('red')).select()
>>> for item in products:
        print item.name, item.colors
Toy Car ['red', 'green']
``:code

``list:integer`` works in the same way but the items must be integers.

As usual the requirements are enforced at the level of forms, not at the level of ``insert``.

------
For ``list:<type>`` fields the ``contains(value)`` operator maps into a non trivial query that checks for lists containing the ``value``.  The ``contains`` operator also works for regular ``string`` and ``text`` fields and it maps into a ``LIKE '%value%'``.
------

The ``list:reference`` and the ``contains(value)`` operator are particularly useful to de-normalize many-to-many relations. Here is an example:

``
>>> db.define_table('tag',Field('name'),format='%(name)s')
>>> db.define_table('product',
        Field('name'),
        Field('tags','list:reference tag'))
>>> a = db.tag.insert(name='red')
>>> b = db.tag.insert(name='green')
>>> c = db.tag.insert(name='blue')
>>> db.product.insert(name='Toy Car',tags=[a, b, c])
>>> products = db(db.product.tags.contains(b)).select()
>>> for item in products:
        print item.name, item.tags
Toy Car [1, 2, 3]
>>> for item in products:
        print item.name, db.product.tags.represent(item.tags)
Toy Car red, green, blue
``:code

Notice that a ``list:reference tag`` field get a default constraint

``
requires = IS_IN_DB(db,'tag.id',db.tag._format,multiple=True)
``:code

that produces a ``SELECT/OPTION`` multiple drop-box in forms.

Also notice that this field gets a default ``represent`` attribute which represents the list of references as a comma-separated list of formatted references. This is used in read forms and ``SQLTABLE``s.

-----
While ``list:reference`` has a default validator and a default representation, ``list:integer`` and ``list:string`` do not. So these two need an ``IS_IN_SET`` or an ``IS_IN_DB`` validator if you want to use them in forms.
-----


### Other operators

web2py has other operators that provide an API to access equivalent SQL operators.
Let's define another table "log" to store security events, their event_time and severity, where the severity is an integer number.

``date``:inxx ``datetime``:inxx ``time``:inxx
``
>>> db.define_table('log', Field('event'),
                           Field('event_time', 'datetime'),
                           Field('severity', 'integer'))
``:code

As before, insert a few events, a "port scan", an "xss injection" and an "unauthorized login".
For the sake of the example, you can log events with the same event_time but with different severities (1, 2, and 3 respectively).
``
>>> import datetime
>>> now = datetime.datetime.now()
>>> print db.log.insert(
        event='port scan', event_time=now, severity=1)
1
>>> print db.log.insert(
        event='xss injection', event_time=now, severity=2)
2
>>> print db.log.insert(
        event='unauthorized login', event_time=now, severity=3)
3
``:code

#### ``like``, ``regexp``, ``startswith``, ``contains``, ``upper``, ``lower``

``like``:inxx ``startswith``:inxx ``regexp``:inxx
``contains``:inxx ``upper``:inxx ``lower``:inxx

Fields have a like operator that you can use to match strings:

``
>>> for row in db(db.log.event.like('port%')).select():
        print row.event
port scan
``:code

Here "port%" indicates a string starting with "port". The percent sign character, "%", is a wild-card character that means "any sequence of characters".

The like operator is case-insensitive but it can be made case-sensitive with

``
db.mytable.myfield.like('value',case_sensitive=True)
``:code


web2py also provides some shortcuts:

``
db.mytable.myfield.startswith('value')
db.mytable.myfield.contains('value')
``:code

which are equivalent respectively to

``
db.mytable.myfield.like('value%')
db.mytable.myfield.like('%value%')
``:code

Notice that ``contains`` has a special meaning for ``list:<type>`` fields and it was discussed in a previous section.

The ``contains`` method can also be passed a list of values and an optional boolean argument ``all`` to search for records that contain all values:

``
db.mytable.myfield.contains(['value1','value2'], all=True)
``
or any value from the list
``
db.mytable.myfield.contains(['value1','value2'], all=false)
``

There is a also a ``regexp`` method that works like the ``like`` method but allows regular expression syntax for the look-up expression. It is only supported by PostgreSQL and SQLite.

The ``upper`` and ``lower`` methods allow you to convert the value of the field to upper or lower case, and you can also combine them with the like operator:

``upper``:inxx ``lower``:inxx
``
>>> for row in db(db.log.event.upper().like('PORT%')).select():
        print row.event
port scan
``:code

#### ``year``, ``month``, ``day``, ``hour``, ``minutes``, ``seconds``
``hour``:inxx ``minutes``:inxx ``seconds``:inxx ``day``:inxx ``month``:inxx ``year``:inxx

The date and datetime fields have day, month and year methods. The datetime and time fields have hour, minutes and seconds methods. Here is an example:

``
>>> for row in db(db.log.event_time.year()==2013).select():
        print row.event
port scan
xss injection
unauthorized login
``:code

#### ``belongs``

The SQL IN operator is realized via the belongs method which returns true when the field value belongs to the specified set (list or tuples):

``belongs``:inxx
``
>>> for row in db(db.log.severity.belongs((1, 2))).select():
        print row.event
port scan
xss injection
``:code

The DAL also allows a nested select as the argument of the belongs operator. The only caveat is that the nested select has to be a ``_select``, not a ``select``, and only one field has to be selected explicitly, the one that defines the set.

``nested select``:inxx
``
>>> bad_days = db(db.log.severity==3)._select(db.log.event_time)
>>> for row in db(db.log.event_time.belongs(bad_days)).select():
        print row.event
port scan
xss injection
unauthorized login
``:code

In those cases where a nested select is required and the look-up field is a reference we can also use a query as argument. For example:

``
db.define_table('person', Field('name'))
db.define_table('thing', Field('name'), Field('owner_id', 'reference thing'))
db(db.thing.owner_id.belongs(db.person.name=='Jonathan')).select()
``:code

In this case it is obvious that the next select only needs the field referenced by the ``db.thing.owner_id`` field so we do not need the more verbose ``_select`` notation.

``nested_select``:inxx

A nested select can also be used as insert/update value but in this case the syntax is different:

``
lazy = db(db.person.name=='Jonathan').nested_select(db.person.id)
db(db.thing.id==1).update(owner_id = lazy)
``:code

In this case ``lazy`` is a nested expression that computes the ``id`` of person "Jonathan". The two lines result in one single SQL query.

#### ``sum``, ``avg``, ``min``, ``max`` and ``len``

``sum``:inxx ``avg``:inxx ``min``:inxx ``max``:inxx
Previously, you have used the count operator to count records. Similarly, you can use the sum operator to add (sum) the values of a specific field from a group of records. As in the case of count, the result of a sum is retrieved via the store object:
``
>>> sum = db.log.severity.sum()
>>> print db().select(sum).first()[sum]
6
``:code

You can also use ``avg``, ``min``, and ``max`` to the average, minimum, and maximum value respectively for the selected records. For example:

``
>>> max = db.log.severity.max()
>>> print db().select(max).first()[max]
3
``:code

``.len()`` computes the length of a string, text or boolean fields.

Expressions can be combined to form more complex expressions. For example here we are computing the sum of the length of all the severity strings in the logs, increased of one:

``
>>> sum = (db.log.severity.len()+1).sum()
>>> print db().select(sum).first()[sum]
``:code

#### Substrings

One can build an expression to refer to a substring. For example, we can group things whose name starts with the same three characters and select only one from each group:

``
db(db.thing).select(distinct = db.thing.name[:3])
``:code


#### Default values with ``coalesce`` and ``coalesce_zero``

There are times when you need to pull a value from database but also need a default values if the value for a record is set to NULL. In SQL there is a keyword, ``COALESCE``, for this. web2py has an equivalent ``coalesce`` method:

``
>>> db.define_table('sysuser',Field('username'),Field('fullname'))
>>> db.sysuser.insert(username='max',fullname='Max Power')
>>> db.sysuser.insert(username='tim',fullname=None)
print db(db.sysuser).select(db.sysuser.fullname.coalesce(db.sysuser.username))
"COALESCE(sysuser.fullname,sysuser.username)"
Max Power
tim
``

Other times you need to compute a mathematical expression but some fields have a value set to None while it should be zero.
``coalesce_zero`` comes to the rescue by defaulting None to zero in the query:

``
>>> db.define_table('sysuser',Field('username'),Field('points'))
>>> db.sysuser.insert(username='max',points=10)
>>> db.sysuser.insert(username='tim',points=None)
>>> print db(db.sysuser).select(db.sysuser.points.coalesce_zero().sum())
"SUM(COALESCE(sysuser.points,0))"
10
``

### Generating raw sql
``raw SQL``:inxx

Sometimes you need to generate the SQL but not execute it. This is easy to do with web2py since every command that performs database IO has an equivalent command that does not, and simply returns the SQL that would have been executed. These commands have the same names and syntax as the functional ones, but they start with an underscore:

Here is ``_insert`` ``_insert``:inxx
``
>>> print db.person._insert(name='Alex')
INSERT INTO person(name) VALUES ('Alex');
``:code

Here is ``_count`` ``_count``:inxx
``
>>> print db(db.person.name=='Alex')._count()
SELECT count(*) FROM person WHERE person.name='Alex';
``:code

Here is ``_select`` ``_select``:inxx
``
>>> print db(db.person.name=='Alex')._select()
SELECT person.id, person.name FROM person WHERE person.name='Alex';
``:code

Here is ``_delete`` ``_delete``:inxx
``
>>> print db(db.person.name=='Alex')._delete()
DELETE FROM person WHERE person.name='Alex';
``:code

And finally, here is ``_update`` ``_update``:inxx
``
>>> print db(db.person.name=='Alex')._update()
UPDATE person SET  WHERE person.name='Alex';
``:code

-----
Moreover you can always use ``db._lastsql`` to return the most recent
SQL code, whether it was executed manually using executesql or was SQL
generated by the DAL.
-----

### Exporting and importing data
``export``:inxx ``import``:inxx

#### CSV (one Table at a time)

When a Rows object is converted to a string it is automatically
serialized in CSV:

``csv``:inxx
``
>>> rows = db(db.person.id==db.thing.owner_id).select()
>>> print rows
person.id,person.name,thing.id,thing.name,thing.owner_id
1,Alex,1,Boat,1
1,Alex,2,Chair,1
2,Bob,3,Shoes,2
``:code

You can serialize a single table in CSV and store it in a file "test.csv":
``
>>> open('test.csv', 'wb').write(str(db(db.person.id).select()))
``:code

This is equivalent to

``
>>> rows = db(db.person.id).select()
>>> rows.export_to_csv_file(open('test.csv', 'wb'))
``:code

You can read the CSV file back with:
``
>>> db.person.import_from_csv_file(open('test.csv', 'r'))
``:code

When importing, web2py looks for the field names in the CSV header. In this example, it finds two columns: "person.id" and "person.name". It ignores the "person." prefix, and it ignores the "id" fields. Then all records are appended and assigned new ids. Both of these operations can be performed via the appadmin web interface.

#### CSV (all tables at once)

In web2py, you can backup/restore an entire database with two commands:

To export:
``
>>> db.export_to_csv_file(open('somefile.csv', 'wb'))
``:code

To import:
``
>>> db.import_from_csv_file(open('somefile.csv', 'rb'))
``:code

This mechanism can be used even if the importing database is of a different type than the exporting database. The data is stored in "somefile.csv" as a CSV file where each table starts with one line that indicates the tablename, and another line with the fieldnames:
``
TABLE tablename
field1, field2, field3, ...
``:code

Two tables are separated ``\r\n\r\n``. The file ends with the line
``
END
``:code

The file does not include uploaded files if these are not stored in the database. In any case it is easy enough to zip the "uploads" folder separately.

When importing, the new records will be appended to the database if it is not empty. In general the new imported records will not have the same record id as the original (saved) records but web2py will restore references so they are not broken, even if the id values may change.

If a table contains a field called
"uuid", this field will be used to identify duplicates.  Also, if an
imported record has the same "uuid" as an existing record, the
previous record will be updated.

#### CSV and remote database synchronization

Consider the following model:
``
db = DAL('sqlite:memory:')
db.define_table('person',
    Field('name'),
    format='%(name)s')
db.define_table('thing',
    Field('owner_id', 'reference person'),
    Field('name'),
    format='%(name)s')

if not db(db.person).count():
    id = db.person.insert(name="Massimo")
    db.thing.insert(owner_id=id, name="Chair")
``:code

Each record is identified by an ID and referenced by that ID. If you
have two copies of the database used by distinct web2py installations,
the ID is unique only within each database and not across the databases.
This is a problem when merging records from different databases.

In order to make a record uniquely identifiable across databases, they
must:
- have a unique id (UUID),
- have an event_time (to figure out which one is more recent if multiple copies),
- reference the UUID instead of the id.

This can be achieved without modifying web2py. Here is what to do:

Change the above model into:

``
db.define_table('person',
    Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
    Field('modified_on', 'datetime', default=request.now),
    Field('name'),
    format='%(name)s')

db.define_table('thing',
    Field('uuid', length=64, default=lambda:str(uuid.uuid4())),
    Field('modified_on', 'datetime', default=request.now),
    Field('owner_id', length=64),
    Field('name'),
    format='%(name)s')

db.thing.owner_id.requires = IS_IN_DB(db,'person.uuid','%(name)s')

if not db(db.person.id).count():
    id = uuid.uuid4()
    db.person.insert(name="Massimo", uuid=id)
    db.thing.insert(owner_id=id, name="Chair")
``:code

-------
Notice that in the above table definitions, the default value for the two ``uuid`` fields is set to a lambda function, which returns a UUID (converted to a string). The lambda function is called once for each record inserted, ensuring that each record gets a unique UUID, even if multiple records are inserted in a single transaction.
-------

Create a controller action to export the database:

``
def export():
    s = StringIO.StringIO()
    db.export_to_csv_file(s)
    response.headers['Content-Type'] = 'text/csv'
    return s.getvalue()
``:code

Create a controller action to import a saved copy of the other database and sync records:

``
def import_and_sync():
    form = FORM(INPUT(_type='file', _name='data'), INPUT(_type='submit'))
    if form.process().accepted:
        db.import_from_csv_file(form.vars.data.file,unique=False)
        # for every table
        for table in db.tables:
            # for every uuid, delete all but the latest
            items = db(db[table]).select(db[table].id,
                       db[table].uuid,
                       orderby=db[table].modified_on,
                       groupby=db[table].uuid)
            for item in items:
                db((db[table].uuid==item.uuid)&\
                   (db[table].id!=item.id)).delete()
    return dict(form=form)
``:code

Optionally you should create an index manually to make the search by uuid faster.


``XML-RPC``:inxx
Alternatively, you can use XML-RPC to export/import the file.

If the records reference uploaded files, you also need to export/import the content of the uploads folder. Notice that files therein are already labeled by UUIDs so you do not need to worry about naming conflicts and references.

#### HTML and XML (one Table at a time)

``Rows objects``:inxx
Rows objects also have an ``xml`` method (like helpers) that serializes it to XML/HTML:

``HTML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print rows.xml()
<table>
  <thead>
    <tr>
      <th>person.id</th>
      <th>person.name</th>
      <th>thing.id</th>
      <th>thing.name</th>
      <th>thing.owner_id</th>
    </tr>
  </thead>
  <tbody>
    <tr class="even">
      <td>1</td>
      <td>Alex</td>
      <td>1</td>
      <td>Boat</td>
      <td>1</td>
    </tr>
    ...
  </tbody>
</table>
``:code

``Rows custom tags``:inxx
If you need to serialize the Rows in any other XML format with custom tags, you can easily do that using the universal TAG helper and the * notation:
``XML``:inxx

``
>>> rows = db(db.person.id > 0).select()
>>> print TAG.result(*[TAG.row(*[TAG.field(r[f], _name=f) \
          for f in db.person.fields]) for r in rows])
<result>
  <row>
    <field name="id">1</field>
    <field name="name">Alex</field>
  </row>
  ...
</result>
``:code

#### Data representation

``export_to_csv_file``:inxx
The ``export_to_csv_file`` function accepts a keyword argument named ``represent``. When ``True`` it will use the columns ``represent`` function while exporting the data instead of the raw data.

``colnames``:inxx
The function also accepts a keyword argument named ``colnames`` that should contain a list of column names one wish to export. It defaults to all columns.

Both ``export_to_csv_file`` and ``import_from_csv_file`` accept keyword arguments that tell the csv parser the format to save/load the files:
- ``delimiter``: delimiter to separate values (default ',')
- ``quotechar``: character to use to quote string values (default to double quotes)
- ``quoting``: quote system (default ``csv.QUOTE_MINIMAL``)

Here is some example usage:
``
>>> import csv
>>> rows = db(query).select()
>>> rows.export_to_csv_file(open('/tmp/test.txt', 'w'),
        delimiter='|',
        quotechar='"',
        quoting=csv.QUOTE_NONNUMERIC)
``:code

Which would render something similar to
``
"hello"|35|"this is the text description"|"2013-03-03"
``:code

For more information consult the official Python documentation ``quoteall``:cite

### Caching selects

The select method also takes a cache argument, which defaults to None. For caching purposes, it should be set to a tuple where the first element is the cache model (cache.ram, cache.disk, etc.), and the second element is the expiration time in seconds.

In the following example, you see a controller that caches a select on the previously defined db.log table. The actual select fetches data from the back-end database no more frequently than once every 60 seconds and stores the result in cache.ram. If the next call to this controller occurs in less than 60 seconds since the last database IO, it simply fetches the previous data from cache.ram.

``cache select``:inxx
``
def cache_db_select():
    logs = db().select(db.log.ALL, cache=(cache.ram, 60))
    return dict(logs=logs)
``:code

``cacheable``:inxx

The ``select`` method has an optional ``cacheable`` argument, normally set to ``False``. When ``cacheable=True`` the resulting ``Rows`` is serializable but The ``Row``s lack ``update_record`` and ``delete_record`` methods.

If you do not need these methods you can speed up selects a lot by setting the cacheable attribute:

``
rows = db(query).select(cacheable=True)
``:code

When the ``cache`` argument is set but ``cacheable=False`` (default) only the database results are cached, not the actual Rows object. When the ``cache`` argument is used in conjunction with ``cacheable=True`` the entire Rows object is cached and this results in much faster caching:

``
rows = db(query).select(cache=(cache.ram,3600),cacheable=True)
``:code

### Self-Reference and aliases

``self reference``:inxx
``alias``:inxx
It is possible to define tables with fields that refer to themselves, here is an example:
``reference table``:inxx
``
db.define_table('person',
    Field('name'),
    Field('father_id', 'reference person'),
    Field('mother_id', 'reference person'))
``:code

Notice that the alternative notation of using a table object as field type will fail in this case, because it uses a variable ``db.person`` before it is defined:
``
db.define_table('person',
    Field('name'),
    Field('father_id', db.person), # wrong!
    Field('mother_id', db.person)) # wrong!
``:code

In general ``db.tablename`` and ``"reference tablename"`` are equivalent field types, but the latter is the only one allowed for self.references.

``with_alias``:inxx
If the table refers to itself, then it is not possible to perform a JOIN to select a person and its parents without use of the SQL "AS" keyword. This is achieved in web2py using the ``with_alias``. Here is an example:
``
>>> Father = db.person.with_alias('father')
>>> Mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father_id=1, mother_id=2)
3
>>> rows = db().select(db.person.name, Father.name, Mother.name,
      left=(Father.on(Father.id==db.person.father_id),
            Mother.on(Mother.id==db.person.mother_id)))
>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name
Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

Notice that we have chosen to make a distinction between:
- "father_id": the field name used in the table "person";
- "father": the alias we want to use for the table referenced by the above field; this is communicated to the database;
- "Father": the variable used by web2py to refer to that alias.

The difference is subtle, and there is nothing wrong in using the same name for the three of them:
``
db.define_table('person',
    Field('name'),
    Field('father', 'reference person'),
    Field('mother', 'reference person'))
>>> father = db.person.with_alias('father')
>>> mother = db.person.with_alias('mother')
>>> db.person.insert(name='Massimo')
1
>>> db.person.insert(name='Claudia')
2
>>> db.person.insert(name='Marco', father=1, mother=2)
3
>>> rows = db().select(db.person.name, father.name, mother.name,
      left=(father.on(father.id==db.person.father),
            mother.on(mother.id==db.person.mother)))
>>> for row in rows:
        print row.person.name, row.father.name, row.mother.name
Massimo None None
Claudia None None
Marco Massimo Claudia
``:code

But it is important to have the distinction clear in order to build correct queries.

### Advanced features

#### Table inheritance
``inheritance``:inxx

It is possible to create a table that contains all the fields from another table. It is sufficient to pass the other table in place of a field to ``define_table``. For example
``
db.define_table('person', Field('name'))
db.define_table('doctor', db.person, Field('specialization'))
``:code

``dummy table``:inxx
It is also possible to define a dummy table that is not stored in a database in order to reuse it in multiple other places. For example:

``
signature = db.Table(db, 'signature',
    Field('created_on', 'datetime', default=request.now),
    Field('created_by', db.auth_user, default=auth.user_id),
    Field('updated_on', 'datetime', update=request.now),
    Field('updated_by', db.auth_user, update=auth.user_id))

db.define_table('payment', Field('amount', 'double'), signature)
``:code

This example assumes that standard web2py authentication is enabled.

Notice that if you use ``Auth`` web2py already creates one such table for you:

``
auth = Auth(db)
db.define_table('payment', Field('amount', 'double'), auth.signature)
``

When using table inheritance, if you want the inheriting table to inherit validators, be sure to define the validators of the parent table before defining the inheriting table.

#### ``filter_in`` and ``filter_out``
``filter_in``:inxx ``filter_out``:inxx

It is possible to define a filter for each field to be called before a value is inserted into the database for that field and after a value is retrieved from the database.

Imagine for example that you want to store a serializable Python data structure in a field in the json format. Here is how it could be accomplished:

``
>>> from simplejson import loads, dumps
>>> db.define_table('anyobj',Field('name'),Field('data','text'))
>>> db.anyobj.data.filter_in = lambda obj, dumps=dumps: dumps(obj)
>>> db.anyobj.data.filter_out = lambda txt, loads=loads: loads(txt)
>>> myobj = ['hello', 'world', 1, {2: 3}]
>>> id = db.anyobj.insert(name='myobjname', data=myobj)
>>> row = db.anyobj(id)
>>> row.data
['hello', 'world', 1, {2: 3}]
``:code

Another way to accomplish the same is by using a Field of type ``SQLCustomType``, as discussed later.

#### before and after callbacks

``_before_insert``:inxx
``_after_insert``:inxx
``_before_update``:inxx
``_after_update``:inxx
``_before_delete``:inxx
``_after_delete``:inxx

Web2py provides a mechanism to register callbacks to be called before and/or after insert, update and delete of records.

Each table stores six lists of callbacks:

``
db.mytable._before_insert
db.mytable._after_insert
db.mytable._before_update
db.mytable._after_update
db.mytable._before_delete
db.mytable._after_delete
``:code

You can register callback function by appending it the corresponding function to one of those lists. The caveat is that depending on the functionality, the callback has different signature.

This is best explained via some examples.

``
>>> db.define_table('person',Field('name'))
>>> def pprint(*args): print args
>>> db.person._before_insert.append(lambda f: pprint(f))
>>> db.person._after_insert.append(lambda f,id: pprint(f,id))
>>> db.person._before_update.append(lambda s,f: pprint(s,f))
>>> db.person._after_update.append(lambda s,f: pprint(s,f))
>>> db.person._before_delete.append(lambda s: pprint(s))
>>> db.person._after_delete.append(lambda s: pprint(s))
``:code

Here ``f`` is a dict of fields passed to insert or update, ``id`` is the id of the newly inserted record, ``s`` is the Set object used for update or delete.

``
>>> db.person.insert(name='John')
({'name': 'John'},)
({'name': 'John'}, 1)
>>> db(db.person.id==1).update(name='Tim')
(<Set (person.id = 1)>, {'name': 'Tim'})
(<Set (person.id = 1)>, {'name': 'Tim'})
>>> db(db.person.id==1).delete()
(<Set (person.id = 1)>,)
(<Set (person.id = 1)>,)
``:code

The return values of these callback should be ``None`` or ``False``. If any of the ``_before_*`` callback returns a ``True`` value it will abort the actual insert/update/delete operation.

``update_naive``:inxx.

Some times a callback may need to perform an update in the same or a different table and one wants to avoid callbacks calling themselves recursively.

For this purpose there the Set objects have an ``update_naive`` method that works like ``update`` but ignores before and after callbacks.

#### Record versioning

``_enable_record_versioning``:inxx

It is possible to ask web2py to save every copy of a record when the record is individually modified. There are different ways to do it and it can be done for all tables at once using the syntax:

``
auth.enable_record_versioning(db)
``:code

this requires Auth and it is discussed in the chapter about authentication.
It can also be done for each individual table as discussed below.

Consider the following table:

``
db.define_table('stored_item',
    Field('name'),
    Field('quantity','integer'),
    Field('is_active','boolean',
          writable=False,readable=False,default=True))
``:code

Notice the hidden boolean field called ``is_active`` and defaulting to
True.

We can tell web2py to create a new table (in the same or a different database) and store all previous versions of each record in the table, when modified.

This is done in the following way:
``
db.stored_item._enable_record_versioning()
``:code

or in a more verbose syntax:

``
db.stored_item._enable_record_versioning(
    archive_db = db,
    archive_name = 'stored_item_archive',
    current_record = 'current_record',
    is_active = 'is_active')
``

The ``archive_db=db`` tells web2py to store the archive table in the same database as the ``stored_item`` table. The ``archive_name`` sets the name for the archive table. The archive table has the same fields as the original table ``stored_item`` except that unique fields are no longer unique (because it needs to store multiple versions) and has an extra field which name is specified by ``current_record`` and which is a reference to the current record in the ``stored_item`` table.

When records are deleted, they are not really deleted. A deleted record is copied in the ``stored_item_archive`` table (like when it is modified) and the ``is_active`` field is set to False. By enabling record versioning web2py sets a ``custom_filter`` on this table that hides all records in table ``stored_item`` where the ``is_active`` field is set to False. The ``is_active`` parameter in the ``_enable_record_versioning`` method allows to specify the name of the field used by the ``custom_filter`` to determine if the field was deleted or not.

``custom_filter``s are ignored by the appadmin interface.

#### Common fields and multi-tenancy
``common fields``:inxx
``multi tenancy``:inxx

``db._common_fields`` is a list of fields that should belong to all the tables. This list can also contain tables and it is understood as all fields from the table. For example occasionally you find yourself in need to add a signature to all your tables but the ```auth`` tables. In this case, after you ``db.define_tables()`` but before defining any other table, insert

``
db._common_fields.append(auth.signature)
``

One field is special: "request_tenant".
This field does not exist but you can create it and add it to any of your tables (or them all):

``
db._common_fields.append(Field('request_tenant',
    default=request.env.http_host,writable=False))
``

For every table with a field called ``db._request_tenant``, all records for all queries are always automatically filtered by:

``
db.table.request_tenant == db.table.request_tenant.default
``:code

and for every record insert, this field is set to the default value.
In the example above we have chosen
``
default = request.env.http_host
``
i.e. we have chose to ask our app to filter all tables in all queries with
``
db.table.request_tenant == request.env.http_host
``

This simple trick allow us to turn any application into a multi-tenant application. i.e. even if we run one instance of the app and we use one single database, if the app is accessed under two or more domains (in the example the domain name is retrieved from ``request.env.http_host``) the visitors will see different data depending on the domain. Think of running multiple web stores under different domains with one app and one database.

You can turn off multi tenancy filters using: ``ignore_common_filters``:inxx
``
rows = db(query, ignore_common_filters=True).select()
``:code

#### Common filters

A common filter is a generalization of the above multi-tenancy idea.
It provides an easy way to prevent repeating of the same query.
Consider for example the following table:

``
db.define_table('blog_post',
    Field('subject'),
    Field('post_text', 'text'),
    Field('is_public', 'boolean'),
    common_filter = lambda query: db.blog_post.is_public==True
)
``

Any select, delete or update in this table, will include only public blog posts. The attribute can also be changed in controllers:

``
db.blog_post._common_filter = lambda query: db.blog_post.is_public == True
``

It serves both as a way to avoid repeating the "db.blog_post.is_public==True" phrase in each blog post search, and also as a security enhancement, that prevents you from forgetting to disallow viewing of none public posts.

In case you actually do want items left out by the common filter (for example, allowing the admin to see none public posts), you can either remove the filter:
``
db.blog_post._common_filter = None
``
or ignore it:
``
db(query, ignore_common_filters=True).select(...)
``

#### Custom ``Field`` types (experimental)

``SQLCustomType``:inxx

Aside for using ``filter_in`` and ``filter_out``, it is possible to define new/custom field types.
For example we consider here a field that contains binary data in compressed form:

``
from gluon.dal import SQLCustomType
import zlib

compressed = SQLCustomType(
     type ='text',
     native='text',
     encoder =(lambda x: zlib.compress(x or '')),
     decoder = (lambda x: zlib.decompress(x))
)

db.define_table('example', Field('data',type=compressed))
``:code

``SQLCustomType`` is a field type factory. Its ``type`` argument must be one of the standard web2py types. It tells web2py how to treat the field values at the web2py level. ``native`` is the name of the field as far as the database is concerned. Allowed names depend on the database engine. ``encoder`` is an optional transformation function applied when the data is stored and ``decoder`` is the optional reversed transformation function.

This feature is marked as experimental. In practice it has been in web2py for a long time and it works but it can make the code not portable, for example when the native type is database specific. It does not work on Google App Engine NoSQL.

#### Using DAL without define tables

The DAL can be used from any Python program simply by doing this:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite',folder='path/to/app/databases')
``:code

i.e. import the DAL, Field, connect and specify the folder which contains the .table files (the app/databases folder).

To access the data and its attributes we still have to define all the tables we are going to access with ``db.define_tables(...)``.

If we just need access to the data but not to the web2py table attributes, we get away without re-defining the tables but simply asking web2py to read the necessary info from the metadata in the .table files:

``
from gluon import DAL, Field
db = DAL('sqlite://storage.sqlite',folder='path/to/app/databases',
         auto_import=True))
``:code

This allows us to access any ``db.table`` without need to re-define it.

#### PostGIS, SpatiaLite, and MS Geo (experimental)

``PostGIS``:inxx ``StatiaLite``:inxx ``Geo Extensions``:inxx
``geometry``:inxx ``geoPoint``:inxx ``geoLine``:inxx ``geoPolygon``:inxx

The DAL supports geographical APIs using PostGIS (for PostgreSQL), spatialite (for SQLite), and MSSQL and Spatial Extensions. This is a feature that was sponsored by the Sahana project and implemented by Denes Lengyel.

DAL provides geometry and geography fields types and the following functions:

``st_asgeojson``:inxx ``st_astext``:inxx ``st_contains``:inxx
``st_distance``:inxx ``st_equals``:inxx ``st_intersects``:inxx ``st_overlaps``:inxx
``st_simplify``:inxx ``st_touches``:inxx ``st_within``:inxx

``
st_asgeojson (PostGIS only)
st_astext
st_contains
st_distance
st_equals
st_intersects
st_overlaps
st_simplify (PostGIS only)
st_touches
st_within
st_x
st_y
``

Here are some examples:

``
from gluon.dal import DAL, Field, geoPoint, geoLine, geoPolygon
db = DAL("mssql://user:pass@host:db")
sp = db.define_table('spatial', Field('loc','geometry()'))
``:code

Below we insert a point, a line, and a polygon:
``
sp.insert(loc=geoPoint(1,1))
sp.insert(loc=geoLine((100,100),(20,180),(180,180)))
sp.insert(loc=geoPolygon((0,0),(150,0),(150,150),(0,150),(0,0)))
``:code

Notice that
``
rows = db(sp.id>0).select()
``:code

Always returns the geometry data serialized as text.
You can also do the same more explicitly using ``st_astext()``:

``
print db(sp.id>0).select(sp.id, sp.loc.st_astext())
spatial.id,spatial.loc.STAsText()
1, "POINT (1 2)"
2, "LINESTRING (100 100, 20 180, 180 180)"
3, "POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

You can ask for the native representation by using ``st_asgeojson()`` (in PostGIS only):

``
print db(sp.id>0).select(sp.id, sp.loc.st_asgeojson().with_alias('loc'))
spatial.id,loc
1, [1, 2]
2, [[100, 100], [20 180], [180, 180]]
3, [[[0, 0], [150, 0], [150, 150], [0, 150], [0, 0]]]
``:code

(notice an array is a point, an array of arrays is a line, and an array of array of arrays is a polygon).

Here are example of how to use geographical functions:

``
query = sp.loc.st_intersects(geoLine((20,120),(60,160)))
query = sp.loc.st_overlaps(geoPolygon((1,1),(11,1),(11,11),(11,1),(1,1)))
query = sp.loc.st_contains(geoPoint(1,1))
print db(query).select(sp.id,sp.loc)
spatial.id,spatial.loc
3,"POLYGON ((0 0, 150 0, 150 150, 0 150, 0 0))"
``:code

Computed distances can also be retrieved as floating point numbers:

``
dist = sp.loc.st_distance(geoPoint(-1,2)).with_alias('dist')
print db(sp.id>0).select(sp.id, dist)
spatial.id, dist
1 2.0
2 140.714249456
3 1.0
``:code

#### Copy data from one db into another

Consider the situation in which you have been using the following database:

``
db = DAL('sqlite://storage.sqlite')
``

and you wish to move to another database using a different connection string:

``
db = DAL('postgres://username:password@localhost/mydb')
``

Before you switch, you want to move the data and rebuild all the metadata for the new database. We assume the new database to exist but we also assume it is empty.

Web2py provides a script that does this work for you:

``
cd web2py
python scripts/cpdb.py \
   -f applications/app/databases \
   -y 'sqlite://storage.sqlite' \
   -Y 'postgres://username:password@localhost/mydb'
``

After running the script you can simply switch the connection string in the model and everything should work out of the box. The new data should be there.

This script provides various command line options that allows you to move data from one application to another, move all tables or only some tables, clear the data in the tables. for more info try:

``
python scripts/cpdb.py -h
``

#### Note on new DAL and adapters

The source code of the Database Abstraction Layer was completely rewritten in 2010. While it stays backward compatible, the rewrite made it more modular and easier to extend. Here we explain the main logic.

The file "gluon/dal.py" defines, among other, the following classes.

``
ConnectionPool
BaseAdapter extends ConnectionPool
Row
DAL
Reference
Table
Expression
Field
Query
Set
Rows
``

Their use has been explained in the previous sections, except for ``BaseAdapter``. When the methods of a ``Table`` or ``Set`` object need to communicate with the database they delegate to methods of the adapter the task to generate the SQL and or the function call.

For example:

``
db.mytable.insert(myfield='myvalue')
``

calls

``
Table.insert(myfield='myvalue')
``

which delegates the adapter by returning:

``
db._adapter.insert(db.mytable,db.mytable._listify(dict(myfield='myvalue')))
``

Here ``db.mytable._listify`` converts the dict of arguments into a list of ``(field,value)`` and calls the ``insert`` method of the ``adapter``. ``db._adapter`` does more or less the following:

``
query = db._adapter._insert(db.mytable,list_of_fields)
db._adapter.execute(query)
``

where the first line builds the query and the second executes it.

``BaseAdapter`` defines the interface for all adapters.

"gluon/dal.py" at the moment of writing this book, contains the following adapters:

``
SQLiteAdapter extends BaseAdapter
JDBCSQLiteAdapter extends SQLiteAdapter
MySQLAdapter extends BaseAdapter
PostgreSQLAdapter extends BaseAdapter
JDBCPostgreSQLAdapter extends PostgreSQLAdapter
OracleAdapter extends BaseAdapter
MSSQLAdapter extends BaseAdapter
MSSQL2Adapter extends MSSQLAdapter
FireBirdAdapter extends BaseAdapter
FireBirdEmbeddedAdapter extends FireBirdAdapter
InformixAdapter extends BaseAdapter
DB2Adapter extends BaseAdapter
IngresAdapter extends BaseAdapter
IngresUnicodeAdapter extends IngresAdapter
GoogleSQLAdapter extends MySQLAdapter
NoSQLAdapter extends BaseAdapter
GoogleDatastoreAdapter extends NoSQLAdapter
CubridAdapter extends MySQLAdapter (experimental)
TeradataAdapter extends DB2Adapter (experimental)
SAPDBAdapter extends BaseAdapter (experimental)
CouchDBAdapter extends NoSQLAdapter (experimental)
IMAPAdapter extends NoSQLAdapter (experimental)
MongoDBAdapter extends NoSQLAdapter (experimental)
``

which override the behavior of the ``BaseAdapter``.

Each adapter has more or less this structure:

``
class MySQLAdapter(BaseAdapter):

    # specify a diver to use
    driver = globals().get('pymysql',None)

    # map web2py types into database types
    types = {
        'boolean': 'CHAR(1)',
        'string': 'VARCHAR(%(length)s)',
        'text': 'LONGTEXT',
        ...
        }

    # connect to the database using driver
    def __init__(self,db,uri,pool_size=0,folder=None,db_codec ='UTF-8',
                credential_decoder=lambda x:x, driver_args={},
                adapter_args={}):
        # parse uri string and store parameters in driver_args
        ...
        # define a connection function
        def connect(driver_args=driver_args):
            return self.driver.connect(**driver_args)
        # place it in the pool
        self.pool_connection(connect)
        # set optional parameters (after connection)
        self.execute('SET FOREIGN_KEY_CHECKS=1;')
        self.execute("SET sql_mode='NO_BACKSLASH_ESCAPES';")

   # override BaseAdapter methods as needed
   def lastrowid(self,table):
        self.execute('select last_insert_id();')
        return int(self.cursor.fetchone()[0])

``:code

Looking at the various adapters as example should be easy to write new ones.

When ``db`` instance is created:

``
db = DAL('mysql://...')
``

the prefix in the uri string defines the adapter. The mapping is defined in the following dictionary also in "gluon/dal.py":

``
ADAPTERS = {
    'sqlite': SQLiteAdapter,
    'sqlite:memory': SQLiteAdapter,
    'mysql': MySQLAdapter,
    'postgres': PostgreSQLAdapter,
    'oracle': OracleAdapter,
    'mssql': MSSQLAdapter,
    'mssql2': MSSQL2Adapter,
    'db2': DB2Adapter,
    'teradata': TeradataAdapter,
    'informix': InformixAdapter,
    'firebird': FireBirdAdapter,
    'firebird_embedded': FireBirdAdapter,
    'ingres': IngresAdapter,
    'ingresu': IngresUnicodeAdapter,
    'sapdb': SAPDBAdapter,
    'cubrid': CubridAdapter,
    'jdbc:sqlite': JDBCSQLiteAdapter,
    'jdbc:sqlite:memory': JDBCSQLiteAdapter,
    'jdbc:postgres': JDBCPostgreSQLAdapter,
    'gae': GoogleDatastoreAdapter, # discouraged, for backward compatibility
    'google:datastore': GoogleDatastoreAdapter,
    'google:sql': GoogleSQLAdapter,
    'couchdb': CouchDBAdapter,
    'mongodb': MongoDBAdapter,
    'imap': IMAPAdapter
}
``:code

the uri string is then parsed in more detail by the adapter itself.

For any adapter you can replace the driver with a different one:

``
import MySQLdb as mysqldb
from gluon.dal import MySQLAdapter
MySQLAdapter.driver = mysqldb
``
i.e. ``mysqldb`` has to be ''that module'' with a .connect() method.
You can specify optional driver arguments and adapter arguments:

``
db =DAL(..., driver_args={}, adapter_args={})
``


#### Gotchas

**SQLite** does not support dropping and altering columns. That means that web2py migrations will work up to a point. If you delete a field from a table, the column will remain in the database but will be invisible to web2py. If you decide to reinstate the column, web2py will try re-create it and fail. In this case you must set ``fake_migrate=True`` so that metadata is rebuilt without attempting to add the column again. Also, for the same reason, **SQLite** is not aware of any change of column type. If you insert a number in a string field, it will be stored as string. If you later change the model and replace the type "string" with type "integer", SQLite will continue to keep the number as a string and this may cause problem when you try to extract the data.

**MySQL** does not support multiple ALTER TABLE within a single transaction. This means that any migration process is broken into multiple commits. If something happens that causes a failure it is possible to break a migration (the web2py metadata are no longer in sync with the actual table structure in the database). This is unfortunate but it can be prevented (migrate one table at the time) or it can be fixed a posteriori (revert the web2py model to what corresponds to the table structure in database, set ``fake_migrate=True`` and after the metadata has been rebuilt, set ``fake_migrate=False`` and migrate the table again).

**Google SQL** has the same problems as MySQL and more. In particular table metadata itself must be stored in the database in a table that is not migrated by web2py. This is because Google App Engine has a read-only file system. Web2py migrations in Google:SQL combined with the MySQL issue described above can result in metadata corruption. Again, this can be prevented (by migrating the table at once and then setting migrate=False so that the metadata table is not accessed any more) or it can fixed a posteriori (by accessing the database using the Google dashboard and deleting any corrupted entry from the table called ``web2py_filesystem``.

``limitby``:inxx
**MSSQL** does not support the SQL OFFSET keyword. Therefore the database cannot do pagination. When doing a ``limitby=(a,b)`` web2py will fetch the first ``b`` rows and discard the first ``a``. This may result in a considerable overhead when compared with other database engines.

**Oracle** also does not support pagination. It does not support neither the OFFSET nor the LIMIT keywords. Web2py achieves pagination by translating a ``db(...).select(limitby=(a,b))`` into a complex three-way nested select (as suggested by official Oracle documentation). This works for simple select but may break for complex selects involving aliased fields and or joins.

**MSSQL** has problems with circular references in tables that have ONDELETE CASCADE. This is an MSSQL bug and you work around it by setting the ondelete attribute for all reference fields to "NO ACTION". You can also do it once and for all before you define tables:

``
db = DAL('mssql://....')
for key in ['reference','reference FK']:
    db._adapter.types[key]=db._adapter.types[key].replace(
        '%(on_delete_action)s','NO ACTION')
``:code

**MSSQL** also has problems with arguments passed to the DISTINCT keyword and therefore
 while this works,

``
db(query).select(distinct=True)
``

this does not

``
db(query).select(distinct=db.mytable.myfield)
``

**Google NoSQL (Datastore)** does not allow joins, left joins, aggregates, expression, OR involving more than one table, the ‘like’ operator searches in "text" fields. Transactions are limited and not provided automatically by web2py (you need to use the Google API ``run_in_transaction`` which you can look up in the Google App Engine documentation online). Google also limits the number of records you can retrieve in each one query (1000 at the time of writing). On the Google datastore record IDs are integer but they are not sequential. While on SQL the "list:string" type is mapped into a "text" type, on the Google Datastore it is mapped into a ``ListStringProperty``. Similarly "list:integer" and "list:reference" are mapped into "ListProperty". This makes searches for content inside these fields types are more efficient on Google NoSQL than on SQL databases.
